# -*- coding: utf-8 -*-
"""Network Anomaly Detection .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HLDl1r-SdW37GaxUiAwZJyxRMqdzKGfs
"""

# Essential Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os, math, warnings, gzip, pickle, datetime, time

# Scipy and Statistical Analysis
from scipy import stats
from scipy.stats import (
    ttest_ind, levene, anderson, shapiro, mannwhitneyu,
    f_oneway, kruskal, chi2_contingency, fisher_exact
)
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.stats.contingency_tables import StratifiedTable

# Machine Learning Libraries
from sklearn.preprocessing import (
    StandardScaler, MinMaxScaler, OneHotEncoder, PowerTransformer, label_binarize
)
from sklearn.model_selection import (
    train_test_split, KFold, learning_curve, validation_curve,
    GridSearchCV, RandomizedSearchCV, cross_val_score, cross_validate, cross_val_predict
)
from sklearn.impute import KNNImputer

# Outlier Detection
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor, NearestNeighbors
from sklearn.covariance import EllipticEnvelope
from sklearn.svm import OneClassSVM

# Clustering and Dimensionality Reduction
from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering
from sklearn.decomposition import PCA
from umap import UMAP

# Classifiers and Model Ensembles
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree, export_graphviz
from sklearn.ensemble import (
    RandomForestClassifier, RandomForestRegressor, BaggingClassifier,
    VotingClassifier, AdaBoostClassifier, StackingClassifier, GradientBoostingClassifier
)
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

# Imbalanced Data Handling
from imblearn.over_sampling import SMOTE

# Metrics and Evaluation
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, fbeta_score,
    roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay,
    precision_recall_curve, classification_report, mean_absolute_error,
    mean_squared_error, precision_recall_fscore_support, auc
)
from sklearn.metrics.cluster import (
    silhouette_score, davies_bouldin_score, fowlkes_mallows_score,
    adjusted_mutual_info_score, adjusted_rand_score, normalized_mutual_info_score,
    homogeneity_score, completeness_score, v_measure_score, pair_confusion_matrix,
    contingency_matrix
)

!pip install mlflow
import mlflow

# Pandas Settings
pd.set_option('display.max_columns', None)

!gdown 1AlZak8gC27ntWFR0-ZJ0tMxVWFac-XPf

df=pd.read_csv("Network_anomaly_data.csv")

df

df.shape

# DATATYPES AND INFO OF ALL THE ATTRIBUTES
df.info()

# SHAPE OF THE DATA
print(f"Number of rows in the dataset = {df.shape[0]}")
print(f"Number of columns in the dataset = {df.shape[1]}")

# MISSING VALUE OR NULL VALUE DETECTION
df.isnull().sum()

"""Insights:- There are no null values in the given dataset."""

# DESCRIPTIVE STATISTICS
df.describe()

df.describe(include="object")

"""Insights:-
  1. There is no primary key in the given dataset. Combination of all the features creates primary key.
  2. Features like duration, wrongfragment, urgent, hot, numfailedlogins, numcompromised, numroot, numfilecreations, numshells and numaccessfiles have mostly filled with zeros. Because of that, These features have zeros in 25th, 50th and 75th percentiles. These are sparse features.
  3. Features like land, loggedin, rootshell, suattempted, ishostlogin, isguestlogin should binary discrete features according to the feature description. But suattempted has max = 2, So we have to convert that feature to binary by replacing all the 2’s with 1’s in that feature.
  4. Features like protocoltype, service, flag and attack are nominal categorical features.
  5. lastflag feature is also ordinal categorical feature. lastflag feature should not be considered for ml modeling as it by_product of mulitple ml model predictions and it may create bias. It can be used for evaluating the difficulty level of our trained model.
  6. All the rate related feature are having the range 0 to 1.
  7. Crucial Continuous features in the dataset are duration, srcbytes, dstbytes, count, srvcount, dsthostcount, dsthostsrvcount.
"""

#  NUMBER OF UNIQUE VALUES
for i in df.columns:
    print(i,":",df[i].nunique())

# UNIQUE VALUES OF COLUMNS WHOSE NUNIQUE <= 70
for i in df.columns:
    if df[i].nunique() <= 70:
        print(i,(df[i].unique()),"",sep = "\n")

"""Insights:-
  1. numoutboundcmds feature has only one unique value. So that feature has no significance. We can drop that feature.
  2. To get clear understanding, we can categorise the service, flag and attack features.
  3. protocoltype has 3 unique features which can be abbreviated as tcp — transmission control protocol, user datagram protocol, icmp — internet control message protocol
"""

# Service Categories

#Using ChatGPT and Networking knowledge, Services are classified into following 8 categories. (This categorisation is subjective in nature)

service_categories_dict = {
    "Remote Access and Control Services": [
        "telnet", "ssh", "login", "kshell", "klogin", "remote_job", "rje", "shell", "supdup"
    ],
    "File Transfer and Storage Services": [
        "ftp", "ftp_data", "tftp_u", "uucp", "uucp_path", "pm_dump", "printer"
    ],
    "Web and Internet Services": [
        "http", "http_443", "http_2784", "http_8001", "gopher", "whois", "Z39_50", "efs"
    ],
    "Email and Messaging Services": [
        "smtp", "imap4", "pop_2", "pop_3", "IRC", "nntp", "nnsp"
    ],
    "Networking Protocols and Name Services": [
        "domain", "domain_u", "netbios_dgm", "netbios_ns", "netbios_ssn", "ntp_u", "name", "hostnames"
    ],
    "Database and Directory Services": [
        "ldap", "sql_net"
    ],
    "Error and Diagnostic Services": [
        "echo", "discard", "netstat", "systat"
    ],
    "Miscellaneous and Legacy Services": [
        "aol", "auth", "bgp", "csnet_ns", "daytime", "exec", "finger", "time",
        "tim_i", "urh_i", "urp_i", "vmnet", "sunrpc", "iso_tsap", "ctf",
        "mtp", "link", "harvest", "courier", "X11", "red_i",
        "eco_i", "ecr_i", "other", "private"
    ]
}

# Flag Categories

# Using ChatGPT and Networking knowledge, Flags are classified into following 5 categories. (This categorisation is subjective in nature)

flag_categories_dict = {
    "Success Flag": ["SF"],
    "S Flag": ["S0", "S1", "S2", "S3"],
    "R Flag": ["REJ"],
    "Reset Flag": ["RSTR", "RSTO", "RSTOS0"],
    "SH&Oth Flag": ["SH","OTH"]
}

# ALL NUMERICAL COLUMNS RANGE
for i in df.columns:
    if not isinstance(df[i][0],str):
        print(f"Maximum of {i}",df[i].max())
        print(f"Minimum of {i}",df[i].min())
        print()

"""Insights:- Scaling is required to apply because of various ranges."""

# ALL COLUMNS WITH NUNIQUE <= 70

for i in df.columns:
    if df[i].nunique() <= 70:
        print("Value Counts of {}".format(i),end="\n\n")
        print(df[i].value_counts(dropna= False),end="\n\n")

"""Insights:-
  1. land, wrongfragment, urgent, hot, numfailedlogins, rootshell, suattempted, numfilecreations, numshells, numaccessfiles, ishostlogin, isguestlogin Features are very high right skewed features because there are greater than 120000 records in these features as zero.
  2. numoutboundcmds feature can be removed.
  3. attack type distribution is not balanced. so while splitting the data for test or validation, It is better to use Stratified sampling based on distribution.

# FEATURE ENGINEERING
"""

# Dictionary created for encoding purpose
attack_categories_dict = {
    "Normal": ['normal'],
    "DOS": ['back', 'land', 'neptune', 'pod', 'smurf', 'teardrop'],
    "R2L": ['ftp_write', 'guess_passwd', 'imap', 'multihop', 'phf', 'spy', 'warezclient', 'warezmaster'],
    "U2R": ['buffer_overflow', 'loadmodule', 'perl', 'rootkit'],
    "Probe": ['ipsweep', 'nmap', 'portsweep', 'satan']
}

# Creating attack_category, service_category, flag_category Features using respective dictionaries

# Creating a deep copy for adding new features
nadp_add = df.copy(deep=True)

# Create a reverse mapping dictionaries for easier lookup
attack_to_category = {attack: attack_category for attack_category, attacks in attack_categories_dict.items() for attack in attacks}
service_to_category = {service: service_category for service_category, services in service_categories_dict.items() for service in services}
flag_to_category = {flag: flag_category for flag_category, flags in flag_categories_dict.items() for flag in flags}

# Function to map flags to categories
def map_attack_to_category(attack):
    return attack_to_category.get(attack, "NAN")  # Default to "NAN"
def map_service_to_category(service):
    return service_to_category.get(service, "NAN")  # Default to "NAN"
def map_flag_to_category(flag):
    return flag_to_category.get(flag, "NAN")  # Default to "NAN"

# Apply the mapping function to create a new feature
nadp_add['attack_category'] = nadp_add['attack'].apply(map_attack_to_category)
nadp_add['service_category'] = nadp_add['service'].apply(map_service_to_category)
nadp_add['flag_category'] = nadp_add['flag'].apply(map_flag_to_category)
nadp_add['attack_or_normal'] = nadp_add['attack'].apply(lambda x: 0 if x == "normal" else 1)

#  Multiplying rate features with its respective count/srvcount/dsthostcount/dsthostsrvcount feature
#rate_features = ['srv_serror_rate', 'srv_rerror_rate', 'srv_rerror_rate', 'dst_host_srv_serror_rate', 'dst_host_srv_serror_rate'

nadp_add['serrors_count'] = df['serrorrate']*df['count']
nadp_add['rerrors_count'] = df['rerrorrate']*df['count']

nadp_add['samesrv_count'] = df['samesrvrate']*df['count']
nadp_add['diffsrv_count'] = df['diffsrvrate']*df['count']

nadp_add['serrors_srvcount'] = df['srvserrorrate']*df['srvcount']
nadp_add['rerrors_srvcount'] = df['srvrerrorrate']*df['srvcount']

nadp_add['srvdiffhost_srvcount'] = df['srvdiffhostrate']*df['srvcount']

nadp_add['dsthost_serrors_count'] = df['dsthostserrorrate']*df['dsthostcount']
nadp_add['dsthost_rerrors_count'] = df['dsthostrerrorrate']*df['dsthostcount']

nadp_add['dsthost_samesrv_count'] = df['dsthostsamesrvrate']*df['dsthostcount']
nadp_add['dsthost_diffsrv_count'] = df['dsthostdiffsrvrate']*df['dsthostcount']

nadp_add['dsthost_serrors_srvcount'] = df['dsthostsrvserrorrate']*df['dsthostsrvcount']
nadp_add['dsthost_rerrors_srvcount'] = df['dsthostsrvrerrorrate']*df['dsthostsrvcount']

nadp_add['dsthost_samesrcport_srvcount'] = df['dsthostsamesrcportrate']*df['dsthostsrvcount']
nadp_add['dsthost_srvdiffhost_srvcount'] = df['dsthostsrvdiffhostrate']*df['dsthostsrvcount']

# Remove `numoutboundcmds` feature

nadp_add = nadp_add.drop(["numoutboundcmds"],axis = 1)

# Add Data Speed features by Dividing bytes by duration

nadp_add['srcbytes/sec'] = nadp_add.apply(
    lambda row: row['srcbytes'] / row['duration'] if row['duration'] != 0 else row['srcbytes'] / (row['duration'] + 0.001),
    axis=1
)
nadp_add['dstbytes/sec'] = nadp_add.apply(
    lambda row: row['dstbytes'] / row['duration'] if row['duration'] != 0 else row['dstbytes'] / (row['duration'] + 0.001),
    axis=1
)

# Modify suattempted such that it is binary

nadp_add["suattempted"] = nadp_add["suattempted"].apply(lambda x: 0 if x == 0 else 1)

"""# **DATA VISUALISATION**

**Univariate Analysis**
"""

cont_cols = ['srcbytes', 'dstbytes','srcbytes/sec','dstbytes/sec',
             'duration', 'wrongfragment', 'urgent', 'hot',
             'numfailedlogins', 'numcompromised', 'numroot', 'numfilecreations',
             'numshells', 'numaccessfiles','count', 'srvcount',
             'serrorrate','rerrorrate', 'samesrvrate','diffsrvrate',
             'srvserrorrate', 'srvrerrorrate','srvdiffhostrate','srvdiffhost_srvcount',
             'dsthostcount', 'dsthostsrvcount', 'dsthostserrorrate','dsthostrerrorrate',
             'dsthostsamesrvrate','dsthostdiffsrvrate', 'dsthostsrvserrorrate', 'dsthostsrvrerrorrate',
             'dsthostsamesrcportrate', 'dsthostsrvdiffhostrate','serrors_count', 'rerrors_count',
             'samesrv_count','diffsrv_count', 'serrors_srvcount', 'rerrors_srvcount',
             'dsthost_serrors_count','dsthost_rerrors_count','dsthost_samesrv_count','dsthost_diffsrv_count',
             'dsthost_serrors_srvcount','dsthost_rerrors_srvcount','dsthost_samesrcport_srvcount','dsthost_srvdiffhost_srvcount']

cat_cols = ['protocoltype', 'service', 'flag','land','loggedin', 'rootshell',
            'suattempted','ishostlogin', 'isguestlogin','attack', 'lastflag',
            'attack_category', 'service_category', 'flag_category','attack_or_normal']

# Distribution plots of all Numerical Columns

num_cols = 4 #number of columns
fig = plt.figure(figsize = (num_cols*10,len(cont_cols)*10/num_cols))
plt.suptitle("hist plots for all numerical columns\n",fontsize=24)
k = 1
for i in cont_cols:
    plt.subplot(math.ceil(len(cont_cols)/num_cols),num_cols,k)
    plt.title("{}".format(i),fontsize = 20)
    k += 1
    plot = sns.histplot(data=nadp_add,x = i,kde = True,bins = 20)
    plt.xlabel("")  # No xlabel to keep plots clean
    plt.ylabel("")  # No ylabel to keep plots clean
    warnings.filterwarnings('ignore')
plt.tight_layout()
plt.subplots_adjust(top=0.96)
plt.show()
warnings.filterwarnings('ignore')

"""Insights:-     
1. Highly Right Skewed Distributions with single significal peak at 0 are — All Basic and Content Related Features
2. Count & Srvcount are having slighly better Right Skewed distribution with some dispersion
3. dsthostcount & dsthostsrvcount are having Left Skewed Distribution with two peaks. One at max & one at min. Similar distribution can be observed in dsthostsamesrvrate
4. Most of the rate related features are having two peaks. One at min and one at max.
"""

num_cols = 4
fig = plt.figure(figsize = (num_cols*10,len(cont_cols)*10/num_cols))
plt.suptitle("kde plots for all numerical columns with attack_or_normal as hue\n",fontsize=24)
k = 1
for i in cont_cols:
    plt.subplot(math.ceil(len(cont_cols)/num_cols),num_cols,k)
    plt.title("{}".format(i),fontsize = 20)
    k += 1
    plot = sns.kdeplot(data=nadp_add,x = i,hue = "attack_or_normal")
    plt.xlabel("")  # No xlabel to keep plots clean
    plt.ylabel("")  # No ylabel to keep plots clean
    plt.legend(plot.get_legend_handles_labels,labels = ["attack","normal"],fontsize = 14, title_fontsize = 16)
    warnings.filterwarnings('ignore')
plt.tight_layout()
plt.subplots_adjust(top=0.96)
plt.show()
warnings.filterwarnings('ignore')

"""Insights:-
  1. Wrong Fragments is a very important KPI based on attack vs normal kde distribution. All Postive wrong fragments are from attack categories only.
  2. All Time related and host related features are having clear distinction between attack and normal. These features may have high feature importance. Among them , Count, Serrorrate, Rerrorrate, samesrvrate, diffsrvrate, srvserrorrate, srvrerrorrate, dsthostcount, dsthostsrvcount, dsthostserrorrate, dsthostrerrorrate, dsthost_samesrv_count, dsthost_diffsrv_count are having significant difference between attack and normal.
"""

# Count plots of all categorical columns

num_cols = 5
fig = plt.figure(figsize = (num_cols*10,len(cat_cols)*10/num_cols))
plt.suptitle("count plots for all categorical columns\n",fontsize=24)
k = 1
for i in cat_cols:
    plt.subplot(math.ceil(len(cat_cols)/num_cols),num_cols,k)
    plt.title("{}".format(i),fontsize = 20)
    k += 1
    category_order = nadp_add[i].value_counts().index
    plot = sns.countplot(data=nadp_add,x = i,order=category_order)
    plt.xticks(rotation = 90,fontsize = 16)
    plt.xlabel("")  # No xlabel to keep plots clean
    plt.ylabel("")  # No ylabel to keep plots clean
    warnings.filterwarnings('ignore')
plt.tight_layout()
plt.subplots_adjust(top=0.96)
plt.show()
warnings.filterwarnings('ignore')

fig = plt.figure(figsize=(10,5))
category_order = nadp_add["service"].value_counts().index
sns.countplot(data=nadp_add,x = "service",order=category_order)
plt.xticks(rotation = 90,fontsize = 10)
plt.show()

"""Insights:-     
  1. tcp dominates in protocoltype
  2. http and private dominates in service
  3. SF dominates in flag
  4. neptune dominates in attack types
  5. u2r, r2l has very less number of rows. Highly imbalanced in case of 5 class classification
  6. Miscellaneous and Legacy Services & Web& Internet services dominates in service_category
  7. attack_or_normal has better balanced dataset. So Binary classification may work better than 5 class multi class classification.
"""

num_cols = 5
fig = plt.figure(figsize = (num_cols*10,len(cat_cols)*10/num_cols))
plt.suptitle("count plots for all categorical columns with hue attack_or_normal\n",fontsize=24)
k = 1
for i in cat_cols:
    plt.subplot(math.ceil(len(cat_cols)/num_cols),num_cols,k)
    plt.title("{}".format(i),fontsize = 20)
    k += 1
    plot = sns.countplot(data=nadp_add,x = i,hue = "attack_or_normal")
    plt.legend(plot.get_legend_handles_labels,labels = ["normal","attack"],fontsize = 14, title_fontsize = 16)
    plt.xticks(rotation = 90,fontsize = 16)
    plt.xlabel("")  # No xlabel to keep plots clean
    plt.ylabel("")  # No ylabel to keep plots clean
    warnings.filterwarnings('ignore')
plt.tight_layout()
plt.subplots_adjust(top=0.96)
plt.show()
warnings.filterwarnings('ignore')

fig = plt.figure(figsize=(15,5))
sns.countplot(data=nadp_add,x = "service",hue="attack_or_normal")
plt.xticks(rotation = 90,fontsize = 10)
plt.show()

"""Insights:-
attack dominates normal in icmp protocoltype, All Flags except SF, when no login in, private service type and Miscellaneous and legacy Services

# OUTLIER DETECTION
"""

# Box plots of all Numerical columns

fig = plt.figure(figsize = (8*5,len(cont_cols)*5/(8/2)))
plt.suptitle("box plots for all numerical columns\n",fontsize=24)
k = 1
for i in cont_cols:
    plt.subplot(math.ceil(len(cont_cols)/8),8,k)
    plt.title("{}".format(i),fontsize = 20)
    k += 1
    plot = sns.boxplot(data=nadp_add,y = i)

    # Increase label and legend font sizes
    plot.set_xlabel(plot.get_xlabel(), fontsize=18)
    plot.set_ylabel(plot.get_ylabel(), fontsize=18)

    warnings.filterwarnings('ignore')
plt.tight_layout()
plt.subplots_adjust(top=0.96)
plt.show()
warnings.filterwarnings('ignore')

"""Insights:-
  1. May be because of lot of zeros in the distribution, Box plots are showing lot of outliers
  2. Lets remove the zeros and plot them again
"""

fig = plt.figure(figsize = (8*5,len(cont_cols)*5/(8/2)))
plt.suptitle("box plots for all numerical columns\n",fontsize=24)
k = 1
for i in cont_cols:
    plt.subplot(math.ceil(len(cont_cols)/8),8,k)
    plt.title("{}".format(i),fontsize = 20)
    k += 1
    plot = sns.boxplot(data=nadp_add[nadp_add[i] != 0],y = i)

    # Increase label and legend font sizes
    plot.set_xlabel(plot.get_xlabel(), fontsize=18)
    plot.set_ylabel(plot.get_ylabel(), fontsize=18)

    warnings.filterwarnings('ignore')
plt.tight_layout()
plt.subplots_adjust(top=0.96)
plt.show()
warnings.filterwarnings('ignore')

"""Insights:- Even after removing zeros, There are significant number of outliers.

"""

# Boxcox transformation

# Initialize transformed DataFrame and outlier count
nadp_boxcox = nadp_add.copy(deep=True)
outlier_counts = pd.Series(0, index=nadp_add.index)

# Set up the plot figure
fig = plt.figure(figsize=(8 * 5, len(cont_cols) * 5 / (8 / 2)))
plt.suptitle("Box plots for all numerical columns after Box-Cox transformation\n", fontsize=24)
k = 1

for col in cont_cols:
    # Apply Box-Cox transformation
    transformed_data, _ = stats.boxcox(nadp_add[col] + 0.001)  # Avoid zero by adding a small constant
    nadp_boxcox[col] = transformed_data  # Store transformed data in nadp_boxcox

    # Identify outliers using IQR
    Q1 = nadp_boxcox[col].quantile(0.25)
    Q3 = nadp_boxcox[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Flag outliers for this column
    is_outlier = (nadp_boxcox[col] < lower_bound) | (nadp_boxcox[col] > upper_bound)
    outlier_counts += is_outlier.astype(int)  # Increment outlier count for each row

    # Plotting
    plt.subplot(math.ceil(len(cont_cols) / 8), 8, k)
    plt.title(col, fontsize=20)
    k += 1

    # Create the boxplot of transformed data
    sns.boxplot(y=nadp_boxcox[col])
    plt.xlabel("")  # No xlabel to keep plots clean
    plt.ylabel("")  # No ylabel to keep plots clean

# Add outlier count as a new feature
nadp_boxcox["number_of_features_identified_as_outlier"] = outlier_counts

# Finalize and display plot
warnings.filterwarnings('ignore')
plt.tight_layout()
plt.subplots_adjust(top=0.96)
plt.show()
warnings.filterwarnings('ignore')

# Check the additional feature "number_of_features_identified_as_outlier"
nadp_boxcox.head()

nadp_boxcox[nadp_boxcox["attack_or_normal"] == 0]["number_of_features_identified_as_outlier"].value_counts()

nadp_boxcox[nadp_boxcox["attack_or_normal"] == 1]["number_of_features_identified_as_outlier"].value_counts()

sns.kdeplot(data=nadp_boxcox,x = "number_of_features_identified_as_outlier",hue = "attack_or_normal")

"""Insights:-
  1. We can clearly identify that Boxcox transformation helps to remove the significant number of outliers.
  2. But it is not advisable to remove the outliers. Because Outliers are identified as Anomalies.
  3. We have created nadp_boxcox dataframe to compare the results with & without boxcox transformation to get an understanding.
  4. number_of_features_identified_as_outlier is an additional feature in nadp_boxcox. It represents how many features identified that particular row as outlier.
  5. If we observe top 5 value counts for attack vs normal, For normal → “0 2 1 4 3”, For Attack → “0 8 2 1 9”. It indicates that among the detected outliers (that means ignore 0), Attack category are identified more number of times than normal category. We can observe this kde plot at number 8 and 9 with high peak for attack category.

# BI-VARIATE ANALYSIS
"""

# Categorical Vs Categorical

# Import the permutations function from the itertools module
from itertools import permutations

num_cols = 3
imp_cat_features = ['protocoltype', 'service_category', 'flag_category', 'attack_category']
cat_perm = list(permutations(imp_cat_features, 2))

fig = plt.figure(figsize=(num_cols * 8, len(cat_perm) * 8 / num_cols))
plt.suptitle("Stacked hist plots of imp_categorical_features permutation\n", fontsize=20)
k = 1

for p, q in cat_perm:
    plt.subplot(math.ceil(len(cat_perm) / num_cols), num_cols, k)
    plt.title(f"Cross tab between {p} and {q} \nnormalizing about {q} in percentages", fontsize=18)
    k += 1

    # Create the cross-tabulated data for plotting
    plot = nadp_add.groupby([p])[q].value_counts(normalize=True).mul(100).reset_index(name='percentage')

    # Plot the histogram with stacked bars and store the axis object
    ax = sns.histplot(x=p, hue=q, weights='percentage', multiple='stack', data=plot, shrink=0.7)

    # Set x-axis tick label font size
    plt.xticks(rotation=45, fontsize=16)
    plt.xlabel("")  # No xlabel to keep plots clean
    plt.ylabel("")  # No ylabel to keep plots clean

    warnings.filterwarnings('ignore')

plt.tight_layout()
plt.subplots_adjust(top=0.95)
plt.show()
warnings.filterwarnings('ignore')

"""Insights:-
  1. icmp has dominated by Miscellaneous and Legacy Services only. udp has only two types of services Miscellaneous and Legacy Services and File Transfer and Storage Services.
  2. icmp has totally success flag but it also has mostly attacked.
  3. Error Flag distribution is high in Database and Directory Services, Error and Diagnostic services, Remote access and control services. That means, System able to flag properly in these services.
  4. Database and Directory Services, Error and Diagnostic services has dominated by attack categories only.
  5. Unable to flag properly in udp & icmp protocoltype.
  6. S Flag has mostly DOS attack category. SH&OTH Flag has mostly Probe attack category.
  7. R2L uses only tcp protocol.

# Numerical Vs Categorical
"""

# Basic and Content Related features

imp_cat_features = ['protocoltype', 'service_category', 'flag_category', 'attack_category']
imp_cont_features = ['duration', 'srcbytes','dstbytes','wrongfragment','urgent','hot','numfailedlogins',
                     'numcompromised','numroot','numfilecreations','numshells','numaccessfiles']
Cat_Vs_cont = []
for i in range(len(imp_cat_features)):
    for j in range(len(imp_cont_features)):
        if (nadp_add[imp_cat_features[i]].nunique()<50):
            Cat_Vs_cont.append((imp_cat_features[i], imp_cont_features[j]))

num_cols = 4
fig = plt.figure(figsize=(num_cols * 8, len(Cat_Vs_cont) * 8 / num_cols))
plt.suptitle("Barplot of Mean Values of Basic & Content Numerical Features with respect to Important Categorical Columns\n", fontsize=20)
k = 1

for p, q in Cat_Vs_cont:
    plt.subplot(math.ceil(len(Cat_Vs_cont) / num_cols), num_cols, k)
    plt.title(f"Bar plot between {p} and \nMean values of {q}", fontsize=18)
    k += 1

    # Plot the histogram with stacked bars and store the axis object
    df = pd.DataFrame(nadp_add.groupby([p])[[q]].mean().reset_index())
    sns.barplot(data = df,x = p,y= q,order = df.sort_values(q,ascending = False)[p])

    # Set x-axis tick label font size
    plt.xticks(rotation=45, fontsize=16)
    plt.xlabel("")  # No xlabel to keep plots clean
    plt.ylabel("")  # No ylabel to keep plots clean

    warnings.filterwarnings('ignore')

plt.tight_layout()
plt.subplots_adjust(top=0.97)
plt.show()
warnings.filterwarnings('ignore')

"""Insights:-
  1. udp dominates in avg duration & avg wrongfragments. In remaining all numerical features, tcp dominates.
  2. Miscellaneous and Legacy Services dominates in avg duration, avg dstbytes, avg wrong fragments. Error and Diagnostic Services dominates in avg srcbytes. Remote Access and Control Services dominates in urgent packets, numfailedlogins, numcompromised, numroot, numfilecreations, numshells, numaccessfiles. File Transfer and Storage Services dominates in hot indicators.
  3. Reset Flag dominates in avg duration, avg srcbytes, avg dstbytes, avg numfailedlogins. Remaining numerical features are dominated by Success flag.
  4. Probe dominates in avg duration, avg srcbytes, avg dstbytes. DOS dominates in wrong fragments. U2R dominates in urgent, numcompromised, numroot, numfilecreations, numshells, numaccessfiles. R2L dominates in hot, numfailedlogins.
"""

# Time and Host Related Features

imp_cat_features = ['protocoltype', 'service_category', 'flag_category', 'attack_category']
imp_cont_features2 =    ['dsthostcount', 'dsthost_serrors_count','dsthost_rerrors_count', 'dsthost_samesrv_count','dsthost_diffsrv_count',
                         'dsthostsrvcount','dsthost_serrors_srvcount','dsthost_rerrors_srvcount', 'dsthost_samesrcport_srvcount','dsthost_srvdiffhost_srvcount',
                        'count', 'serrors_count', 'rerrors_count', 'samesrv_count','diffsrv_count',
                        'srvcount','serrors_srvcount', 'rerrors_srvcount','srvdiffhost_srvcount']
Cat_Vs_cont2 = []
for i in range(len(imp_cat_features)):
    for j in range(len(imp_cont_features2)):
        if (nadp_add[imp_cat_features[i]].nunique()<50):
            Cat_Vs_cont2.append((imp_cat_features[i], imp_cont_features2[j]))

num_cols = 5
fig = plt.figure(figsize=(num_cols * 8, len(Cat_Vs_cont2) * 8 / num_cols))
plt.suptitle("Barplot of Mean Values of Time & Host Related Numerical Features with respect to Important Categorical Columns\n", fontsize=20)
k = 1

for p, q in Cat_Vs_cont2:
    plt.subplot(math.ceil(len(Cat_Vs_cont2) / num_cols), num_cols, k)
    plt.title(f"Bar plot between {p} and \nMean values of {q}", fontsize=18)
    k += 1

    # Plot the histogram with stacked bars and store the axis object
    df = pd.DataFrame(nadp_add.groupby([p])[[q]].mean().reset_index())
    sns.barplot(data = df,x = p,y= q,order = df.sort_values(q,ascending = False)[p])

    # Set x-axis tick label font size
    plt.xticks(rotation=45, fontsize=16)
    plt.xlabel("")  # No xlabel to keep plots clean
    plt.ylabel("")  # No ylabel to keep plots clean

    warnings.filterwarnings('ignore')

plt.tight_layout()
plt.subplots_adjust(top=0.97)
plt.show()
warnings.filterwarnings('ignore')

"""Insights:-
  1. udp dominates in dsthostcount & dsthostsrvcount and dsthost_samesrv_count, dsthost_diffsrv_count. icmp dominates in count & srcount and dsthost_samesrcport_srvcount, dsthost_srvdiffhost_srvcount, samesrvcount, srvdiffhost_srvcount. tcp dominates in all types of serrors and rerrors and diffsrvcount.
  2. Database and Directory Service dominates in dsthostcount, dsthost_rerrors_count, serrors_srvcount, rerrors_srvcount. Error and Diagnostic Services dominates in dsthost_serrors_count, count, serros_count, rerrors_count. Networking Protocol & Name services dominates in dsthost_samesrv_count, samesrv_count, srvcount. Miscellaneous and Legacy Services dominates in dsthost_diffsrv_count, dsthost_samesrcport_srvcount, diffsrv_count, srvdiffhost_srvcount. Web and Internet services dominates in dsthostsrvcount, dsthost_rerrors_srvcount, dsthost_srvdiffhost_srvcount.
  3. S Flag dominates in dsthostcount, dsthost_serrors_count, dsthost_serrors_srvcount, count, serrors_count, serrors_srvcount. Reset Flag dominates in dsthost_rerrors_count. R Flag dominates in dsthost_rerrors_srvcount, dsthost_srvdiffhost_srvcount, rerrors_count, diffsrv_count, rerrors_srvcount. SH&Oth Flag dominates in dsthost_diffsrv_count. Success Flag dominates in remaining features.
  4. DOS attack type dominates in dsthostcount, dsthost_serrors_count, dsthost_serrors_srvcount, count, serrors_count, samesrv_count, srvcount, serrors_srvcount, rerrors_srvcount. Probe dominates in dsthost_rerrors_count, dsthost_diffsrv_count, dsthost_samesrcport_srvcount, dsthost_srvdiffhost_srvcount, rerrors_count, diffsrv_count. Normal Flag dominates in remaining features.

## MULTI-VARIATE ANALYSIS
"""

# Numerical Vs Categorical Vs Target

num_cols = 6
fig = plt.figure(figsize=(num_cols * 8, len(Cat_Vs_cont[:-12]) * 8 / num_cols))
plt.suptitle("Barplot of Mean Values of Basic & Content Numerical Features with respect to Important Categorical Columns with hue as attack_or_normal\n", fontsize=20)
k = 1

for p, q in Cat_Vs_cont[:-12]:
    plt.subplot(math.ceil(len(Cat_Vs_cont[:-12]) / num_cols), num_cols, k)
    plt.title(f"Bar plot between {p} and \nMean values of {q}", fontsize=18)
    k += 1

    # Plot the histogram with stacked bars and store the axis object
    df = pd.DataFrame(nadp_add.groupby([p,"attack_or_normal"])[[q]].mean().reset_index())
    sns.barplot(data = df,x = p,y= q,hue = "attack_or_normal",order =df.sort_values(q,ascending = False)[p])

    # Set x-axis tick label font size
    plt.xticks(rotation=45, fontsize=16)
    plt.xlabel("")  # No xlabel to keep plots clean
    plt.ylabel("")  # No ylabel to keep plots clean

    warnings.filterwarnings('ignore')

plt.tight_layout()
plt.subplots_adjust(top=0.96)
plt.show()
warnings.filterwarnings('ignore')

"""Insights:-
  1. These bar plots help to understand the attack distribution too.
  2. Attack is significant due to bar plots related to srcbytes, dstbytes, wrongfragment, urgent, numshells.
"""

num_cols = 5
fig = plt.figure(figsize=(num_cols * 8, len(Cat_Vs_cont2[:-19]) * 8 / num_cols))
plt.suptitle("Barplot of Mean Values of Time & Host Related Numerical Features with respect to Important Categorical Columns with hue as attack_or_normal\n", fontsize=20)
k = 1

for p, q in Cat_Vs_cont2[:-19]:
    plt.subplot(math.ceil(len(Cat_Vs_cont2[:-19]) / num_cols), num_cols, k)
    plt.title(f"Bar plot between {p} and \nMean values of {q}", fontsize=18)
    k += 1

    # Plot the histogram with stacked bars and store the axis object
    df = pd.DataFrame(nadp_add.groupby([p,"attack_or_normal"])[[q]].mean().reset_index())
    sns.barplot(data = df,x = p,y= q,hue="attack_or_normal",order = df.sort_values(q,ascending = False)[p])

    # Set x-axis tick label font size
    plt.xticks(rotation=45, fontsize=16)
    plt.xlabel("")  # No xlabel to keep plots clean
    plt.ylabel("")  # No ylabel to keep plots clean

    warnings.filterwarnings('ignore')

plt.tight_layout()
plt.subplots_adjust(top=0.97)
plt.show()
warnings.filterwarnings('ignore')

"""Insights:-
  1. These bar plots help to understand the attack distribution too.
  2. Attack is significant in bar plots related to all types of serrors and rerrors counts.

# Numerical Vs Numerical Vs Target
"""

from itertools import combinations # Import the combinations function from itertools

imp_cont_cols = ['duration','srcbytes', 'dstbytes','count', 'srvcount','dsthostcount', 'dsthostsrvcount']
cont_comb = list(combinations(imp_cont_cols,2))
num_cols = 3
fig = plt.figure(figsize=(num_cols * 8, len(cont_comb) * 8 / num_cols))
plt.suptitle("Scatter plot between Important Numerical Features with hue as attack_or_normal\n", fontsize=20)
k = 1

for p, q in cont_comb:
    plt.subplot(math.ceil(len(cont_comb) / num_cols), num_cols, k)
    plt.title(f"Scatter plot between {p} and {q}", fontsize=18)
    k += 1
    sns.scatterplot(data = nadp_add,x=p,y=q,hue = "attack_or_normal")
    warnings.filterwarnings('ignore')

plt.tight_layout()
plt.subplots_adjust(top=0.96)
plt.show()
warnings.filterwarnings('ignore')

"""Insights:-
Among all the plots, clear distinction between attack and normal can be seen in count vs srvcount.

# **CORRELATION HEAT MAPS**
"""

# Basic and Content Features Correlation

cols = ['duration', 'protocoltype', 'service', 'flag', 'srcbytes', 'dstbytes',
       'land', 'wrongfragment', 'urgent', 'hot', 'numfailedlogins', 'loggedin',
       'numcompromised', 'rootshell', 'suattempted', 'numroot',
       'numfilecreations', 'numshells', 'numaccessfiles', 'ishostlogin',
       'isguestlogin','srcbytes/sec', 'dstbytes/sec']

corr = nadp_add[cols].corr(numeric_only=True)
plt.figure(figsize=(25,20))
sns.heatmap(corr,annot=True)
plt.show()

"""Insights:-
  1. Correlation > 0.8 is observed between isguestlogin & hot.
  2. Remaining all combinations of features in Basic and content related features have no significant correlation.
"""

# Time and Host Related Features Correlation

cols = ['count', 'srvcount', 'serrorrate', 'srvserrorrate',
       'rerrorrate', 'srvrerrorrate', 'samesrvrate', 'diffsrvrate',
       'srvdiffhostrate', 'dsthostcount', 'dsthostsrvcount',
       'dsthostsamesrvrate', 'dsthostdiffsrvrate', 'dsthostsamesrcportrate',
       'dsthostsrvdiffhostrate', 'dsthostserrorrate', 'dsthostsrvserrorrate',
       'dsthostrerrorrate', 'dsthostsrvrerrorrate', 'attack', 'lastflag',
       'attack_category', 'service_category', 'flag_category',
       'attack_or_normal', 'serrors_count', 'rerrors_count', 'samesrv_count',
       'diffsrv_count', 'serrors_srvcount', 'rerrors_srvcount',
       'srvdiffhost_srvcount', 'dsthost_serrors_count',
       'dsthost_rerrors_count', 'dsthost_samesrv_count',
       'dsthost_diffsrv_count', 'dsthost_serrors_srvcount',
       'dsthost_rerrors_srvcount', 'dsthost_samesrcport_srvcount',
       'dsthost_srvdiffhost_srvcount']

corr = nadp_add[cols].corr(numeric_only=True)
plt.figure(figsize=(30,25))
sns.heatmap(corr,annot=True)
plt.show()

"""Insights:-

Positive correlation > 0.9 is observed between

    srvserrorrate and serrorrate correlation: 0.99
    srvrerrorrate and rerrorrate correlation: 0.99
    dsthostserrorrate and serrorrate correlation: 0.98
    dsthostserrorrate and srvserrorrate correlation: 0.98
    dsthostsrvserrorrate and serrorrate correlation: 0.98
    dsthostsrvserrorrate and srvserrorrate correlation: 0.99
    dsthostsrvserrorrate and dsthostserrorrate correlation: 0.99
    dsthostrerrorrate and rerrorrate correlation: 0.93
    dsthostrerrorrate and srvrerrorrate correlation: 0.92
    dsthostsrvrerrorrate and rerrorrate correlation: 0.96
    dsthostsrvrerrorrate and srvrerrorrate correlation: 0.97
    dsthostsrvrerrorrate and dsthostrerrorrate correlation: 0.92
    samesrv_count and srvcount correlation: 0.98
    dsthost_serrors_count and serrorrate correlation: 0.96
    dsthost_serrors_count and srvserrorrate correlation: 0.96
    dsthost_serrors_count and dsthostserrorrate correlation: 0.99
    dsthost_serrors_count and dsthostsrvserrorrate correlation: 0.98
    dsthost_diffsrv_count and dsthostdiffsrvrate correlation: 0.92

Negative correlation < -0.7 is observed between

    Samesrvrate and serrorrate correlation: -0.76
    samesrvrate and srvserrorrate correlation: -0.76
    dsthostserrorrate and samesrvrate correlation: -0.76
    dsthostsrvserrorrate and samesrvrate correlation: -0.77
    attack_or_normal and samesrvrate correlation: -0.75
    attack_or_normal and dsthostsrvcount correlation: -0.72
    serrors_count and samesrvrate correlation: -0.73
    dsthost_serrors_count and samesrvrate correlation: -0.76

# **HYPOTHESIS TESTING**
"""

# NETWORK TRAFFIC VOLUME AND ANOMALIES

# Does network connections with unusually high or low traffic volumes (bytes transferred) are more likely to be anomalous?

# srcbytes vs attack_or_normal

# data groups
srcbytes_normal = nadp_add[nadp_add["attack_or_normal"] == 0]["srcbytes"]
srcbytes_attack = nadp_add[nadp_add["attack_or_normal"] == 1]["srcbytes"]

# visual analysis
sns.barplot(data = nadp_add, x = "attack_or_normal",y = "srcbytes",estimator="mean")

"""Insights:-
  1. Graph indicates that there is significant difference between srcbytes transferred during attack vs during normal.
  2. We can observe error bar has very high and unequal dispersions. So it is difficult to judge by bar plot. Let’s use hypothesis testing.
  3. We can frame alternate hypothesis as the means of srcbytes_normal != srcbytes_attack or srcbytes_normal < srcbytes_attack.

Selection of Appropriate Test

attack_or_normal is a categorical column with two categories — 0 and 1 (normal and attack). srcbytes column is providing number of srcbytes transferred from source to destination in a network connection. srcbytes is a numerical column.

So here, two independent samples are tested based on number of srcbytes transferred from source to destination.

Comparing means of two independent samples is required. So ttest_ind is appropriate if distributions are normal. Mann Whitney U test is appropriate if distributions are non-normal.

ttest_ind Hypothesis Formulation
"""

# two tailed_t_test
alpha = 0.05
print("variance of srcbytes_normal",np.var(srcbytes_normal),"\nvariance of srcbytes_attack", np.var(srcbytes_attack))

tstat,p_val = ttest_ind(a=srcbytes_normal, b=srcbytes_attack, equal_var=False,alternative = "two-sided")
print("ttest_ind t_stat:", tstat, "p-value:", p_val)
if p_val <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of ttest_ind test i.e., the mean of srcbytes_normal is not equal to the mean of srcbytes_attack")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of ttest_ind test i.e., the mean of srcbytes_normal is equal to the mean of srcbytes_attack")

# one_tailed_t_test

stat,p_val = ttest_ind(a=srcbytes_normal, b=srcbytes_attack, equal_var=False,alternative = "less")
print("ttest_ind t_stat:", tstat, "p-value:", p_val)
if p_val <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of ttest_ind test i.e., the mean of srcbytes_normal is less than the mean of srcbytes_attack")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of ttest_ind test i.e., the mean of srcbytes_normal is greater than or equal to the mean of srcbytes_attack")

"""Check the assumptions of ttest_ind

The two-sample independent t-test assumes the following characteristics about the data:

  1. Independence: The observations in each sample must be independent of each other. This means that the value of one observation should not be related to the value of any other observation in the same sample.
  2. Normality: The data within each group should follow a normal distribution. However, the t-test is quite robust to violations of normality, especially for large sample sizes (typically n>30).
  3. Homogeneity of Variances: The variances of the two groups should be equal.

Independence
srcbytes_normal and srcbytes_attack are independent groups.
"""

# Normality

## Checking the distribution of the two
plt.hist(srcbytes_normal, bins=5, alpha=0.2, color='r', label='srcbytes_normal')
plt.hist(srcbytes_attack, bins=5, alpha=0.2, color='b', label='srcbytes_attack')
plt.legend()
plt.show()

# Shapiro test for normality check.

# Running the shapiro test
shapiro_stat1, shapiro_pval1 = shapiro(srcbytes_normal)
shapiro_stat2, shapiro_pval2 = shapiro(srcbytes_attack)

print("shapiro test for Normality:")
print("srcbytes_normal - Statistic:", shapiro_stat1, "p-value:", shapiro_pval1)
print("srcbytes_attack - Statistic:", shapiro_stat2, "p-value:", shapiro_pval2)

if shapiro_pval1 <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of shapiro test i.e., srcbytes_normal is not normal distributed")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of shapiro test i.e., srcbytes_normal is normal distributed")

if shapiro_pval2 <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of shapiro test i.e., srcbytes_attack is not normal distributed")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of shapiro test i.e., srcbytes_attack is normal distributed")

"""NOTE
We cannot use shapiro test because it is suited for sample size < 5000. Let’s use anderson test because it is suited for large datasets.
"""

# anderson test

# Running the Anderson test for both groups
anderson_result1 = anderson(srcbytes_normal, dist="norm")
anderson_result2 = anderson(srcbytes_attack, dist="norm")

# Displaying the results
print("Anderson test for Normality:")
print("srcbytes_normal - Statistic:", anderson_result1.statistic)
print("srcbytes_normal - Critical Values:", anderson_result1.critical_values)
print("srcbytes_attack - Statistic:", anderson_result2.statistic)
print("srcbytes_attack - Critical Values:", anderson_result2.critical_values)

# Define alpha
alpha = 0.05

# Check for normality in srcbytes_normal
if anderson_result1.statistic > anderson_result1.critical_values[2]:  # Use the 5% critical value
    print("As the statistic exceeds the 5% critical value, we reject the null hypothesis of the Anderson test; i.e., srcbytes_normal is not normally distributed.")
else:
    print("As the statistic does not exceed the 5% critical value, we cannot reject the null hypothesis of the Anderson test; i.e., srcbytes_normal is normally distributed.")

# Check for normality in srcbytes_attack
if anderson_result2.statistic > anderson_result2.critical_values[2]:  # Use the 5% critical value
    print("As the statistic exceeds the 5% critical value, we reject the null hypothesis of the Anderson test; i.e., srcbytes_attack is not normally distributed.")
else:
    print("As the statistic does not exceed the 5% critical value, we cannot reject the null hypothesis of the Anderson test; i.e., srcbytes_attack is normally distributed.")

"""# Checking Homogeneity of Variances"""

# Levene test

stat, p_value = levene(srcbytes_normal, srcbytes_attack)
print(f"levene test between srcbytes_normal and srcbytes_attack\nstat   : {stat}\np_value : {p_value}")
if p_value <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of levene test i.e., srcbytes_attack and srcbytes_normal has unequal variances")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of levene test i.e., srcbytes_attack and srcbytes_normal has equal variances")

"""Insights:-
  1. As srcbytes_normal and srcbytes_attack groups are violating normality and homogeneity of variances assumptions, we cannot use ttest_ind (parametric test).
  2. So we need to apply non-parametric test — Mann Whitney U Test (also called as Wilcoxon Rank sum test).

**Mann Whitney U Test Hypothesis Formulation**
"""

# two tailed_mann_whitney test

stat,p_val = mannwhitneyu(x=srcbytes_normal, y=srcbytes_attack,alternative = "two-sided")
print("mannwhitneyu stat:", stat, "p-value:", p_val)
if p_val <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of mannwhitneyu test i.e., the distribution of srcbytes_normal is stochastically not equal to the distribution of srcbytes_attack")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of mannwhitneyu test i.e., the distribution of srcbytes_normal is stochastically equal to the distribution of srcbytes_attack")

# one tailed_mann_whitney test

stat,p_val = mannwhitneyu(x=srcbytes_normal, y=srcbytes_attack,alternative = "less")
print("mannwhitneyu stat:", stat, "p-value:", p_val)
if p_val <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of mannwhitneyu test i.e., the distribution of srcbytes_normal is stochastically less than the distribution of srcbytes_attack")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of mannwhitneyu test i.e., the distribution of srcbytes_normal is stochasitically greater than or equal to the distribution of srcbytes_attack")

"""Insights:-
  1. As the ttest_ind assumptions are not satisfied by srcbytes, we use mann_whitney u test.
  2. Two-tailed test suggests that the distribution of srcbytes_normal is stochastically not equal to the distribution of srcbytes_attack.
  3. One-tailed test suggests that the distribution of srcbytes_normal is stochastically greater than or equal to the distribution of srcbytes_attack.
"""

# dstbytes vs attack_or_normal

# data groups
dstbytes_normal = nadp_add[nadp_add["attack_or_normal"] == 0]["dstbytes"]
dstbytes_attack = nadp_add[nadp_add["attack_or_normal"] == 1]["dstbytes"]


# visual analysisi

sns.barplot(data = nadp_add, x = "attack_or_normal",y = "dstbytes",estimator="mean")

"""Insights:-
  1. Graph indicates that there is significant difference between dstbytes transferred during attack vs during normal.
  2. We can observe error bar has very high and unequal dispersions. So it is difficult to judge by bar plot. Let’s use hypothesis testing.
  3. We can frame alternate hypothesis as the means of dstbytes_normal != dstbytes_attack or dstbytes_normal < dstbytes_attack.

Selection of Appropriate Test

attack_or_normal is a categorical column with two categories — 0 and 1 (normal and attack). dstbytes column is providing number of dstbytes transferred from destination to source in a network connection. dstbytes is a numerical column.

So here, two independent samples are tested based on number of dstbytes transferred from destination to source.

Comparing means of two independent samples is required. So ttest_ind is appropriate if distributions are normal. Mann Whitney U test is appropriate if distributions are non-normal.

ttest_ind Hypothesis Formulation
"""

# two tailed t_test

print("variance of dstbytes_normal",np.var(dstbytes_normal),"\nvariance of dstbytes_attack", np.var(dstbytes_attack))

tstat,p_val = ttest_ind(a=dstbytes_normal, b=dstbytes_attack, equal_var=True,alternative = "two-sided")
print("ttest_ind t_stat:", tstat, "p-value:", p_val)
if p_val <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of ttest_ind test i.e., the mean of dstbytes_normal is not equal to the mean of dstbytes_attack")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of ttest_ind test i.e., the mean of dstbytes_normal is equal to the mean of dstbytes_attack")

# # one tailed t_test

tstat,p_val = ttest_ind(a=dstbytes_normal, b=dstbytes_attack, equal_var=True,alternative = "less")
print("ttest_ind t_stat:", tstat, "p-value:", p_val)
if p_val <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of ttest_ind test i.e., the mean of dstbytes_normal is less than the mean of dstbytes_attack")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of ttest_ind test i.e., the mean of dstbytes_normal is greater than or equal to the mean of dstbytes_attack")

## Check the assumptions of ttest_ind

# Independence

# dstbytes_normal and dstbytes_attack are independent groups.

# Normality


## Checking the distribution of the two
plt.hist(dstbytes_normal, bins=5, alpha=0.2, color='r', label='dstbytes_normal')
plt.hist(dstbytes_attack, bins=5, alpha=0.2, color='b', label='dstbytes_attack')
plt.legend()
plt.show()

# We cannot use shapiro test because it is suited for sample size < 5000. Let’s use anderson test because it is suited for large datasets.

# # Running the Anderson test for both groups
anderson_result1 = anderson(dstbytes_normal, dist="norm")
anderson_result2 = anderson(dstbytes_attack, dist="norm")

# Displaying the results
print("Anderson test for Normality:")
print("dstbytes_normal - Statistic:", anderson_result1.statistic)
print("dstbytes_normal - Critical Values:", anderson_result1.critical_values)
print("dstbytes_attack - Statistic:", anderson_result2.statistic)
print("dstbytes_attack - Critical Values:", anderson_result2.critical_values)

# Define alpha
alpha = 0.05

# Check for normality in dstbytes_normal
if anderson_result1.statistic > anderson_result1.critical_values[2]:  # Use the 5% critical value
    print("As the statistic exceeds the 5% critical value, we reject the null hypothesis of the Anderson test; i.e., dstbytes_normal is not normally distributed.")
else:
    print("As the statistic does not exceed the 5% critical value, we cannot reject the null hypothesis of the Anderson test; i.e., dstbytes_normal is normally distributed.")

# Check for normality in dstbytes_attack
if anderson_result2.statistic > anderson_result2.critical_values[2]:  # Use the 5% critical value
    print("As the statistic exceeds the 5% critical value, we reject the null hypothesis of the Anderson test; i.e., dstbytes_attack is not normally distributed.")
else:
    print("As the statistic does not exceed the 5% critical value, we cannot reject the null hypothesis of the Anderson test; i.e., dstbytes_attack is normally distributed.")

# Checking Homogeneity of Variances

# Levene test is better than Bartlett test with non-normally distributed datasets

stat, p_value = levene(dstbytes_normal, dstbytes_attack)
print(f"levene test between dstbytes_normal and dstbytes_attack\nstat   : {stat}\np_value : {p_value}")
if p_value <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of levene test i.e., dstbytes_attack and dstbytes_normal has unequal variances")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of levene test i.e., dstbytes_attack and dstbytes_normal has equal variances")

"""Insights:-     
1. As dstbytes_normal and dstbytes_attack groups are violating normality assumption, we cannot use ttest_ind (parametric test).
2. So we need to apply non-parametric test — Mann Whitney U Test (also called as Wilcoxon Rank sum test).

**Mann Whitney U Test Hypothesis Formulation**
"""

# two tailed Mann Whitney U Test

stat,p_val = mannwhitneyu(x=dstbytes_normal, y=dstbytes_attack,alternative = "two-sided")
print("mannwhitneyu stat:", stat, "p-value:", p_val)
if p_val <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of mannwhitneyu test i.e., the distribution of dstbytes_normal is stochastically not equal to the distribution of dstbytes_attack")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of mannwhitneyu test i.e., the distribution of dstbytes_normal is stochastically equal to the distribution of dstbytes_attack")

# one tailed Mann Whitney U Test

stat,p_val = mannwhitneyu(x=dstbytes_normal, y=dstbytes_attack,alternative = "less")
print("mannwhitneyu t_stat:", stat, "p-value:", p_val)
if p_val <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of mannwhitneyu test i.e., the distribution of dstbytes_normal is stochastically less than the distribution of dstbytes_attack")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of mannwhitneyu test i.e., the distribution of dstbytes_normal is stochasitically greater than or equal to the distribution of dstbytes_attack")

"""Insights:-
1. As the ttest_ind assumptions are not satisfied by dstbytes, we use mann_whitney u test.

2. Two-tailed test suggests that the distribution of dstbytes_normal is stochastically not equal to the distribution of dstbytes_attack.

3. One-tailed test suggests that the distribution of dstbytes_normal is stochastically greater than or equal to the distribution of dstbytes_attack.
"""

# srcbytes vs attack_categories

# data groups
srcbytes_Normal = nadp_add[nadp_add["attack_category"] == "Normal"]["srcbytes"]
srcbytes_DOS = nadp_add[nadp_add["attack_category"] == "DOS"]["srcbytes"]
srcbytes_R2L = nadp_add[nadp_add["attack_category"] == "R2L"]["srcbytes"]
srcbytes_Probe = nadp_add[nadp_add["attack_category"] == "Probe"]["srcbytes"]
srcbytes_U2R = nadp_add[nadp_add["attack_category"] == "U2R"]["srcbytes"]


# Visual Analysis

sns.barplot(data = nadp_add, x = "attack_category",y = "srcbytes",estimator="mean")

"""Insights:-
  1. Graph indicates that there is significant difference between srcbytes transferred during different attack categories.
  2. We can observe error bar has very high and unequal dispersions. So it is difficult to judge by bar plot. Let’s use hypothesis testing.
  3. We can frame alternate hypothesis as the at least one of the attack category srcbytes average is different from others.

Selection of Appropriate Test

attack_category is a categorical column with five categories. srcbytes column is providing number of srcbytes transferred from source to destination in a network connection. srcbytes is a numerical column.

So here, five independent samples are tested based on number of srcbytes transferred from source to destination.

Comparing means of five independent samples is required. So f_oneway (anova) is appropriate if distributions are normal. Kruskal (anova) is appropriate if distributions are non-normal.

f_oneway Hypothesis Formulation

Null Hypothesis H0:
The means of srcbytes across all 5 attack categories are equal.

Alternate Hypothesis Ha:
At least one of the attack category srcbytes means is different from the others.

Significance level: 0.05
"""

alpha = 0.05
print("variance of srcbytes_Normal",np.var(srcbytes_Normal),"\nvariance of srcbytes_DOS", np.var(srcbytes_DOS),"\nvariance of srcbytes_R2L", np.var(srcbytes_R2L),"\nvariance of srcbytes_Probe", np.var(srcbytes_Probe),"\nvariance of srcbytes_U2R", np.var(srcbytes_U2R))

fstat,p_val = f_oneway(srcbytes_Normal,srcbytes_DOS,srcbytes_R2L,srcbytes_Probe,srcbytes_U2R)
print("f_oneway f_stat:", fstat, "p-value:", p_val)
if p_val <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of f_oneway test i.e., Atleast one of the attack category srcbytes mean is different from the others.")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of f_oneway test i.e., The means of srcbytes across all 5 attack categories are equal.")

"""Check the assumptions of f_oneway

The two-samples independent t-test assume the following characteristics about the data:

  1. Independence:- The observations in each sample must be independent of each other. This means that the value of one observation should not be related to the value of any other observation in the same sample.
  2. Normality:- The data within each group should follow a normal distribution.
  3. Homogeneity of Variances:- The variances of the groups should be equal.

Independence:

  1. srcbytes_Normal, srcbytes_DOS, srcbytes_R2L, srcbytes_Probe, and srcbytes_U2R are independent groups.


"""

# Normality

## Checking the distribution of the Five groups
plt.hist(srcbytes_Normal, bins=5, label='srcbytes_Normal')
plt.hist(srcbytes_DOS, bins=5, label='srcbytes_DOS')
plt.hist(srcbytes_R2L, bins=5, label='srcbytes_R2L')
plt.hist(srcbytes_Probe, bins=5, label='srcbytes_Probe')
plt.hist(srcbytes_U2R, bins=5, label='srcbytes_U2R')
plt.legend()
plt.show()

# We cannot use Shapiro test because it is suited for sample sizes < 5000. Let’s use Anderson test because it is suited for large datasets.

# Running the Anderson test for both groups
anderson_result1 = anderson(srcbytes_Normal, dist="norm")
anderson_result2 = anderson(srcbytes_DOS, dist="norm")
anderson_result3 = anderson(srcbytes_R2L, dist="norm")
anderson_result4 = anderson(srcbytes_Probe, dist="norm")
anderson_result5 = anderson(srcbytes_U2R, dist="norm")

# Displaying the results
print("Anderson test for Normality:")
print("srcbytes_Normal - Statistic:", anderson_result1.statistic)
print("srcbytes_Normal - Critical Values:", anderson_result1.critical_values)
print("srcbytes_DOS - Statistic:", anderson_result2.statistic)
print("srcbytes_DOS - Critical Values:", anderson_result2.critical_values)
print("srcbytes_R2L - Statistic:", anderson_result3.statistic)
print("srcbytes_R2L - Critical Values:", anderson_result3.critical_values)
print("srcbytes_Probe - Statistic:", anderson_result4.statistic)
print("srcbytes_Probe - Critical Values:", anderson_result4.critical_values)
print("srcbytes_U2R - Statistic:", anderson_result5.statistic)
print("srcbytes_U2R - Critical Values:", anderson_result5.critical_values)

# Define alpha
alpha = 0.05

# Check for normality in srcbytes_Normal
if anderson_result1.statistic > anderson_result1.critical_values[2]:  # Use the 5% critical value
    print("As the statistic exceeds the 5% critical value, we reject the null hypothesis of the Anderson test; i.e., srcbytes_Normal is not normally distributed.")
else:
    print("As the statistic does not exceed the 5% critical value, we cannot reject the null hypothesis of the Anderson test; i.e., srcbytes_Normal is normally distributed.")

# Check for normality in srcbytes_DOS
if anderson_result2.statistic > anderson_result2.critical_values[2]:  # Use the 5% critical value
    print("As the statistic exceeds the 5% critical value, we reject the null hypothesis of the Anderson test; i.e., srcbytes_DOS is not normally distributed.")
else:
    print("As the statistic does not exceed the 5% critical value, we cannot reject the null hypothesis of the Anderson test; i.e., srcbytes_DOS is normally distributed.")

# Check for normality in srcbytes_R2L
if anderson_result3.statistic > anderson_result3.critical_values[2]:  # Use the 5% critical value
    print("As the statistic exceeds the 5% critical value, we reject the null hypothesis of the Anderson test; i.e., srcbytes_R2L is not normally distributed.")
else:
    print("As the statistic does not exceed the 5% critical value, we cannot reject the null hypothesis of the Anderson test; i.e., srcbytes_R2L is normally distributed.")

# Check for normality in srcbytes_Probe
if anderson_result4.statistic > anderson_result4.critical_values[2]:  # Use the 5% critical value
    print("As the statistic exceeds the 5% critical value, we reject the null hypothesis of the Anderson test; i.e., srcbytes_Probe is not normally distributed.")
else:
    print("As the statistic does not exceed the 5% critical value, we cannot reject the null hypothesis of the Anderson test; i.e., srcbytes_Probe is normally distributed.")

# Check for normality in srcbytes_U2R
if anderson_result5.statistic > anderson_result5.critical_values[2]:  # Use the 5% critical value
    print("As the statistic exceeds the 5% critical value, we reject the null hypothesis of the Anderson test; i.e., srcbytes_U2R is not normally distributed.")
else:
    print("As the statistic does not exceed the 5% critical value, we cannot reject the null hypothesis of the Anderson test; i.e., srcbytes_U2R is normally distributed.")

# Checking Homogeneity of Variances

stat, p_value = levene(srcbytes_Normal, srcbytes_DOS, srcbytes_R2L, srcbytes_Probe, srcbytes_U2R)
print(f"levene test between srcbytes_Normal, srcbytes_DOS, srcbytes_R2L, srcbytes_Probe, srcbytes_U2R\nstat   : {stat}\np_value : {p_value}")
if p_value <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of levene test i.e., srcbytes_Normal, srcbytes_DOS, srcbytes_R2L, srcbytes_Probe, srcbytes_U2R has unequal variances")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of levene test i.e., srcbytes_Normal, srcbytes_DOS, srcbytes_R2L, srcbytes_Probe, srcbytes_U2R has equal variances")

"""Insights:-

  1. As srcbytes_Normal, srcbytes_DOS, srcbytes_R2L, srcbytes_Probe, srcbytes_U2R groups are violating normality and homogeneity of variances assumptions, we cannot use f_oneway (parametric test).
  2. So we need to apply non-parametric test — Kruskal Wallis test.
"""

#Kruskal Wallis Test Hypothesis Formulation

#Null Hypothesis H0:
#The medians of srcbytes across all 5 attack categories are equal.

#Alternate Hypothesis Ha:
#At least one of the attack category srcbytes medians is different from the others.

# Significance level: 0.05.

fstat,p_val = kruskal(srcbytes_Normal,srcbytes_DOS,srcbytes_R2L,srcbytes_Probe,srcbytes_U2R)
print("kruskal f_stat:", fstat, "p-value:", p_val)
if p_val <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of kruskal test i.e., Atleast one of the attack category srcbytes median is different from the others.")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of kruskal test i.e., The medians of srcbytes across all 5 attack categories are equal.")

"""Insights:-
  1. As the f_oneway assumptions are not satisfied by srcbytes, We use kruskal test.
  2. Kruskal test suggests that at least one of the attack category srcbytes medians is different from the others.
"""

# dstbytes vs attack_categories

# data groups
dstbytes_Normal = nadp_add[nadp_add["attack_category"] == "Normal"]["dstbytes"]
dstbytes_DOS = nadp_add[nadp_add["attack_category"] == "DOS"]["dstbytes"]
dstbytes_R2L = nadp_add[nadp_add["attack_category"] == "R2L"]["dstbytes"]
dstbytes_Probe = nadp_add[nadp_add["attack_category"] == "Probe"]["dstbytes"]
dstbytes_U2R = nadp_add[nadp_add["attack_category"] == "U2R"]["dstbytes"]

# visula analysisi

sns.barplot(data = nadp_add, x = "attack_category",y = "dstbytes",estimator="mean")

"""Insights:-
  1. Graph indicates that there is significant difference between dstbytes transferred during different attack categories.
  2. We can observe error bar has very high and unequal dispersions. So it is difficult to judge by bar plot. Let’s use hypothesis testing.*
  3. We can frame alternate hypothesis as at least one of the attack category dstbytes average is different from others.

Selection of Appropriate Test

attack_category is categorical column with five categories. dstbytes column is providing number of dstbytes transferred from source to destination in a network connection. dstbytes is numerical column.

So here five independent samples are tested based on number of dstbytes transferred from source to destination.

Comparing means of five independent samples is required. So f_oneway(anova) is appropriate if distributions are normal. Kruskal(anova) is appropriate if distributions are non-normal.

f_oneway Hypothesis Formulation

Null Hypothesis H0: The means of dstbytes across all 5 attack categories are equal.

Alternate Hypothesis Ha: Atleast one of the attack category dstbytes mean is different from the others.

Significance level: 0.05
"""

alpha = 0.05
print("variance of dstbytes_Normal",np.var(dstbytes_Normal),"\nvariance of dstbytes_DOS", np.var(dstbytes_DOS),"\nvariance of dstbytes_R2L", np.var(dstbytes_R2L),"\nvariance of dstbytes_Probe", np.var(dstbytes_Probe),"\nvariance of dstbytes_U2R", np.var(dstbytes_U2R))

fstat,p_val = f_oneway(dstbytes_Normal,dstbytes_DOS,dstbytes_R2L,dstbytes_Probe,dstbytes_U2R)
print("f_oneway f_stat:", fstat, "p-value:", p_val)
if p_val <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of f_oneway test i.e., Atleast one of the attack category dstbytes mean is different from the others.")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of f_oneway test i.e., The means of dstbytes across all 5 attack categories are equal.")

# Check the assumptions of f_oneway

# Independence

# dstbytes_Normal, dstbytes_DOS, dstbytes_R2L, dstbytes_Probe, and dstbytes_U2R are independent groups.

# Normality

## Checking the distribution of the Five groups
plt.hist(dstbytes_Normal, bins=5, label='dstbytes_Normal')
plt.hist(dstbytes_DOS, bins=5, label='dstbytes_DOS')
plt.hist(dstbytes_R2L, bins=5, label='dstbytes_R2L')
plt.hist(dstbytes_Probe, bins=5, label='dstbytes_Probe')
plt.hist(dstbytes_U2R, bins=5, label='dstbytes_U2R')
plt.legend()
plt.show()

# Running the Anderson test for both groups
anderson_result1 = anderson(dstbytes_Normal, dist="norm")
anderson_result2 = anderson(dstbytes_DOS, dist="norm")
anderson_result3 = anderson(dstbytes_R2L, dist="norm")
anderson_result4 = anderson(dstbytes_Probe, dist="norm")
anderson_result5 = anderson(dstbytes_U2R, dist="norm")

# Displaying the results
print("Anderson test for Normality:")
print("dstbytes_Normal - Statistic:", anderson_result1.statistic)
print("dstbytes_Normal - Critical Values:", anderson_result1.critical_values)
print("dstbytes_DOS - Statistic:", anderson_result2.statistic)
print("dstbytes_DOS - Critical Values:", anderson_result2.critical_values)
print("dstbytes_R2L - Statistic:", anderson_result3.statistic)
print("dstbytes_R2L - Critical Values:", anderson_result3.critical_values)
print("dstbytes_Probe - Statistic:", anderson_result4.statistic)
print("dstbytes_Probe - Critical Values:", anderson_result4.critical_values)
print("dstbytes_U2R - Statistic:", anderson_result5.statistic)
print("dstbytes_U2R - Critical Values:", anderson_result5.critical_values)

# Define alpha
alpha = 0.05

# Check for normality in dstbytes_Normal
if anderson_result1.statistic > anderson_result1.critical_values[2]:  # Use the 5% critical value
    print("As the statistic exceeds the 5% critical value, we reject the null hypothesis of the Anderson test; i.e., dstbytes_Normal is not normally distributed.")
else:
    print("As the statistic does not exceed the 5% critical value, we cannot reject the null hypothesis of the Anderson test; i.e., dstbytes_Normal is normally distributed.")

# Check for normality in dstbytes_DOS
if anderson_result2.statistic > anderson_result2.critical_values[2]:  # Use the 5% critical value
    print("As the statistic exceeds the 5% critical value, we reject the null hypothesis of the Anderson test; i.e., dstbytes_DOS is not normally distributed.")
else:
    print("As the statistic does not exceed the 5% critical value, we cannot reject the null hypothesis of the Anderson test; i.e., dstbytes_DOS is normally distributed.")

# Check for normality in dstbytes_R2L
if anderson_result3.statistic > anderson_result3.critical_values[2]:  # Use the 5% critical value
    print("As the statistic exceeds the 5% critical value, we reject the null hypothesis of the Anderson test; i.e., dstbytes_R2L is not normally distributed.")
else:
    print("As the statistic does not exceed the 5% critical value, we cannot reject the null hypothesis of the Anderson test; i.e., dstbytes_R2L is normally distributed.")

# Check for normality in dstbytes_Probe
if anderson_result4.statistic > anderson_result4.critical_values[2]:  # Use the 5% critical value
    print("As the statistic exceeds the 5% critical value, we reject the null hypothesis of the Anderson test; i.e., dstbytes_Probe is not normally distributed.")
else:
    print("As the statistic does not exceed the 5% critical value, we cannot reject the null hypothesis of the Anderson test; i.e., dstbytes_Probe is normally distributed.")

# Check for normality in dstbytes_U2R
if anderson_result5.statistic > anderson_result5.critical_values[2]:  # Use the 5% critical value
    print("As the statistic exceeds the 5% critical value, we reject the null hypothesis of the Anderson test; i.e., dstbytes_U2R is not normally distributed.")
else:
    print("As the statistic does not exceed the 5% critical value, we cannot reject the null hypothesis of the Anderson test; i.e., dstbytes_U2R is normally distributed.")

# Checking Homogeneity of Variances

stat, p_value = levene(dstbytes_Normal, dstbytes_DOS, dstbytes_R2L, dstbytes_Probe, dstbytes_U2R)
print(f"levene test between dstbytes_Normal, dstbytes_DOS, dstbytes_R2L, dstbytes_Probe, dstbytes_U2R\nstat   : {stat}\np_value : {p_value}")
if p_value <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of levene test i.e., dstbytes_Normal, dstbytes_DOS, dstbytes_R2L, dstbytes_Probe, dstbytes_U2R has unequal variances")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of levene test i.e., dstbytes_Normal, dstbytes_DOS, dstbytes_R2L, dstbytes_Probe, dstbytes_U2R has equal variances")

"""Insights:-

  1. as dstbytes_Normal, dstbytes_DOS, dstbytes_R2L, dstbytes_Probe, dstbytes_U2R groups are violating normality and homogeneity of variances assumptions, we cannot use ttest_ind (parametric test).
  2. So we need to apply non-parametric test — Kruskal Wallis test.
"""

#Kruskal Wallis Test Hypothesis Formulation

#Null Hypothesis H0:
#The medians of dstbytes across all 5 attack categories are equal.

#Alternate Hypothesis Ha:
#At least one of the attack category dstbytes median is different from the others.

#Significance level: 0.05.

fstat,p_val = kruskal(dstbytes_Normal,dstbytes_DOS,dstbytes_R2L,dstbytes_Probe,dstbytes_U2R)
print("kruskal f_stat:", fstat, "p-value:", p_val)
if p_val <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of kruskal test i.e., Atleast one of the attack category dstbytes median is different from the others.")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of kruskal test i.e., The medians of dstbytes across all 5 attack categories are equal.")

"""Insights:-
  1. As the f_oneway assumptions are not satisfied by dstbytes, we use Kruskal test.
  2. Kruskal test suggests that at least one of the attack category dstbytes medians is different from the others.

# IMPACT OF PROTOCOL TYPE ON ANOMALY DETECTION
"""

#qDoes Certain protocols are more frequently associated with network anomalies.?
# protocoltype vs attack_or_normal

# data groups
Protocoltype = nadp_add["protocoltype"]
Attack_or_normal = nadp_add["attack_or_normal"]
# Create a contingency table
contingency_table = pd.crosstab(Protocoltype, Attack_or_normal)


# Visual Analysis

sns.heatmap(contingency_table, annot=True, fmt="d", cmap="YlGnBu")

"""Insights:-
1. It is difficult to judge the relation between two categorical features by contingency heat plot. Let’s use hypothesis testing.
2. We can frame alternate hypothesis as protocoltype influences attack_or_normal categorization.

Selection of Appropriate Test

attack_or_normal is categorical column with two categories - 0 and 1 (normal and attack). protocoltype column is providing type of network protocol used in a network connection. It is a categorical column.

So here the independence between two categorical features should be tested.

Comparing observed contingency table with expected contingency table is required. So chi2_contingency test is appropriate if at least 20% cells in expected frequencies > 5. fisher_exact test is appropriate if more than 20% cells in expected frequencies < 5 and 2x2 contingency table. For larger table, we should use advanced techniques like chi2_contingency along with montecarlo simulation.

chi2_contingency Hypothesis Formulation

Null Hypothesis H0: protocoltype does not influence whether a connection is classified as attack_or_normal.

Alternate Hypothesis Ha: protocoltype does influence whether a connection is classified as attack_or_normal.

Significance level: 0.05
"""

alpha = 0.05
chi2, p_value, dof, expected = chi2_contingency(contingency_table)
print(f"Chi-square Statistic: {chi2}")
print(f"P-value: {p_value}")
print(f"Degrees of Freedom: {dof}")
print("Expected Frequencies:")
print(expected)
if p_val <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of chi2_contingency test i.e., protocoltype does influences whether a connection is classified as attack_or_normal.")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of chi2_contingency test i.e., protocoltype does not influences whether a connection is classified as attack_or_normal.")

"""Check the assumptions of chi2_contingency test

The chi2_contingency test assumes the following characteristics about the data:

  1. Independence:- The observations in each sample must be independent of each other. This means that the value of one observation should not be related to the value of any other observation in the same sample.
  2. Expected counts:- No more than 20% of the expected counts should be less than 5, and all individual expected counts should be 1 or greater.

Independence

  1. protocoltype and attack_or_normal are independent groups.

Expected counts
"""

expected_df = pd.DataFrame(expected,
                            index=contingency_table.index.tolist(),
                            columns=contingency_table.columns.tolist())
sns.heatmap(expected_df, annot=True, fmt=".2f", cmap="Blues", cbar=True)

"""All values in expected contingency table are greater than 5.

Observation

  1. As all the assumptions of chi2_contingency test is satisfied. no need to apply chi2_contingency with monte carlo simulation. (larger table)
  2. chi2_contingency test suggests that protocoltype does influences whether a connection is classified as attack_or_normal.
"""

# protocoltype vs attack_categories

# data groups
Protocoltype = nadp_add["protocoltype"]
Attack_category = nadp_add["attack_category"]
# Create a contingency table
contingency_table = pd.crosstab(Protocoltype, Attack_category)

# Visual Analysis

sns.heatmap(contingency_table, annot=True, fmt="d", cmap="YlGnBu")

"""Observation

  1. It is difficult to judge the relation between two categorical features by contingency heat plot. let’s use hypothesis testing.
  2. We can frame alternate hypothesis as protocoltype influences attack_category categorization.

Selection of Appropriate Test

attack_category is a categorical column with five categories. protocoltype column provides the type of network protocol used in a network connection. It is also a categorical column.

So, here the independence between two categorical features should be tested.

Comparing the observed contingency table with the expected contingency table is required. The chi2_contingency test is appropriate if at least 20% of cells in expected frequencies are greater than 5. The fisher_exact test is appropriate if more than 20% of cells in expected frequencies are less than 5 and it's a 2x2 contingency table. For larger tables, we should use advanced techniques like chi2_contingency along with Monte Carlo simulation.

chi2_contingency Hypothesis Formulation

Null Hypothesis H0: protocoltype does not influence attack_category.

Alternate Hypothesis Ha: protocoltype does influence attack_category.

Significance level: 0.05.
"""

chi2, p_value, dof, expected = chi2_contingency(contingency_table)
print(f"Chi-square Statistic: {chi2}")
print(f"P-value: {p_value}")
print(f"Degrees of Freedom: {dof}")
print("Expected Frequencies:")
print(expected)
if p_val <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of chi2_contingency test i.e., protocoltype does influences attack_category.")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of chi2_contingency test i.e., protocoltype does not influences attack_category.")

# Check the assumptions of chi2_contingency test

# Independence: Protocoltype and attack_or_normal are independent groups.

# Expected counts

expected_df = pd.DataFrame(expected,
                            index=contingency_table.index.tolist(),
                            columns=contingency_table.columns.tolist())
sns.heatmap(expected_df, annot=True, fmt=".2f", cmap="Blues", cbar=True)

"""Observation

  1. tcp-U2R value in expected contingency table is less than 5. But only one out of 15 cells, It is less than 20% of cells (3 cells max). So no need to apply chi2_contingency with montecarlo simulation (larger table).
  2. chi2_contingency test suggests that protocoltype does influence whether a connection is classified as attack_or_normal.

# ROLE OF SERVICE IN NETWORK SECURITY
"""

# Does Specific services are targets of network anomalies more often than others?

# service vs attack_or_normal

# data groups
Service = nadp_add["service"]
Attack_or_normal = nadp_add["attack_or_normal"]
# Create a contingency table
contingency_table = pd.crosstab(Service, Attack_or_normal)


# Visual Analysis

fig = plt.figure(figsize=(10,20))
sns.heatmap(contingency_table, annot=True, fmt="d", cmap="YlGnBu")

"""Observation

  1. It is difficult to judge the relation between two categorical features by contingency heat plot. Let’s use hypothesis testing.
  2. We can frame the alternate hypothesis as service type influences attack_or_normal categorization.

Selection of Appropriate Test

attack_or_normal is a categorical column with two categories - 0 and 1 (normal and attack). The service column provides the type of service used in a network connection, and it is a categorical column.

So, here the independence between two categorical features should be tested.

Comparing the observed contingency table with the expected contingency table is required. Therefore, the chi2_contingency test is appropriate if at least 20% of cells in expected frequencies are greater than 5. The fisher_exact test is appropriate if more than 20% of cells in expected frequencies are less than 5, and the table is 2x2. For larger tables, advanced techniques like chi2_contingency along with Monte Carlo simulation should be used.

chi2_contingency Hypothesis Formulation

Null Hypothesis H0: Service type does not influence whether a connection is classified as attack_or_normal.

Alternate Hypothesis Ha: Service type does influence whether a connection is classified as attack_or_normal.

Significance level: 0.05
"""

chi2, p_value, dof, expected = chi2_contingency(contingency_table)
print(f"Chi-square Statistic: {chi2}")
print(f"P-value: {p_value}")
print(f"Degrees of Freedom: {dof}")
print("Expected Frequencies:")
print(expected)
if p_val <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of chi2_contingency test i.e., service type does influences whether a connection is classified as attack_or_normal.")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of chi2_contingency test i.e., service type does not influences whether a connection is classified as attack_or_normal.")

# Check the assumptions of chi2_contingency test

#Independence

# Service and attack_or_normal are independent groups.

# Expected counts


fig = plt.figure(figsize=(10,20))
expected_df = pd.DataFrame(expected,
                            index=contingency_table.index.tolist(),
                            columns=contingency_table.columns.tolist())
sns.heatmap(expected_df, annot=True, fmt=".2f", cmap="Blues", cbar=True)

# Calculate the number of cells with counts less than 5
count_less_than_5 = (expected_df < 5).sum().sum()
Total_cells = expected_df.shape[0]*expected_df.shape[1]
# Print the result
print("Number of cells with count less than 5:", count_less_than_5)
print("Total Number of cells:", Total_cells)
print("20% of Total Number of cells", 0.2*Total_cells)

"""Observation

  1. 17 values in the expected contingency table are less than 5. 17 is less than 20% of the total number of cells (28 max). So, there is no need to apply chi2_contingency with Monte Carlo simulation (larger table).
  2. The chi2_contingency test suggests that service type does influence whether a connection is classified as attack_or_normal.
"""

# service vs attack_categories

# data groups
Service = nadp_add["service"]
Attack_category = nadp_add["attack_category"]
# Create a contingency table
contingency_table = pd.crosstab(Service, Attack_category)

# Visual Analysis

fig = plt.figure(figsize=(15,20))
sns.heatmap(contingency_table, annot=True, fmt="d", cmap="YlGnBu")

"""Observation

  1. It is difficult to judge the relation between two categorical features by contingency heat plot. Let’s use hypothesis testing.
  2. We can frame the alternate hypothesis as service type influences attack_category categorization.

Selection of Appropriate Test

attack_category is a categorical column with five categories. The service column provides the type of service used in a network connection, and it is a categorical column.

So, here the independence between two categorical features should be tested.

Comparing the observed contingency table with the expected contingency table is required. Therefore, the chi2_contingency test is appropriate if at least 20% of cells in expected frequencies are greater than 5. The fisher_exact test is appropriate if more than 20% of cells in expected frequencies are less than 5, and the table is 2x2. For larger tables, advanced techniques like chi2_contingency along with Monte Carlo simulation should be used.

chi2_contingency Hypothesis Formulation

Null Hypothesis H0: Service type does not influence attack_category.

Alternate Hypothesis Ha: Service type does influence attack_category.

Significance level: 0.05
"""

chi2, p_value, dof, expected = chi2_contingency(contingency_table)
print(f"Chi-square Statistic: {chi2}")
print(f"P-value: {p_value}")
print(f"Degrees of Freedom: {dof}")
print("Expected Frequencies:")
print(expected)
if p_val <= alpha:
    print("As p_value <= 0.05, We reject the null hypothesis of chi2_contingency test i.e., service type does influences attack_category.")
else:
    print("As p_value > 0.05, We cannot reject the null hypothesis of chi2_contingency test i.e., service type does not influences attack_category.")

# Check the assumptions of chi2_contingency test

# Independence

# Service and attack_category are independent groups.

# Expected counts

fig = plt.figure(figsize=(15,20))
expected_df = pd.DataFrame(expected,
                            index=contingency_table.index.tolist(),
                            columns=contingency_table.columns.tolist())
sns.heatmap(expected_df, annot=True, fmt=".2f", cmap="Blues", cbar=True)

# Calculate the number of cells with counts less than 5
count_less_than_5 = (expected_df < 5).sum().sum()
Total_cells = expected_df.shape[0]*expected_df.shape[1]
# Print the result
print("Number of cells with count less than 5:", count_less_than_5)
print("Total Number of cells:", Total_cells)
print("20% of Total Number of cells", 0.2*Total_cells)

"""Observation

    143 values in the expected contingency table are less than 5. 143 is greater than 20% of the total number of cells (70 max). So, we need to apply chi2_contingency with Monte Carlo simulation.
"""

# chi2_contingency test with monte carlo simulation

def monte_carlo_chi_square(observed, n_simulations=10000):
    # Run chi2_contingency to get observed chi-square and expected frequencies
    chi_square_stat, p_val, dof, expected = chi2_contingency(observed)

    # Store simulated chi-square statistics
    simulated_stats = []

    # Run Monte Carlo simulations
    for _ in range(n_simulations):
        # Generate a random contingency table under the null hypothesis
        simulated_data = np.random.multinomial(observed.sum(), expected.flatten() / expected.sum())

        # Avoid zero cells in the expected array
        expected_nonzero = expected.flatten()
        expected_nonzero[expected_nonzero == 0] = 1e-10  # Small value to prevent zero division

        # Calculate chi-square statistic for the simulated table
        simulated_stat = ((simulated_data - expected_nonzero) ** 2 / expected_nonzero).sum()
        simulated_stats.append(simulated_stat)

    # Calculate Monte Carlo p-value
    p_value_mc = np.sum(np.array(simulated_stats) >= chi_square_stat) / n_simulations

    return chi_square_stat, p_value_mc

observed = np.array(contingency_table)

# Perform Monte Carlo chi-square test
chi_square_stat, p_value_mc = monte_carlo_chi_square(observed)

# Print results
print("Chi-square Statistic:", chi_square_stat)
print("Monte Carlo P-value:", p_value_mc)

if p_value_mc <= alpha:
    print("As p_value <= 0.05, we reject the null hypothesis: service type influences attack_category.")
else:
    print("As p_value > 0.05, we cannot reject the null hypothesis: service type does not influence attack_category.")

"""Observation

    As chi2_contingency assumptions are failed, we have performed the chi-square test with Monte Carlo simulation. It suggests that service type influences attack_category.

**# ERROR_FLAG VS ANOMALIES & URGENT VS ANOMALIES**


    Does Error Flags in the Flag Feature Are Significantly Associated with Anomalies?
    Are Connections that Include Urgent Packets More Likely to Be Anomalous?

4.4.1 Error_flag vs Attack_or_normal & Urgent vs Attack_or_normal

Selection of Appropriate Test

For finding the association between "error_flag_or_not" & "attack_or_normal" and "urgent_or_not" & "attack_or_normal" features, we are applying statsmodels logit method to get the p-value using the Maximum Likelihood Estimation method.
"""

nadp_logit = nadp_add.copy(deep = True)
# Should create a new flag feature where all flag except "SF" are treated as error flags. Lets name it as "error_flag_or_not"
nadp_logit["error_flag_or_not"] = nadp_logit["flag"].apply(lambda x: 0 if x == "SF" else 1)
# Should create a new urgent feature where atleast one urgent activated is 1 and not activated is 0
nadp_logit["urgent_or_not"] = nadp_logit["urgent"].apply(lambda x: 0 if x == 0 else 1)

# Pre processing required for applying logistic regression


# lets drop flag_category and flag as there is error_flag_or_not feature
# lets drop urgent as there is urgent_or_not feature
# lets drop attack and attack_category features as we are taking attack_or_normal feature as Binary target variable here
# lets drop lastflag feature also as it is providing the information about success of classification models
nadp_logit.drop(["flag_category","attack","attack_category","flag","lastflag","urgent"],axis= 1,inplace=True)

# ENCODING THE CATEGORICAL FEATURES
# Calculate the mean of the target for each category
protocoltype_mean = nadp_logit.groupby('protocoltype')['attack_or_normal'].mean()
service_category_mean = nadp_logit.groupby('service_category')['attack_or_normal'].mean()

# Map the mean encoding back to the original nadp_logit
nadp_logit['protocoltype'] = nadp_logit['protocoltype'].map(protocoltype_mean)
nadp_logit['service_category'] = nadp_logit['service_category'].map(service_category_mean)

# Get value counts and sort
service_value_counts = nadp_logit['service'].value_counts()

# Create a mapping from category to its rank based on frequency
encoding = {category: rank for rank, category in enumerate(service_value_counts.index)}

# Apply the encoding
nadp_logit['service'] = nadp_logit['service'].map(encoding)

# SCALING
# Assuming nadp_logit is your original DataFrame and you want to scale all but the target variable
X = nadp_logit.drop(["attack_or_normal"], axis=1)  # Features
y = nadp_logit["attack_or_normal"]  # Target variable

# Create a StandardScaler object
scaler = StandardScaler()

# Fit the scaler to the features and transform them
X_scaled = scaler.fit_transform(X)

# Convert the scaled features back to a DataFrame
nadp_logit_scaled = pd.DataFrame(X_scaled, columns=X.columns)

"""Hypothesis Formulation

  1. error_flag_or_not vs attack_or_normal
    Null Hypothesis H0: There is no significant association between error_flag_or_not and attack_or_normal.
    Alternate Hypothesis Ha: There is a significant association between error_flag_or_not and attack_or_normal.
    Significance level: 0.05
  2. urgent_or_not vs attack_or_normal
    Null Hypothesis H0: There is no significant association between urgent_or_not and attack_or_normal.
    Alternate Hypothesis Ha: There is a significant association between urgent_or_not and attack_or_normal.
    Significance level: 0.05

Applying Statsmodels logit function
"""

# Independent variables
X = sm.add_constant(nadp_logit_scaled)  # Adds a constant term to the predictor

# Fit the model
model = sm.Logit(y, X)
result = model.fit()

# Print the summary
print(result.summary())

# Hypothesis test decision
alpha = 0.05  # Significance level

# Checking p_value
error_flag_p_value = result.pvalues['error_flag_or_not']
urgent_p_value = result.pvalues["urgent_or_not"]

if error_flag_p_value < alpha:
    print("Reject H0: There is a significant association between error_flag_or_not and attack_or_normal.")
else:
    print("Accept H0: There is no significant association between error_flag_or_not and attack_or_normal.")

if urgent_p_value < alpha:
    print("Reject H0: There is a significant association between urgent_or_not and attack_or_normal.")
else:
    print("Accept H0: There is no significant association between urgent_or_not and attack_or_normal.")

"""Observation
Before applying VIF

  1. There is a significant association between error_flag_or_not and attack_or_normal.
  2. There is no significant association between urgent_or_not and attack_or_normal.

Check the assumptions of logistic regression
The two-sample independent t-test assumes the following characteristics about the data:

  1. Independence: The observations in each sample must be independent of each other. This means that the value of one observation should not be related to the value of any other observation in the same sample.
  2. Binary Outcome: By default, logistic regression assumes that the outcome variable is binary.
  3. Absence of Multicollinearity: Multicollinearity corresponds to a situation where the data contain highly correlated independent variables. This is a problem because it reduces the precision of the estimated coefficients, which weakens the statistical power of the logistic regression model.

Independence

    Every sample in the nadp_logit dataset is independent of each other.

Binary Outcome

    attack_or_normal feature is a binary feature.

Absence of Multi-collinearity
"""

def calculate_vif(X):
    vif = pd.DataFrame()
    vif["Features"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif

def remove_worst_feature(X):
    vif = calculate_vif(X)
    vif["VIF"] = round(vif["VIF"], 2)
    vif = vif.sort_values(by="VIF", ascending=False)

    # Check if all VIF values are less than 10
    if vif["VIF"].max() < 10:
        return X  # Stop if all VIFs are acceptable

    # Identify the worst feature, skipping 'error_flag_or_not' and 'urgent_or_not' if necessary
    for i in range(len(vif)):
        worst_feature = vif["Features"].iloc[i]
        if worst_feature not in ["error_flag_or_not", "urgent_or_not"]:
            print(f"Removing feature: {worst_feature} with VIF: {vif['VIF'].iloc[i]}")
            return remove_worst_feature(X.drop(columns=[worst_feature]))

        # Skip if the worst feature is 'error_flag_or_not' or 'urgent_or_not'
        print(f"Skipping removal of '{worst_feature}' with VIF: {vif['VIF'].iloc[i]}")

    # If only 'error_flag_or_not' and 'urgent_or_not' remain with high VIF, return without removal
    print("No more removable features with VIF >= 10 that are not 'error_flag_or_not' or 'urgent_or_not'")
    return X

# Example usage
X_t = nadp_logit_scaled.copy(deep=True)  # Assuming nadp_logit is your original DataFrame
X_reduced = remove_worst_feature(X_t)

# The reduced dataset will have all VIFs < 10 except for 'error_flag_or_not' and 'urgent_or_not'
print("Final features after VIF removal:", X_reduced.columns)

vif = calculate_vif(nadp_logit_scaled[X_reduced.columns])
vif["VIF"] = round(vif["VIF"], 2)
vif = vif.sort_values(by="VIF", ascending=False)
vif

# Applying statsmodels logit after removing vif > 10 features

# Independent variables
X = sm.add_constant(nadp_logit_scaled[X_reduced.columns])  # Adds a constant term to the predictor

# Dependent variable
y = y

# Fit the model
model = sm.Logit(y, X)
result = model.fit()

# Print the summary
print(result.summary())

# Hypothesis test decision
alpha = 0.05  # Significance level

# Checking p_value
error_flag_p_value = result.pvalues['error_flag_or_not']
urgent_p_value = result.pvalues["urgent_or_not"]

if error_flag_p_value < alpha:
    print("Reject H0: There is a significant association between error_flag_or_not and attack_or_normal.")
else:
    print("Accept H0: There is no significant association between error_flag_or_not and attack_or_normal.")

if urgent_p_value < alpha:
    print("Reject H0: There is a significant association between urgent_or_not and attack_or_normal.")
else:
    print("Accept H0: There is no significant association between urgent_or_not and attack_or_normal.")

"""Observation
After applying VIF

  1. There is a significant association between error_flag_or_not and attack_or_normal.
  2. There is a significant association between urgent_or_not and attack_or_normal.

Quasi-Separation
Quasi-separation is observed in the above summary. Quasi-separation in logistic regression, particularly in statsmodels’ Logit, occurs when a predictor variable almost perfectly predicts the outcome but not in every case. This means there is an alignment where one category of the predictor variable almost exclusively predicts one outcome in the target, though there are some exceptions.

In practical terms, quasi-separation can cause issues because logistic regression coefficients may become extremely large, leading to instability and difficulties in estimating standard errors. This often results in warnings or errors in statistical software, indicating convergence issues.

wrongfragment and ishostlogin have very high coefficient values compared to other feature coefficients. In these cases, regularization may be helpful to mitigate the effect of quasi-separation

# **MACHINE LEARNING MODELING FOR BINARY CLASSIFICATION**
"""

# Data preprocessing for ML modeling

# Removing unwanted features and rows

# Feature Engineering
nadp_binary = nadp_add.copy(deep=True)

# Remove the attack_category, attack and last_flag features
# Remove hierarchical features like service_category, flag_category
nadp_binary = nadp_binary.drop(["attack_category","attack","lastflag","service_category","flag_category"],axis = 1)

# Checking null values
sum(nadp_binary.isna().sum())

# Checking duplicats after removing unwanted features
nadp_binary.duplicated().sum()

# Drop duplicates
nadp_binary.drop_duplicates(keep="first",inplace=True)

"""## Train Test Split and handling duplicates"""

# Train Test Split and handling duplicates

# Seperate features and target
nadp_X_binary = nadp_binary.drop(["attack_or_normal"], axis=1)  # Features
nadp_y_binary = nadp_binary["attack_or_normal"]  # Target variable

# Split the data with stratification
nadp_X_train_binary, nadp_X_test_binary, nadp_y_train_binary, nadp_y_test_binary = train_test_split(nadp_X_binary, nadp_y_binary, test_size=0.2, random_state=42, stratify=nadp_y_binary)

# Verify the value counts in train and test sets
print("Training set value counts:\n", nadp_y_train_binary.value_counts())
print("Test set value counts:\n", nadp_y_test_binary.value_counts())

# three categorical features
nadp_X_train_binary.describe(include = "object")

# checking duplicates after train test split
nadp_X_train_binary[nadp_X_train_binary.duplicated(keep=False)]

# checking duplicates related nadp_y_train_binary
nadp_y_train_binary[nadp_X_train_binary.duplicated(keep=False)]

# REMOVING DUPLICATES WHOSE nadp_y_train_binary == 0 (keeping attacked rows)
# Create a boolean mask for y_train where the value is 0
mask_y_equals_zero = nadp_y_train_binary == 0

# Identify duplicates in X_train where y_train is 0
duplicates_mask = nadp_X_train_binary.duplicated(keep=False)

# Combine both masks to identify the rows to keep
rows_to_keep = nadp_X_train_binary[~(duplicates_mask & mask_y_equals_zero)]

# Remove duplicates from X_train and corresponding values in y_train
nadp_X_train_binary = nadp_X_train_binary.loc[rows_to_keep.index]
nadp_y_train_binary = nadp_y_train_binary.loc[rows_to_keep.index]

# Optionally, reset the index
nadp_X_train_binary.reset_index(drop=True, inplace=True)
nadp_y_train_binary.reset_index(drop=True, inplace=True)

# Final check of duplicates
nadp_X_train_binary.duplicated().sum()

# Encoding the categorical features

# Encode all the category features except service feature with One hot encoding as their nunique is not large.

# # For service feature, use label encoding in the descending order of their value counts.

# Encode Train dataset first, then use those stats to encode test dataset to avoid target data leak.

# Train Dataset Encoding

nadp_X_train_binary_encoded = nadp_X_train_binary.copy(deep=True)

# Initialize OneHotEncoder
ohe_encoder = OneHotEncoder(drop="first",sparse_output=False)  # Drop first to avoid multicollinearity

# Fit and transform the selected columns
encoded_data = ohe_encoder.fit_transform(nadp_X_train_binary_encoded[['protocoltype', 'flag']])

# Convert to DataFrame with proper column names
encoded_nadp_X_train_binary_encoded = pd.DataFrame(encoded_data, columns=ohe_encoder.get_feature_names_out(['protocoltype', 'flag']))

# reset index
nadp_X_train_binary_encoded = nadp_X_train_binary_encoded.reset_index(drop=True)
encoded_nadp_X_train_binary_encoded = encoded_nadp_X_train_binary_encoded.reset_index(drop=True)

# Combine the original DataFrame with the encoded DataFrame
nadp_X_train_binary_encoded = pd.concat([nadp_X_train_binary_encoded.drop(columns=['protocoltype', 'flag']), encoded_nadp_X_train_binary_encoded], axis=1)

# Get value counts from the training set and create encoding for 'service'
service_value_counts = nadp_X_train_binary_encoded['service'].value_counts()
service_encoding = {category: rank for rank, category in enumerate(service_value_counts.index)}
nadp_X_train_binary_encoded['service'] = nadp_X_train_binary_encoded['service'].map(service_encoding)

sum(nadp_X_train_binary_encoded.isna().sum())

# Storing the ohe_encoder & service_encoding into pickle file for future use

# Save OneHotEncoder
with open('ohe_encoder.pkl', 'wb') as f:
    pickle.dump(ohe_encoder, f)

# Save service encoding mapping
with open('service_encoding.pkl', 'wb') as f:
    pickle.dump(service_encoding, f)

# Test Dataset Encoding

# Assuming nadp_X_test_binary is your test dataset
nadp_X_test_binary_encoded = nadp_X_test_binary.copy(deep=True)

# Transform 'protocoltype' and 'flag' columns using the fitted OneHotEncoder
encoded_test_data = ohe_encoder.transform(nadp_X_test_binary_encoded[['protocoltype', 'flag']])
encoded_nadp_X_test_binary_encoded = pd.DataFrame(encoded_test_data, columns=ohe_encoder.get_feature_names_out(['protocoltype', 'flag']))

# Reset index for both DataFrames
encoded_nadp_X_test_binary_encoded = encoded_nadp_X_test_binary_encoded.reset_index(drop=True)
nadp_X_test_binary_encoded = nadp_X_test_binary_encoded.reset_index(drop=True)

# Combine the original DataFrame with the encoded DataFrame
nadp_X_test_binary_encoded = pd.concat([nadp_X_test_binary_encoded.drop(columns=['protocoltype', 'flag']), encoded_nadp_X_test_binary_encoded], axis=1)

# Apply frequency encoding for 'service' in the test dataset
nadp_X_test_binary_encoded['service'] = nadp_X_test_binary_encoded['service'].map(service_encoding)

# For any new service types in the test dataset that weren't in the training set, assign max + 1
max_service_value = nadp_X_train_binary_encoded['service'].max()
nadp_X_test_binary_encoded['service'].fillna(max_service_value + 1, inplace=True)

sum(nadp_X_test_binary_encoded.isna().sum())

"""**Standard Scaling**"""

# Train Dataset Scaling

# SCALING
# Create a StandardScaler object
nadp_X_train_binary_scaler = StandardScaler()

# Fit the scaler to the training features and transform them
nadp_X_train_binary_scaled = nadp_X_train_binary_scaler.fit_transform(nadp_X_train_binary_encoded)

# Convert the scaled training features back to a DataFrame
nadp_X_train_binary_scaled = pd.DataFrame(nadp_X_train_binary_scaled, columns=nadp_X_train_binary_encoded.columns)

# Test Dataset Scaling

# Scale the test features using the same scaler
nadp_X_test_binary_scaled = nadp_X_train_binary_scaler.transform(nadp_X_test_binary_encoded)

# Convert the scaled test features back to a DataFrame
nadp_X_test_binary_scaled = pd.DataFrame(nadp_X_test_binary_scaled, columns=nadp_X_test_binary_encoded.columns)

# Storing the nadp_X_train_binary_scaler into pickle file for future use

# Save the scaler to a file
with open('nadp_X_train_binary_scaler.pkl', 'wb') as file:
    pickle.dump(nadp_X_train_binary_scaler, file)

# Removing Multi-collinear features using VIF

def calculate_vif(X):
    vif = pd.DataFrame()
    vif["Features"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif

def remove_worst_feature(X):
    vif = calculate_vif(X)
    vif["VIF"] = round(vif["VIF"], 2)
    vif = vif.sort_values(by="VIF", ascending=False)

    # Check if all VIF values are less than 10
    if vif["VIF"].max() < 10:
        return X  # Stop if all VIFs are acceptable

    # Remove the feature with the highest VIF
    worst_feature = vif["Features"].iloc[0]
    print(f"Removing feature: {worst_feature} with VIF: {vif['VIF'].iloc[0]}")

    # Recursively call the function with the reduced dataset
    return remove_worst_feature(X.drop(columns=[worst_feature]))

# VIF should be applied only among continuous features
X_t = nadp_X_train_binary_scaled[['duration', 'srcbytes', 'dstbytes', 'wrongfragment',
       'urgent', 'hot', 'numfailedlogins', 'numcompromised', 'numroot', 'numfilecreations', 'numshells',
       'numaccessfiles', 'count', 'srvcount','serrorrate', 'srvserrorrate', 'rerrorrate', 'srvrerrorrate',
       'samesrvrate', 'diffsrvrate', 'srvdiffhostrate', 'dsthostcount',
       'dsthostsrvcount', 'dsthostsamesrvrate', 'dsthostdiffsrvrate',
       'dsthostsamesrcportrate', 'dsthostsrvdiffhostrate', 'dsthostserrorrate',
       'dsthostsrvserrorrate', 'dsthostrerrorrate', 'dsthostsrvrerrorrate',
       'serrors_count', 'rerrors_count', 'samesrv_count', 'diffsrv_count',
       'serrors_srvcount', 'rerrors_srvcount', 'srvdiffhost_srvcount',
       'dsthost_serrors_count', 'dsthost_rerrors_count',
       'dsthost_samesrv_count', 'dsthost_diffsrv_count',
       'dsthost_serrors_srvcount', 'dsthost_rerrors_srvcount',
       'dsthost_samesrcport_srvcount', 'dsthost_srvdiffhost_srvcount',
       'srcbytes/sec', 'dstbytes/sec']]
VIF_reduced = remove_worst_feature(X_t)

# The reduced dataset will have all VIFs < 10
print("Final features after VIF removal:", VIF_reduced.columns)

VIF_reduced_columns = ['duration', 'srcbytes', 'dstbytes', 'wrongfragment', 'urgent', 'hot',
       'numfailedlogins', 'numcompromised', 'numfilecreations', 'numshells',
       'numaccessfiles', 'serrorrate', 'rerrorrate', 'diffsrvrate',
       'srvdiffhostrate', 'dsthostcount', 'dsthostsrvcount',
       'dsthostdiffsrvrate', 'dsthostsamesrcportrate',
       'dsthostsrvdiffhostrate', 'serrors_count', 'rerrors_count',
       'samesrv_count', 'diffsrv_count', 'serrors_srvcount',
       'rerrors_srvcount', 'srvdiffhost_srvcount', 'dsthost_rerrors_count',
       'dsthost_samesrv_count', 'dsthost_serrors_srvcount',
       'dsthost_rerrors_srvcount', 'dsthost_samesrcport_srvcount',
       'dsthost_srvdiffhost_srvcount', 'srcbytes/sec', 'dstbytes/sec']
cat_features = ['flag_REJ','flag_RSTO', 'flag_RSTOS0', 'flag_RSTR', 'flag_S0',
                'flag_S1', 'flag_S2', 'flag_S3', 'flag_SF', 'flag_SH', 'isguestlogin',
                'ishostlogin','land', 'loggedin', 'protocoltype_tcp', 'protocoltype_udp',
                  'rootshell','service', 'suattempted']
final_selected_features = VIF_reduced_columns + cat_features
print(f"Number of final_selected_features : {len(final_selected_features)}")
print(f"Number of features removed by VIF : {nadp_X_train_binary_scaled.shape[1] - len(final_selected_features)}")

vif = calculate_vif(nadp_X_train_binary_scaled[VIF_reduced_columns])
vif["VIF"] = round(vif["VIF"], 2)
vif = vif.sort_values(by="VIF", ascending=False)
vif

# Filter both the training and test datasets to keep only the selected features
nadp_X_train_binary_final = nadp_X_train_binary_scaled[final_selected_features]
nadp_X_test_binary_final = nadp_X_test_binary_scaled[final_selected_features]
nadp_y_train_binary_final = nadp_y_train_binary.copy(deep = True)
nadp_y_test_binary_final = nadp_y_test_binary.copy(deep = True)

# For deployment purpose
nadp_X_train_binary_final.to_pickle('nadp_X_train_binary_final.pkl', compression='gzip')

"""**Visualise the nadp_X_train_binary_final and nadp_X_test_binary_final ground truth groups using UMAP**"""

# create_binary_umap function
from matplotlib.colors import ListedColormap
!pip install mlflow
import mlflow

binary_cmap = ListedColormap(sns.husl_palette(len(np.unique(nadp_y_train_binary_final))))
binary_train_umap = UMAP(init='random',n_neighbors=200,min_dist=0.1, random_state=42,n_jobs=-1).fit_transform(nadp_X_train_binary_final)
binary_test_umap = UMAP(init='random',n_neighbors=200,min_dist=0.1, random_state=42,n_jobs=-1).fit_transform(nadp_X_test_binary_final)

def create_binary_umap(binary_train_umap,nadp_y_train_binary_final,binary_test_umap,nadp_y_test_binary_final,run_name):
    # Create a figure with 1 row and 2 columns
    fig, axs = plt.subplots(1, 2, figsize=(12, 6))

    # Plot the training data
    im_train = axs[0].scatter(binary_train_umap[:, 0],
                            binary_train_umap[:, 1],
                            s=25,
                            c=np.array(nadp_y_train_binary_final),
                            cmap=binary_cmap,
                            edgecolor='none')
    axs[0].set_title('Train Data UMAP')
    axs[0].set_xlabel('UMAP Dimension 1')
    axs[0].set_ylabel('UMAP Dimension 2')



    # Plot the test data
    im_test = axs[1].scatter(binary_test_umap[:, 0],
                            binary_test_umap[:, 1],
                            s=25,
                            c=np.array(nadp_y_test_binary_final),
                            cmap= binary_cmap,
                            edgecolor='none')
    axs[1].set_title('Test Data UMAP')
    axs[1].set_xlabel('UMAP Dimension 1')
    axs[1].set_ylabel('UMAP Dimension 2')
    # Add colorbar for training data
    cbar_train = fig.colorbar(im_train, ax=axs[1], label='attack_or_normal')

    # Display the plots
    plt.tight_layout()
    plt.savefig(f"{run_name}_umap.png")
    mlflow.log_artifact(f"{run_name}_umap.png")
    plt.show()

# Original nadp_binary_ground_truth_umap

# logging the binary_ground_truth_umap.png artifact into mlflow
experiment_name = "nadp_binary"
run_name = "binary_ground_truth_umap"
#mlflow.create_experiment("nadp_binary")
mlflow.set_experiment("nadp_binary")

with mlflow.start_run(run_name=run_name):
    try:
        # log umap
        create_binary_umap(binary_train_umap,nadp_y_train_binary_final,binary_test_umap,nadp_y_test_binary_final,run_name)
        print("MLFLOW Logging is completed")
    except Exception as e:
        print(f"Error in mlflow_logging_and_metric_printing: {e}")

"""**Creating additional features using scores from Anomaly Detection Algorithms (Unsupervised)**"""

# Creating new dataframes to store the anomaly_scores
nadp_X_train_binary_anomaly = nadp_X_train_binary_final.copy(deep = True)
nadp_X_test_binary_anomaly = nadp_X_test_binary_final.copy(deep = True)

# Local Outlier Factor

# logging the binary_lof_umap.png artifact into mlflow
experiment_name = "nadp_binary"
run_name = "binary_lof"
#mlflow.create_experiment("nadp_binary")
mlflow.set_experiment("nadp_binary")

with mlflow.start_run(run_name=run_name):
    try:
        # log params
        params = {"n_neighbors":20,"contamination":"auto","n_jobs": -1}
        mlflow.log_params(params)

        # Train dataset
        train_binary_lof = LocalOutlierFactor(**params)
        X_train_binary_lof_labels = train_binary_lof.fit_predict(nadp_X_train_binary_final)
        X_train_binary_lof_labels = np.where(X_train_binary_lof_labels == -1,1,0)
        nadp_X_train_binary_anomaly["binary_lof_nof"]= train_binary_lof.negative_outlier_factor_
        train_metrics = {"actual_n_neighbors":train_binary_lof.n_neighbors_,"offset":train_binary_lof.offset_}
        mlflow.log_metrics(train_metrics)

        # Test dataset
        test_binary_lof = LocalOutlierFactor(**params)
        X_test_binary_lof_labels = test_binary_lof.fit_predict(nadp_X_test_binary_final)
        X_test_binary_lof_labels = np.where(X_test_binary_lof_labels == -1,1,0)
        nadp_X_test_binary_anomaly["binary_lof_nof"]= test_binary_lof.negative_outlier_factor_
        test_metrics = {"actual_n_neighbors":test_binary_lof.n_neighbors_,"offset":test_binary_lof.offset_}
        mlflow.log_metrics(test_metrics)

        # log umap
        create_binary_umap(binary_train_umap,X_train_binary_lof_labels,binary_test_umap,X_test_binary_lof_labels,run_name)

        # Log the model
        mlflow.sklearn.log_model(train_binary_lof, f"{run_name}_train_model")
        mlflow.sklearn.log_model(test_binary_lof, f"{run_name}_test_model")
        print("MLFLOW Logging is completed")
    except Exception as e:
        print(f"Error in mlflow_logging_and_metric_printing: {e}")

# Isolation forest

# logging the binary_iforest_umap.png artifact into mlflow
experiment_name = "nadp_binary"
run_name = "binary_iforest"
#mlflow.create_experiment("nadp_binary")
mlflow.set_experiment("nadp_binary")

with mlflow.start_run(run_name=run_name):
    try:
        # log params
        params = {"n_estimators":100,"contamination":"auto","n_jobs": -1,"random_state":42,"verbose":0}
        mlflow.log_params(params)

        # Train dataset
        train_binary_iforest = IsolationForest(**params)
        X_train_binary_iforest_labels = train_binary_iforest.fit_predict(nadp_X_train_binary_final)
        X_train_binary_iforest_labels = np.where(X_train_binary_iforest_labels == -1,1,0)
        nadp_X_train_binary_anomaly["binary_iforest_df"]= train_binary_iforest.decision_function(nadp_X_train_binary_final)
        train_metrics = {"n_estimators":len(train_binary_iforest.estimators_),"offset":train_binary_iforest.offset_}
        mlflow.log_metrics(train_metrics)

        # Test dataset
        X_test_binary_iforest_labels = train_binary_iforest.predict(nadp_X_test_binary_final)
        X_test_binary_iforest_labels = np.where(X_test_binary_iforest_labels == -1,1,0)
        nadp_X_test_binary_anomaly["binary_iforest_df"]= train_binary_iforest.decision_function(nadp_X_test_binary_final)

        # log umap
        create_binary_umap(binary_train_umap,X_train_binary_iforest_labels,binary_test_umap,X_test_binary_iforest_labels,run_name)

        # Log the model
        mlflow.sklearn.log_model(train_binary_iforest, f"{run_name}_train_model")
        print("MLFLOW Logging is completed")
    except Exception as e:
        print(f"Error in mlflow_logging_and_metric_printing: {e}")

# Saving the pkl for purpose of deployment
params = {"n_estimators":100,"contamination":"auto","n_jobs": -1,"random_state":42,"verbose":0}
train_binary_iforest = IsolationForest(**params)
train_binary_iforest.fit(nadp_X_train_binary_final)
with open("train_binary_iforest.pkl","wb") as f:
    pickle.dump(train_binary_iforest,f)

# Elliptic Envelope

# logging the binary_robust_cov_umap.png artifact into mlflow
experiment_name = "nadp_binary"
run_name = "binary_robust_cov"
#mlflow.create_experiment("nadp_binary")
mlflow.set_experiment("nadp_binary")

with mlflow.start_run(run_name=run_name):
    try:
        # Suppress warnings
        warnings.filterwarnings('ignore')

        # log params
        params = {"contamination":0.1,"random_state":42}
        mlflow.log_params(params)

        # Train dataset
        train_binary_robust_cov = EllipticEnvelope(**params)
        X_train_binary_robust_cov_labels = train_binary_robust_cov.fit_predict(nadp_X_train_binary_final)
        X_train_binary_robust_cov_labels = np.where(X_train_binary_robust_cov_labels == -1,1,0)
        nadp_X_train_binary_anomaly["binary_robust_cov_df"]= train_binary_robust_cov.decision_function(nadp_X_train_binary_final)
        train_metrics = {"offset":train_binary_robust_cov.offset_}
        mlflow.log_metrics(train_metrics)

        # Test dataset
        X_test_binary_robust_cov_labels = train_binary_robust_cov.predict(nadp_X_test_binary_final)
        X_test_binary_robust_cov_labels = np.where(X_test_binary_robust_cov_labels == -1,1,0)
        nadp_X_test_binary_anomaly["binary_robust_cov_df"]= train_binary_robust_cov.decision_function(nadp_X_test_binary_final)

        # log umap
        create_binary_umap(binary_train_umap,X_train_binary_robust_cov_labels,binary_test_umap,X_test_binary_robust_cov_labels,run_name)

        # Log the model
        mlflow.sklearn.log_model(train_binary_robust_cov, f"{run_name}_train_model")
        print("MLFLOW Logging is completed")
    except Exception as e:
        print(f"Error in mlflow_logging_and_metric_printing: {e}")

params = {"contamination":0.1,"random_state":42}
train_binary_robust_cov = EllipticEnvelope(**params)
train_binary_robust_cov = train_binary_robust_cov.fit(nadp_X_train_binary_final)
with open("train_binary_robust_cov.pkl","wb") as f:
    pickle.dump(train_binary_robust_cov,f)

#  One Class SVM

# logging the binary_one_class_svm_umap.png artifact into mlflow
experiment_name = "nadp_binary"
run_name = "binary_one_class_svm"
#mlflow.create_experiment("nadp_binary")
mlflow.set_experiment("nadp_binary")

with mlflow.start_run(run_name=run_name):
    try:
        # Suppress warnings
        warnings.filterwarnings('ignore')

        # log params
        params = {"nu":0.1, "verbose":0}
        mlflow.log_params(params)

        # Train dataset
        train_binary_one_class_svm = OneClassSVM(**params)
        X_train_binary_one_class_svm_labels = train_binary_one_class_svm.fit_predict(nadp_X_train_binary_final)
        X_train_binary_one_class_svm_labels = np.where(X_train_binary_one_class_svm_labels == -1,1,0)
        nadp_X_train_binary_anomaly["binary_one_class_svm_df"]= train_binary_one_class_svm.decision_function(nadp_X_train_binary_final)
        train_metrics = {"offset":train_binary_one_class_svm.offset_,"n_support_vectors":len(train_binary_one_class_svm.support_vectors_)}
        mlflow.log_metrics(train_metrics)

        # Test dataset
        X_test_binary_one_class_svm_labels = train_binary_one_class_svm.predict(nadp_X_test_binary_final)
        X_test_binary_one_class_svm_labels = np.where(X_test_binary_one_class_svm_labels == -1,1,0)
        nadp_X_test_binary_anomaly["binary_one_class_svm_df"]= train_binary_one_class_svm.decision_function(nadp_X_test_binary_final)

        # log umap
        create_binary_umap(binary_train_umap,X_train_binary_one_class_svm_labels,binary_test_umap,X_test_binary_one_class_svm_labels,run_name)

        # Log the model
        mlflow.sklearn.log_model(train_binary_one_class_svm, f"{run_name}_train_model")
        print("MLFLOW Logging is completed")
    except Exception as e:
        print(f"Error in mlflow_logging_and_metric_printing: {e}")

"""params = {"nu":0.1, "verbose":0}
train_binary_one_class_svm = OneClassSVM(**params)
train_binary_one_class_svm = train_binary_one_class_svm.fit(nadp_X_train_binary_final)
with open("train_binary_one_class_svm.pkl","wb") as f:
    pickle.dump(train_binary_one_class_svm,f)
"""

#  DBSCAN

# logging the binary_dbscan_umap.png artifact into mlflow
experiment_name = "nadp_binary"
run_name = "binary_dbscan"
#mlflow.create_experiment("nadp_binary")
mlflow.set_experiment("nadp_binary")

with mlflow.start_run(run_name=run_name):
    try:
        # Suppress warnings
        warnings.filterwarnings('ignore')

        # log params
        params = {"eps":0.5, "min_samples":5, "n_jobs":-1}
        mlflow.log_params(params)

        # Train dataset
        train_binary_dbscan = DBSCAN(**params)
        X_train_binary_dbscan_labels = train_binary_dbscan.fit_predict(nadp_X_train_binary_final)
        X_train_binary_dbscan_labels = np.where(X_train_binary_dbscan_labels == -1,1,0)
        nadp_X_train_binary_anomaly["binary_dbscan_labels"]= train_binary_dbscan.labels_
        train_metrics = {"n_unique_labels":len(np.unique(train_binary_dbscan.labels_))}
        mlflow.log_metrics(train_metrics)

        # Test dataset
        test_binary_dbscan = DBSCAN(**params)
        X_test_binary_dbscan_labels = test_binary_dbscan.fit_predict(nadp_X_test_binary_final)
        X_test_binary_dbscan_labels = np.where(X_test_binary_dbscan_labels == -1,1,0)
        nadp_X_test_binary_anomaly["binary_dbscan_labels"]= test_binary_dbscan.labels_
        test_metrics = {"n_unique_labels":len(np.unique(test_binary_dbscan.labels_))}
        mlflow.log_metrics(test_metrics)

        # log umap
        create_binary_umap(binary_train_umap,X_train_binary_dbscan_labels,binary_test_umap,X_test_binary_dbscan_labels,run_name)

        # Log the model
        mlflow.sklearn.log_model(train_binary_dbscan, f"{run_name}_train_model")
        mlflow.sklearn.log_model(test_binary_dbscan, f"{run_name}_test_model")
        print("MLFLOW Logging is completed")
    except Exception as e:
        print(f"Error in mlflow_logging_and_metric_printing: {e}")

# kth Nearest Neighbor distance

# logging the binary_knn_umap.png artifact into mlflow
experiment_name = "nadp_binary"
run_name = "binary_knn"
#mlflow.create_experiment("nadp_binary")
mlflow.set_experiment("nadp_binary")

with mlflow.start_run(run_name=run_name):
    try:
        # Suppress warnings
        warnings.filterwarnings('ignore')

        # log params
        params = {"n_neighbors":5,"n_jobs":-1}
        mlflow.log_params(params)

        # Train dataset
        train_binary_knn = NearestNeighbors(**params)
        train_binary_knn.fit(nadp_X_train_binary_final)
        train_distances, train_indices = train_binary_knn.kneighbors(nadp_X_train_binary_final)
        nadp_X_train_binary_anomaly["binary_knn_kth_distance"]= train_distances[:, -1]

        # Test dataset
        # test_binary_knn = NearestNeighbors(**params)
        # X_train_binary_knn_labels = test_binary_knn.fit(nadp_X_train_binary_final)
        test_distances, test_indices = train_binary_knn.kneighbors(nadp_X_test_binary_final)
        nadp_X_test_binary_anomaly["binary_knn_kth_distance"]= test_distances[:,-1]

        # log umap (No umap in knn because we cannot calculate labels in unsuperviced knn)
        # create_binary_umap(binary_train_umap,X_train_binary_knn_labels,binary_test_umap,X_test_binary_knn_labels,run_name)

        # Log the model
        mlflow.sklearn.log_model(train_binary_knn, f"{run_name}_train_model")
        print("MLFLOW Logging is completed")
    except Exception as e:
        print(f"Error in mlflow_logging_and_metric_printing: {e}")

params = {"n_neighbors":5,"n_jobs":-1}
train_binary_knn = NearestNeighbors(**params)
train_binary_knn = train_binary_knn.fit(nadp_X_train_binary_final)
with open("train_binary_knn.pkl","wb") as f:
    pickle.dump(train_binary_knn,f)

# GMM Score

# logging the binary_gmm_umap.png artifact into mlflow
experiment_name = "nadp_binary"
run_name = "binary_gmm"
#mlflow.create_experiment("nadp_binary")
mlflow.set_experiment("nadp_binary")

with mlflow.start_run(run_name=run_name):
    try:
        # Suppress warnings
        warnings.filterwarnings('ignore')

        # log params
        params = {"n_components":2, "random_state":42,"verbose":0}
        mlflow.log_params(params)

        # Train dataset
        train_binary_gmm = GaussianMixture(**params)
        X_train_binary_gmm_labels = train_binary_gmm.fit_predict(nadp_X_train_binary_final)
        # X_train_binary_gmm_labels = np.where(X_train_binary_gmm_labels == -1,1,0)
        nadp_X_train_binary_anomaly["binary_gmm_score"]= train_binary_gmm.score_samples(nadp_X_train_binary_final)
        train_metrics = {"AIC":train_binary_gmm.aic(nadp_X_train_binary_final),"BIC":train_binary_gmm.bic(nadp_X_train_binary_final)}
        mlflow.log_metrics(train_metrics)

        # Test dataset
        X_test_binary_gmm_labels = train_binary_gmm.predict(nadp_X_test_binary_final)
        # X_test_binary_gmm_labels = np.where(X_test_binary_gmm_labels == -1,1,0)
        nadp_X_test_binary_anomaly["binary_gmm_score"]= train_binary_gmm.score_samples(nadp_X_test_binary_final)

        # log umap
        create_binary_umap(binary_train_umap,X_train_binary_gmm_labels,binary_test_umap,X_test_binary_gmm_labels,run_name)

        # Log the model
        mlflow.sklearn.log_model(train_binary_gmm, f"{run_name}_train_model")
        print("MLFLOW Logging is completed")
    except Exception as e:
        print(f"Error in mlflow_logging_and_metric_printing: {e}")

from sklearn.mixture import GaussianMixture

params = {"n_components":2, "random_state":42,"verbose":0}
train_binary_gmm = GaussianMixture(**params)
train_binary_gmm = train_binary_gmm.fit(nadp_X_train_binary_final)
with open("train_binary_gmm.pkl","wb") as f:
    pickle.dump(train_binary_gmm,f)

"""**Creating an additional feature using kmeans and a new assumption**

Assumptions and Additional Hyper parameter definitions

ASSUMPTION
Normal data belongs to large, dense clusters, while anomalies belong to small, sparse clusters.

ADDITIONAL HYPER PARAMETERS USED
Alpha (α): This parameter determines the threshold for defining a cluster as small. It controls the minimum size below which a cluster is considered anomalous due to its small size. Specifically, if the size of a cluster is less than α * (N / k) (where N is the total number of data points and k is the number of clusters), that cluster is marked as anomalous.

Beta (β): This parameter sets the threshold for sparsity of a cluster. It is used to determine if a cluster is sparse compared to the median within-cluster sum of squares (i.e., the average distance of points within the cluster to their centroid). If the within-cluster average distance ε_i is greater than β * median(E) (where E is the set of within-cluster averages for all clusters), the cluster is marked as anomalous for being sparse.

Gamma (γ): This parameter defines the threshold for extreme density. It identifies clusters that are much denser than usual. If the within-cluster average distance ε_i is less than γ * median(E), the cluster is considered anomalous for being extremely dense.
"""

# Scaling the data with anomaly scores again

# Standardize the data
k_means_scaler = StandardScaler()
data = k_means_scaler.fit_transform(nadp_X_train_binary_anomaly)

# Save the scaler for future use
with open('k_means_scaler.pkl', 'wb') as f:
    pickle.dump(k_means_scaler, f)

# Hyper parameter tuning of alpha, beta, gamma

from itertools import combinations, product

def label_clusters(labels, centroids, points, alpha, beta, gamma):
    """
    Labels clusters as normal or anomaly based on size, density, and extreme density.
    """
    unique_labels = np.unique(labels[labels != -1])
    n_clusters = len(unique_labels)
    cluster_sizes = np.array([np.sum(labels == i) for i in unique_labels])
    N = len(points)
    anomaly_labels = np.full(labels.shape, 'normal')

    # Calculate within-cluster sum of squares
    within_cluster_sums = []
    for i in unique_labels:
        cluster_points = points[labels == i]
        centroid = centroids[i]
        sum_of_squares = np.sum(np.linalg.norm(cluster_points - centroid, axis=1)**2)
        within_cluster_sums.append(sum_of_squares / len(cluster_points) if len(cluster_points) > 0 else np.inf)

    median_within_sum = np.median(within_cluster_sums)

    # Label clusters based on the given conditions
    for i, label in enumerate(unique_labels):
        size = cluster_sizes[i]
        average_within_sum = within_cluster_sums[i]

        if size < alpha * (N / n_clusters):
            anomaly_labels[labels == label] = 'anomal'
        elif average_within_sum > beta * median_within_sum:
            anomaly_labels[labels == label] = 'anomal'
        elif average_within_sum < gamma * median_within_sum:
            anomaly_labels[labels == label] = 'anomal'

    return anomaly_labels

def clustering_methods(data, k_values, alpha, beta, gamma):
    results = {}

    for k in k_values:
        kmeans = KMeans(n_clusters=k, random_state=42).fit(data)
        labels = kmeans.labels_
        centroids = kmeans.cluster_centers_
        labeled_data = label_clusters(labels, centroids, data, alpha, beta, gamma)
        results[f'KMeans_k={k}'] = labeled_data

    return results

# Hyperparameter grid
alpha_values = [0.01,0.05,0.1,0.25,0.5,0.75]
beta_values = [0.5, 1.0, 1.5, 2.0]
gamma_values = [0.025,0.05,0.1,0.15,0.25]
k_values = [2, 4, 8, 16, 32, 64, 128, 256]

# True labels for accuracy calculation
true_labels = np.where(nadp_y_train_binary_final == 1, 'anomal', 'normal')

# Search for the best hyperparameters
best_accuracy = 0
best_params = None

for alpha, beta, gamma in product(alpha_values, beta_values, gamma_values):
    results = clustering_methods(data, k_values, alpha, beta, gamma)
    for method, predicted_labels in results.items():
        accuracy = accuracy_score(true_labels, predicted_labels)
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_params = (method, alpha, beta, gamma)
        print(f"method:{method}, accuracy: {accuracy}, alpha:{alpha}, beta:{beta}, gamma:{gamma}")

print(f"best_accuracy:{best_accuracy},best_params:{best_params}")

# Best kmeans model training and testing and creating additional feature `binary_kmeans_adv`

# Applying the best k_means_scaler and parameters on both train and test data
best_method, best_alpha, best_beta, best_gamma = (2,0.01,2.0,0.25)
data_train = k_means_scaler.transform(nadp_X_train_binary_anomaly)
data_test = k_means_scaler.transform(nadp_X_test_binary_anomaly)

# Re-run the best model using KMeans with the best parameters
kmeans_best = KMeans(n_clusters=best_method, random_state=42)
kmeans_best.fit(data_train)
train_labels = label_clusters(kmeans_best.labels_, kmeans_best.cluster_centers_, data_train, best_alpha, best_beta, best_gamma)
test_labels = label_clusters(kmeans_best.predict(data_test), kmeans_best.cluster_centers_, data_test, best_alpha, best_beta, best_gamma)

# Add the labels as a new feature
nadp_X_train_binary_anomaly["binary_kmeans_adv"] = np.where(train_labels == "anomal",1,0)
nadp_X_test_binary_anomaly["binary_kmeans_adv"] = np.where(test_labels == "anomal",1,0)

with open("kmeans_best.pkl","wb") as f:
    pickle.dump(kmeans_best,f)

# mlflow logging of kmeans_adv
# Log metrics and model using MLflow

!pip install mlflow
import mlflow

experiment_name = "nadp_binary"
run_name = "binary_kmeans_adv"
mlflow.set_experiment(experiment_name)

with mlflow.start_run(run_name=run_name):
    try:
        # Suppress warnings
        warnings.filterwarnings('ignore')

        # Log params
        params = {"alpha": best_alpha, "beta": best_beta, "gamma": best_gamma, "k": best_method.split('=')[-1]}
        mlflow.log_params(params)

        # Define true labels for accuracy calculation
        train_true_labels = np.where(nadp_y_train_binary_final == 1, 'anomal', 'normal')
        test_true_labels = np.where(nadp_y_test_binary_final == 1, 'anomal', 'normal')

        # Flatten labels for comparison
        train_labels_flat = np.where(train_labels == "anomal",1,0).flatten()
        test_labels_flat = np.where(test_labels == "anomal",1,0).flatten()
        train_true_labels_flat = train_true_labels.flatten()
        test_true_labels_flat = test_true_labels.flatten()

        # Check for consistent lengths
        if len(train_true_labels_flat) != len(train_labels_flat):
            print(f"Mismatch between train_true_labels and train_labels: {len(train_true_labels_flat)} != {len(train_labels_flat)}")
            raise ValueError("Labels have inconsistent lengths.")

        if len(test_true_labels_flat) != len(test_labels_flat):
            print(f"Mismatch between test_true_labels and test_labels: {len(test_true_labels_flat)} != {len(test_labels_flat)}")
            raise ValueError("Labels have inconsistent lengths.")

        # Calculate metrics
        train_accuracy = accuracy_score(train_true_labels_flat, train_labels_flat)
        mlflow.log_metric("train_accuracy", train_accuracy)

        test_accuracy = accuracy_score(test_true_labels_flat, test_labels_flat)
        mlflow.log_metric("test_accuracy", test_accuracy)

        mlflow.log_metric("silhouette_score_train", silhouette_score(data_train, kmeans_best.labels_))
        mlflow.log_metric("davies_bouldin_index_train", davies_bouldin_score(data_train, kmeans_best.labels_))

        # Supervised metrics comparing predictions with true labels
        mlflow.log_metric("fowlkes_mallows_index", fowlkes_mallows_score(train_true_labels_flat, train_labels_flat))
        mlflow.log_metric("adjusted_mutual_info", adjusted_mutual_info_score(train_true_labels_flat, train_labels_flat))
        mlflow.log_metric("adjusted_rand_score", adjusted_rand_score(train_true_labels_flat, train_labels_flat))
        mlflow.log_metric("normalized_mutual_info", normalized_mutual_info_score(train_true_labels_flat, train_labels_flat))
        mlflow.log_metric("homogeneity", homogeneity_score(train_true_labels_flat, train_labels_flat))
        mlflow.log_metric("completeness", completeness_score(train_true_labels_flat, train_labels_flat))
        mlflow.log_metric("v_measure", v_measure_score(train_true_labels_flat, train_labels_flat))

        # Create Pairwise Confusion Matrix
        pairwise_cm = pair_confusion_matrix(train_true_labels_flat, train_labels_flat)
        pairwise_cm_disp = ConfusionMatrixDisplay(pairwise_cm)
        pairwise_cm_path = f"{run_name}_pairwise_cm.png"
        pairwise_cm_disp.plot(cmap=plt.cm.Blues)
        plt.savefig(pairwise_cm_path)
        plt.show()
        plt.close()
        mlflow.log_artifact(pairwise_cm_path)

        # Create Contingency Matrix
        cont_matrix = contingency_matrix(train_true_labels_flat, train_labels_flat)
        cont_matrix_disp = ConfusionMatrixDisplay(cont_matrix)
        contingency_cm_path = f"{run_name}_contingency_cm.png"
        cont_matrix_disp.plot(cmap=plt.cm.Blues)
        plt.savefig(contingency_cm_path)
        plt.show()
        plt.close()
        mlflow.log_artifact(contingency_cm_path)

        # Compute learning curve data
        train_sizes, train_scores, test_scores = learning_curve(kmeans_best, data_train, np.array(train_labels_flat).reshape(-1), cv=5, scoring="accuracy", n_jobs=-1)
        train_scores_mean = np.mean(train_scores, axis=1)
        test_scores_mean = np.mean(test_scores, axis=1)

        # Plot the learning curve
        plt.figure()
        plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training Score")
        plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation Score")
        plt.title(f"Learning Curve - {run_name}")
        plt.xlabel("Training Examples")
        plt.ylabel("Accuracy")
        plt.legend(loc="best")
        plt.grid()

        # Save the plot
        learning_curve_path = f"{run_name}_learning_curve.png"
        plt.savefig(learning_curve_path)
        plt.show()
        plt.close()

        # Log the plot as an artifact in MLflow
        mlflow.log_artifact(learning_curve_path)

        # Log umap
        create_binary_umap(binary_train_umap, train_labels_flat, binary_test_umap, test_labels_flat, run_name)

        # Log the trained model
        mlflow.sklearn.log_model(kmeans_best, f"{run_name}_train_model")
        print("MLFLOW Logging is completed")

    except Exception as e:
        print(f"Error in mlflow_logging_and_metric_printing: {e}")

from IPython.display import display

def display_all(df):
    """
    Displays all rows and columns of a Pandas DataFrame.
    """
    with pd.option_context("display.max_rows", None, "display.max_columns", None):
        display(df)

display_all(nadp_X_train_binary_anomaly.head())

# Final scaling before classification algorithms

kmeans_adv_scaler = StandardScaler()
nadp_X_train_binary_anomaly_final = kmeans_adv_scaler.fit_transform(nadp_X_train_binary_anomaly)
nadp_X_test_binary_anomaly_final = kmeans_adv_scaler.transform(nadp_X_test_binary_anomaly)

# Save the scaler for future use
with open('kmeans_adv_scaler.pkl', 'wb') as f:
    pickle.dump(kmeans_adv_scaler, f)

nadp_X_train_binary_anomaly_final = pd.DataFrame(nadp_X_train_binary_anomaly_final,columns=nadp_X_train_binary_anomaly.columns)
nadp_X_test_binary_anomaly_final = pd.DataFrame(nadp_X_test_binary_anomaly_final,columns=nadp_X_test_binary_anomaly.columns)

# Modifying the dataset names for easeness during ML Model experimentation

nadp_X_train_binary_anomaly_final.to_csv("nadp_X_train_binary_anomaly_final.csv",index = False)
nadp_X_test_binary_anomaly_final.to_csv("nadp_X_test_binary_anomaly_final.csv",index = False)

# Save nadp_y_train_binary_final and nadp_y_test_binary_final to CSV files
nadp_y_train_binary_final.to_csv("nadp_y_train_binary_final.csv", index=False)
nadp_y_test_binary_final.to_csv("nadp_y_test_binary_final.csv", index=False)

X_train_imb = pd.read_csv("nadp_X_train_binary_anomaly_final.csv")
X_test_imb = pd.read_csv("nadp_X_test_binary_anomaly_final.csv")

y_train_imb = pd.read_csv("nadp_y_train_binary_final.csv").squeeze()
y_test_imb = pd.read_csv("nadp_y_test_binary_final.csv").squeeze()

# Creating Balanced datasets also to compare

# SMOTE balancing
smt = SMOTE(random_state=42)
X_train_bal , y_train_bal = smt.fit_resample(X_train_imb,y_train_imb)
X_test_bal = X_test_imb.copy(deep = True)
y_test_bal = y_test_imb.copy(deep = True)

print("X_train_bal shape",X_train_bal.shape)
print("X_test_bal shape",X_test_bal.shape)
print("y_train_bal shape",y_train_bal.shape)
print("y_test_bal shape",y_test_bal.shape)
print("y_train_bal value_counts", y_train_bal.value_counts())
print("y_test_bal value_counts", y_test_bal.value_counts())

# Save the scaler for future use
with open('binary_smote.pkl', 'wb') as f:
    pickle.dump(smt, f)

"""**CREATING ALL THE REQUIRED FUNCTIONS TO LOG METRICS, PARAMS, ARTIFACTS, PLOTS INTO MLFLOW AND ALSO PRINT THEM**"""

#

# auc_plots function

def auc_plots(model, X, y):
    try:
        y_pred_prob = model.predict_proba(X)[:, 1]  # Consider only positive class
        fpr, tpr, _ = roc_curve(y, y_pred_prob)
        pr, re, _ = precision_recall_curve(y, y_pred_prob)
        roc_auc = auc(fpr, tpr)
        pr_auc = auc(re, pr)
        return fpr, tpr, pr, re, roc_auc, pr_auc
    except Exception as e:
        print(f"Error in auc_plots: {e}")
        return None, None, None, None, None, None

#  plot_learning_curve function

def plot_learning_curve(model, X, y, run_name):
    try:
        train_sizes, train_scores, validation_scores = learning_curve(model, X, y, cv=5, n_jobs=-1)
        train_mean = train_scores.mean(axis=1)
        train_std = train_scores.std(axis=1)
        validation_mean = validation_scores.mean(axis=1)
        validation_std = validation_scores.std(axis=1)

        plt.figure(figsize=(10, 6))
        plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training score')
        plt.plot(train_sizes, validation_mean, 'o-', color='red', label='Validation score')
        plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
        plt.fill_between(train_sizes, validation_mean - validation_std, validation_mean + validation_std, alpha=0.1, color='red')

        plt.xlabel('Training examples')
        plt.ylabel('Score')
        plt.title('Learning Curve')
        plt.legend(loc='best')
        plt.grid(True)

        # Save the learning curve plot
        plt.savefig(f"{run_name}_learning_curve.png")
        plt.show()
        plt.close()
        return f"{run_name}_learning_curve.png"

    except Exception as e:
        print(f"Error in plot_learning_curve: {e}")
        return None

# mlflow_logging_and_metric_printing function

def mlflow_logging_and_metric_printing(model, run_name, bal_type, X_train, y_train, X_test, y_test, y_pred_train, y_pred_test,hyper_tuning_score = 0, **params):

    mlflow.set_experiment("nadp_binary")

    with mlflow.start_run(run_name=run_name):
        try:
            # Log parameters
            if params:
                mlflow.log_params(params)
            mlflow.log_param("bal_type", bal_type)

            # Calculate metrics
            train_metrics = {
                "Accuracy_train": accuracy_score(y_train, y_pred_train),
                "Precision_train": precision_score(y_train, y_pred_train),
                "Recall_train": recall_score(y_train, y_pred_train),
                "F1_score_train": f1_score(y_train, y_pred_train),
                "F2_score_train": fbeta_score(y_train, y_pred_train, beta=2)  # Emphasize recall
            }

            test_metrics = {
                "Accuracy_test": accuracy_score(y_test, y_pred_test),
                "Precision_test": precision_score(y_test, y_pred_test),
                "Recall_test": recall_score(y_test, y_pred_test),
                "F1_score_test": f1_score(y_test, y_pred_test),
                "F2_score_test": fbeta_score(y_test, y_pred_test, beta=2)  # Emphasize recall
            }

            tuning_metrics = {"hyper_parameter_tuning_best_est_score":hyper_tuning_score}


            # Compute AUC metrics
            train_fpr, train_tpr, train_pr, train_re, train_roc_auc, train_pr_auc = auc_plots(model, X_train, y_train)
            test_fpr, test_tpr, test_pr, test_re, test_roc_auc, test_pr_auc = auc_plots(model, X_test, y_test)

            # Log AUC metrics
            if train_roc_auc is not None:
                train_metrics["Roc_auc_train"] = train_roc_auc
                mlflow.log_metric("Roc_auc_train", train_roc_auc)
            if train_pr_auc is not None:
                train_metrics["Pr_auc_train"] = train_pr_auc
                mlflow.log_metric("Pr_auc_train", train_pr_auc)
            if test_roc_auc is not None:
                test_metrics["Roc_auc_test"] = test_roc_auc
                mlflow.log_metric("Roc_auc_test", test_roc_auc)
            if test_pr_auc is not None:
                test_metrics["Pr_auc_test"] = test_pr_auc
                mlflow.log_metric("Pr_auc_test", test_pr_auc)

            # Print metrics
            print("Train Metrics:")
            for key, value in train_metrics.items():
                print(f"{key}: {value:.4f}")
            print("\nTest Metrics:")
            for key, value in test_metrics.items():
                print(f"{key}: {value:.4f}")
            print("\nTuning Metrics:")
            for key, value in tuning_metrics.items():
                print(f"{key}: {value:.4f}")

            # Classification Reports
            train_clf_report = classification_report(y_train, y_pred_train)
            test_clf_report = classification_report(y_test, y_pred_test)

            # Print classification reports
            print("\nTrain Classification Report:")
            print(train_clf_report)
            print("\nTest Classification Report:")
            print(test_clf_report)

            # Log metrics
            mlflow.log_metrics(train_metrics)
            mlflow.log_metrics(test_metrics)
            mlflow.log_metrics(tuning_metrics)

            # Convert classification reports to DataFrames
            train_clf_report_dict = classification_report(y_train, y_pred_train, output_dict=True)
            train_clf_report_df = pd.DataFrame(train_clf_report_dict).transpose()
            test_clf_report_dict = classification_report(y_test, y_pred_test, output_dict=True)
            test_clf_report_df = pd.DataFrame(test_clf_report_dict).transpose()

            # Save classification reports and log as artifacts
            train_clf_report_df.to_csv(f"{run_name}_train_classification_report.csv")
            mlflow.log_artifact(f"{run_name}_train_classification_report.csv")

            test_clf_report_df.to_csv(f"{run_name}_test_classification_report.csv")
            mlflow.log_artifact(f"{run_name}_test_classification_report.csv")

            # Plot confusion matrices
            fig, axes = plt.subplots(1, 2, figsize=(12, 6))

            ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_train, y_pred_train), display_labels=["Not_churned_off", "Churned_off"]).plot(ax=axes[0], cmap="Blues")
            axes[0].set_title('Train Confusion Matrix')
            # By mistake I have used "Not_Churned_off" & "Churned_off" --> "Change them to "Normal" & "Attack" respectively

            ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred_test), display_labels=["Not_churned_off", "Churned_off"]).plot(ax=axes[1], cmap="Blues")
            axes[1].set_title('Test Confusion Matrix')

            plt.tight_layout()
            plt.savefig(f"{run_name}_confusion_matrix.png")
            mlflow.log_artifact(f"{run_name}_confusion_matrix.png")

            # ROC and Precision-Recall curves
            fig, axes = plt.subplots(2, 2, figsize=(12, 12))
            datasets = [("Train", X_train, y_train), ("Test", X_test, y_test)]

            for i, (name, X, y) in enumerate(datasets):
                fpr, tpr, pr, re, roc_auc, pr_auc = auc_plots(model, X, y)
                if fpr is None:
                    return

                # ROC AUC Curve
                axes[i, 0].plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
                axes[i, 0].plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')
                axes[i, 0].set_xlim([0.0, 1.0])
                axes[i, 0].set_ylim([0.0, 1.0])
                axes[i, 0].set_xlabel('False Positive Rate')
                axes[i, 0].set_ylabel('True Positive Rate')
                axes[i, 0].set_title(f'Receiver Operating Characteristic ({name} data)')
                axes[i, 0].legend(loc='lower right')

                # Precision-Recall Curve
                axes[i, 1].plot(re, pr, color='green', lw=2, label=f'Precision-Recall curve (area = {pr_auc:.2f})')
                axes[i, 1].set_xlabel('Recall')
                axes[i, 1].set_ylabel('Precision')
                axes[i, 1].set_title(f'Precision-Recall Curve ({name} data)')
                axes[i, 1].legend(loc='lower left')

            plt.tight_layout()
            plt.savefig(f"{run_name}_auc_plots.png")
            mlflow.log_artifact(f"{run_name}_auc_plots.png")

            # Plot learning curves
            learning_curve_file = plot_learning_curve(model, X_train, y_train, run_name)
            if learning_curve_file:
                mlflow.log_artifact(learning_curve_file)

            # Log the model
            mlflow.sklearn.log_model(model, f"{run_name}_model")
            print("MLFLOW Logging is completed")

        except Exception as e:
            print(f"Error in mlflow_logging_and_metric_printing: {e}")

    # To view the logged data, run the following command in the terminal:
    # mlflow ui

# Initialise time and feature_importance df

# Initialize DataFrames
time_df = pd.DataFrame(columns=["Model", "bal_type","Training_Time", "Testing_Time","Tuning_Time"])
feature_importance_df = pd.DataFrame()

# Simple_Logistic_Regresssion_on_Imbalanced Dataset

# Model details
name = "Simple_Logistic_Regression_on_Imbalanced_Dataset"
model = LogisticRegression(random_state=42,n_jobs = -1)

# Record training time
start_train_time = time.time()
model.fit(X_train_imb, y_train_imb)
end_train_time = time.time()

# Calculate training time
training_time = end_train_time - start_train_time

# Record testing time
start_test_time = time.time()
y_pred_imb_train = model.predict(X_train_imb)
y_pred_imb_test = model.predict(X_test_imb)
end_test_time = time.time()

# Calculate testing time
testing_time = end_test_time - start_test_time

# Print model name and times
print(f"Model: {name}")
print(f"Training Time: {training_time:.4f} seconds")
print(f"Testing Time: {testing_time:.4f} seconds")

# log_time_and_feature_importance_df function

def log_time_and_feature_importances_df(time_df, feature_importance_df, name, training_time, testing_time, model, nadp_features, bal_type, tuning_time=0):
    # Store the times in the DataFrame using pd.concat
    time_df = pd.concat([time_df, pd.DataFrame({
        "Model": [name],
        "bal_type": [bal_type],
        "Training_Time": [training_time],
        "Testing_Time": [testing_time],
        "Tuning_Time": [tuning_time]
    })], ignore_index=True)

    # Feature Importances
    if hasattr(model, "coef_"):
        # For linear models like Logistic Regression or Linear SVC
        feature_importances = model.coef_.flatten()

    elif hasattr(model, "feature_importances_"):
        # For tree-based models like Decision Trees, RandomForest, GradientBoosting, etc.
        feature_importances = model.feature_importances_

    else:
        # If the model does not have feature importances, we skip logging for this model
        feature_importances = None

    if feature_importances is not None:
        # Create a DataFrame with feature importances
        importance_df = pd.DataFrame(feature_importances, index=nadp_features, columns=[name])
        feature_importance_df = pd.concat([feature_importance_df, importance_df], axis=1)

    return time_df, feature_importance_df

def feature_importances_df_new(feature_importance_df, name, model, nadp_features, bal_type):
    # Feature Importances
    if hasattr(model, "coef_"):
        # For linear models like Logistic Regression or Linear SVC
        feature_importances = model.coef_.flatten()

    elif hasattr(model, "feature_importances_"):
        # For tree-based models like Decision Trees, RandomForest, GradientBoosting, etc.
        feature_importances = model.feature_importances_

    else:
        # If the model does not have feature importances, we skip logging for this model
        feature_importances = None

    if feature_importances is not None:
        # Create a DataFrame with feature importances
        importance_df = pd.DataFrame(feature_importances, index=nadp_features, columns=[name])
        feature_importance_df = pd.concat([feature_importance_df, importance_df], axis=1)

    return feature_importance_df

# ipython-input-153-6f42e39acc84

# Define nadp_features before calling the function
nadp_features = X_train_imb.columns  # Assuming X_train_imb is a Pandas DataFrame

time_df, feature_importance_df = log_time_and_feature_importances_df(
    time_df, feature_importance_df, name, training_time, testing_time,
    model, nadp_features, "Imbalanced"
)

params = {"random_state":42,"n_jobs":-1}
print(name)
mlflow_logging_and_metric_printing(model,name,"Imbalanced",X_train_imb,y_train_imb,X_test_imb,y_test_imb,y_pred_imb_train,y_pred_imb_test,**params)

# Simple_Logistic_Regresssion_on_Balanced Dataset

# Model details
name = "Simple_Logistic_Regresssion_on_balanced_Dataset"
model = LogisticRegression(random_state=42,n_jobs = -1)
bal_type = "Balanced"

# Record training time
start_train_time = time.time()
model.fit(X_train_bal, y_train_bal)
end_train_time = time.time()

# Calculate training time
training_time = end_train_time - start_train_time

# Record testing time
start_test_time = time.time()
y_pred_bal_train = model.predict(X_train_bal)
y_pred_bal_test = model.predict(X_test_bal)
end_test_time = time.time()

# Calculate testing time
testing_time = end_test_time - start_test_time

# Print model name and times
print(f"Model: {name}")
print(f"Training Time: {training_time:.4f} seconds")
print(f"Testing Time: {testing_time:.4f} seconds")

# log time and feature importances into df
time_df,feature_importance_df = log_time_and_feature_importances_df(time_df,feature_importance_df,name,training_time,testing_time,model,nadp_features,bal_type)

params = {"random_state":42,"n_jobs":-1}
print(name)
mlflow_logging_and_metric_printing(model,name,bal_type,X_train_bal,y_train_bal,X_test_bal,y_test_bal,y_pred_bal_train,y_pred_bal_test,**params)

# all_logged_metrics function

def all_logged_metrics():
    # Set the experiment name
    experiment_name = "nadp_binary"

    # Get the experiment details
    experiment = mlflow.get_experiment_by_name(experiment_name)
    experiment_id = experiment.experiment_id

    # Retrieve all runs from the experiment
    runs_df = mlflow.search_runs(experiment_ids=[experiment_id])

    # Extract metrics columns
    metrics_columns = [col for col in runs_df.columns if col.startswith("metrics.")]
    metrics_df = runs_df[metrics_columns]

    # Add run_name as a column
    metrics_df['run_name'] = runs_df['tags.mlflow.runName']
    metrics_df["bal_type"] = runs_df["params.bal_type"]

    # Combine all params into a dictionary
    params_columns = [col for col in runs_df.columns if col.startswith("params.")]
    metrics_df["params_dict"] = runs_df[params_columns].apply(lambda row: row.dropna().to_dict(), axis=1)

    # Sort remaining columns alphabetically
    sorted_columns = sorted(metrics_columns)

    # Rearrange columns: first column is 'run_name', followed by 'bal_type', 'params_dict', and then sorted metric columns
    ordered_columns = ['run_name', "bal_type", "params_dict"] + sorted_columns
    metrics_df = metrics_df[ordered_columns]

    # If you want to view it in a more readable format
    return metrics_df

"""**Visualising all_logged_metrics, time_df and feature_imporatance df**"""

# All_logged_metrics_plots

# Example usage
pd.set_option('display.max_colwidth', None)  # Show full content in cells
all_logged_metrics_df = all_logged_metrics()
all_logged_metrics_df.head()

def all_logged_metrics_df_plots(df):
    # Melt the DataFrame to make it easier to plot
    metrics_columns = df.columns[3:]  # Select only metric columns
    df_melted = df.melt(id_vars=['run_name', 'bal_type'],
                        value_vars=metrics_columns,
                        var_name='Metric',
                        value_name='Value')

    # Create subplots with 8 rows and 2 columns
    n_rows = 8
    n_cols = 2
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 30))
    axes = axes.flatten()  # Flatten the axes array for easy iteration

    # Plot each metric in a separate subplot
    for i, metric in enumerate(metrics_columns):
        sns.barplot(x='bal_type', y='Value', hue='run_name', data=df_melted[df_melted['Metric'] == metric], ax=axes[i])
        axes[i].set_title(metric)
        axes[i].set_xlabel('')
        axes[i].set_ylabel('Value')
        axes[i].legend_.remove()  # Remove legend from individual plots

    # Remove any unused subplots
    for j in range(len(metrics_columns), len(axes)):
        fig.delaxes(axes[j])

    # Add a single legend for all subplots
    handles, labels = axes[0].get_legend_handles_labels()
    fig.legend(handles, labels, loc='upper center', ncol=3, bbox_to_anchor=(0.5, 1.03))

    # Adjust layout
    plt.tight_layout()
    plt.subplots_adjust(top=0.95)  # Adjust top to make space for the global legend
    plt.show()

# Example usage with your DataFrame
# all_logged_metrics_df_plots(all_logged_metrics_df)

# time_df_plots

time_df

def time_df_plots(time_df):
    # Create subplots for Training Time and Testing Time
    fig, axes = plt.subplots(1, 3, figsize=(25, 10))

    # Plot Training Time
    sns.barplot(x='Model', y='Training_Time', hue='bal_type', data=time_df, ax=axes[0])
    axes[0].set_title('Training Time by Model and Dataset Balance')
    axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')
    axes[0].set_xlabel('Model')
    axes[0].set_ylabel('Training Time (seconds)')

    # Plot Testing Time
    sns.barplot(x='Model', y='Testing_Time', hue='bal_type', data=time_df, ax=axes[1])
    axes[1].set_title('Testing Time by Model and Dataset Balance')
    axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')
    axes[1].set_xlabel('Model')
    axes[1].set_ylabel('Testing Time (seconds)')

    # Plot Tuning Time
    sns.barplot(x='Model', y='Tuning_Time', hue='bal_type', data=time_df, ax=axes[2])
    axes[2].set_title('Testing Time by Model and Dataset Balance')
    axes[2].set_xticklabels(axes[2].get_xticklabels(), rotation=45, ha='right')
    axes[2].set_xlabel('Model')
    axes[2].set_ylabel('Tuning Time (seconds)')

    # Adjust layout
    plt.tight_layout()

    # Show the plot
    plt.show()
# time_df_plots(time_df)

# feature_importance_df_plots

feature_importance_df

def feature_importance_plots(feature_importance_df):
    # Reset the index to get the features as a column
    feature_importance_df_reset = feature_importance_df.reset_index()

    # Melt the DataFrame to have a long-form DataFrame suitable for seaborn
    feature_importance_melted = feature_importance_df_reset.melt(id_vars='index',
                                                                 var_name='Model',
                                                                 value_name='Importance')

    # Create a seaborn horizontal barplot
    plt.figure(figsize=(10, 30))  # Adjust the figure size to be taller
    sns.barplot(x='Importance', y='index', hue='Model', data=feature_importance_melted, orient='h')

    # Set title and labels
    plt.title('Feature Importance Comparison')
    plt.xlabel('Importance Value')
    plt.ylabel('Features')
    plt.legend(loc = "lower right")
    # Display the plot
    plt.tight_layout()
    plt.show()

# Example usage with your feature_importance_df
# feature_importance_plots(feature_importance_df)

"""**Binary Classification using “attack_or_normal” as Target**

LOGISTIC REGRESSION MODEL

Imbalanced Dataset
"""

# Hyper parameter Tuning

# Define the parameter grid for RandomizedSearchCV
start_tune_time = time.time()
param_dist = {
    'penalty': ['l1', 'l2', 'elasticnet', 'none'],
    'C': stats.uniform(loc=0.01, scale=5000-0.01),  # Uniform distribution from 0.01 to 5000
    'solver': ['saga'],  # saga solver supports all penalties
    'class_weight': ['balanced']
}

# Initialize the Logistic Regression model
logReg = LogisticRegression(max_iter=100,random_state=42)

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator= logReg,
    param_distributions=param_dist,
    n_iter=10,  # Number of parameter settings to try
    cv=5,  # Number of folds in cross-validation
    verbose=1,
    random_state=42,
    n_jobs=-1
)

# Fit RandomizedSearchCV
random_search.fit(X_train_imb, y_train_imb)

# Best model and hyperparameters
print("Best parameters found:", random_search.best_params_)
print("Best score:", random_search.best_score_)
tuning_score = random_search.best_score_
end_tune_time = time.time()
tuning_time = end_tune_time - start_tune_time
print("Tuning_time",tuning_time)

# Logging Best Logistic Regression Model into MLFLOW

# Model details
name = "Tuned_Logistic_Regression_on_Imbalanced_Dataset"
bal_type = "Imbalanced"
model = random_search.best_estimator_
params = random_search.best_params_

# Record training time
start_train_time = time.time()
model.fit(X_train_imb, y_train_imb)
end_train_time = time.time()

# Calculate training time
training_time = end_train_time - start_train_time

# Record testing time
start_test_time = time.time()
y_pred_imb_train = model.predict(X_train_imb)
y_pred_imb_test = model.predict(X_test_imb)
end_test_time = time.time()

# Calculate testing time
testing_time = end_test_time - start_test_time

# Print model name and times
print(f"Model: {name}")
print(f"params: {params}")
print(f"Training Time: {training_time:.4f} seconds")
print(f"Testing Time: {testing_time:.4f} seconds")
print(f"Tuning Time: {tuning_time:.4f} seconds")

# logging time_df, feature_importance_df, mlflow_logging_and_metric_printing
time_df,feature_importance_df = log_time_and_feature_importances_df(time_df,feature_importance_df,name,training_time,testing_time,model,nadp_features,bal_type,tuning_time)
mlflow_logging_and_metric_printing(model,name,bal_type,X_train_imb,y_train_imb,X_test_imb,y_test_imb,y_pred_imb_train,y_pred_imb_test,tuning_score,**params)

"""*Balanced Dataset*"""

# Hyper parameter Tuning

# Define the parameter grid for RandomizedSearchCV
start_tune_time = time.time()
param_dist = {
    'penalty': ['l1', 'l2', 'elasticnet', 'none'],
    'C': stats.uniform(loc=0.01, scale=5000-0.01),  # Uniform distribution from 0.01 to 5000
    'solver': ['saga'],  # saga solver supports all penalties
    'class_weight': ['balanced']
}

# Initialize the Logistic Regression model
logReg = LogisticRegression(max_iter=100,random_state = 42)

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator= logReg,
    param_distributions=param_dist,
    n_iter=10,  # Number of parameter settings to try
    cv=5,  # Number of folds in cross-validation
    verbose=1,
    random_state=42,
    n_jobs=-1
)

# Fit RandomizedSearchCV
random_search.fit(X_train_bal, y_train_bal)

# Best model and hyperparameters
print("Best parameters found:", random_search.best_params_)
print("Best score:", random_search.best_score_)
tuning_score = random_search.best_score_
end_tune_time = time.time()
tuning_time = end_tune_time - start_tune_time
print("Tuning_time",tuning_time)

# Logging Best Logistic Regression Model into MLFLOW

# Model details
name = "Tuned_Logistic_Regression_on_Balanced_Dataset"
bal_type = "Balanced"
model = random_search.best_estimator_
params = random_search.best_params_

# Record training time
start_train_time = time.time()
model.fit(X_train_bal, y_train_bal)
end_train_time = time.time()

# Calculate training time
training_time = end_train_time - start_train_time

# Record testing time
start_test_time = time.time()
y_pred_bal_train = model.predict(X_train_bal)
y_pred_bal_test = model.predict(X_test_bal)
end_test_time = time.time()

# Calculate testing time
testing_time = end_test_time - start_test_time

# Print model name and times
print(f"Model: {name}")
print(f"params: {params}")
print(f"Training Time: {training_time:.4f} seconds")
print(f"Testing Time: {testing_time:.4f} seconds")
print(f"Tuning Time: {tuning_time:.4f} seconds")

# logging time_df, feature_importance_df, mlflow_logging_and_metric_printing
time_df,feature_importance_df = log_time_and_feature_importances_df(time_df,feature_importance_df,name,training_time,testing_time,model,nadp_features,bal_type,tuning_time)
mlflow_logging_and_metric_printing(model,name,bal_type,X_train_bal,y_train_bal,X_test_bal,y_test_bal,y_pred_bal_train,y_pred_bal_test,tuning_score,**params)

"""**MULTI_LAYER_PERCEPTRON NEURAL NETWORK MODEL**

Imbalanced Dataset
"""

# Hyper parameter Tuning

# Define the parameter grid
start_tune_time = time.time()
param_dist = {
    'hidden_layer_sizes': [(50,), (100,), (100, 50), (50, 50, 50)],
    'activation': ['identity', 'logistic', 'tanh', 'relu'],
    'solver': ['adam'],  # 'adam' is suitable for all activation functions
    'alpha': stats.uniform(0.0001, 5000),  # Uniform distribution from 0.0001 to 5000
    'learning_rate': ['constant', 'invscaling', 'adaptive'],
    'learning_rate_init': stats.uniform(0.0001, 0.1),  # Small learning rates for better convergence
    'max_iter': stats.randint(200, 1000),  # Increase max_iter for better convergence
    'early_stopping': [True, False],  # Use early stopping to prevent overfitting
    'tol': [1e-4, 1e-5, 1e-6],  # Lower tolerance for more precise convergence
}

# Initialize the MLPClassifier
mlp = MLPClassifier(max_iter=100,random_state=42)

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=mlp,
    param_distributions=param_dist,
    n_iter=10,  # Number of parameter settings to try
    cv=5,  # Number of folds in cross-validation
    verbose=1,
    random_state=42,
    n_jobs=-1  # Use all available cores
)

# Fit RandomizedSearchCV
random_search.fit(X_train_imb, y_train_imb)

# Best model and hyperparameters
print("Best parameters found:", random_search.best_params_)
print("Best score:", random_search.best_score_)
tuning_score = random_search.best_score_
end_tune_time = time.time()
tuning_time = end_tune_time - start_tune_time
print("Tuning_time",tuning_time)

# Logging Best MLPClassifier Model into MLFLOW

# Model details
name ="Tuned_MLPClassifier_on_Imbalanced_Dataset"
bal_type = "Imbalanced"
model = random_search.best_estimator_
params = random_search.best_params_

# Record training time
start_train_time = time.time()
model.fit(X_train_imb, y_train_imb)
end_train_time = time.time()

# Calculate training time
training_time = end_train_time - start_train_time

# Record testing time
start_test_time = time.time()
y_pred_imb_train = model.predict(X_train_imb)
y_pred_imb_test = model.predict(X_test_imb)
end_test_time = time.time()

# Calculate testing time
testing_time = end_test_time - start_test_time

# Print model name and times
print(f"Model: {name}")
print(f"params: {params}")
print(f"Training Time: {training_time:.4f} seconds")
print(f"Testing Time: {testing_time:.4f} seconds")
print(f"Tuning Time: {tuning_time:.4f} seconds")

# logging time_df, feature_importance_df, mlflow_logging_and_metric_printing
time_df,feature_importance_df = log_time_and_feature_importances_df(time_df,feature_importance_df,name,training_time,testing_time,model,nadp_features,bal_type,tuning_time)
mlflow_logging_and_metric_printing(model,name,bal_type,X_train_imb,y_train_imb,X_test_imb,y_test_imb,y_pred_imb_train,y_pred_imb_test,tuning_score,**params)

"""Balanced Dataset"""

# Hyper parameter Tuning

# Define the parameter grid
start_tune_time = time.time()
param_dist = {
    'hidden_layer_sizes': [(50,), (100,), (100, 50), (50, 50, 50)],
    'activation': ['identity', 'logistic', 'tanh', 'relu'],
    'solver': ['adam'],  # 'adam' is suitable for all activation functions
    'alpha': stats.uniform(0.0001, 5000),  # Uniform distribution from 0.0001 to 5000
    'learning_rate': ['constant', 'invscaling', 'adaptive'],
    'learning_rate_init': stats.uniform(0.0001, 0.1),  # Small learning rates for better convergence
    'max_iter': stats.randint(200, 1000),  # Increase max_iter for better convergence
    'early_stopping': [True, False],  # Use early stopping to prevent overfitting
    'tol': [1e-4, 1e-5, 1e-6],  # Lower tolerance for more precise convergence
}

# Initialize the MLPClassifier
mlp = MLPClassifier(max_iter=100,random_state=42)

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=mlp,
    param_distributions=param_dist,
    n_iter=10,  # Number of parameter settings to try
    cv=5,  # Number of folds in cross-validation
    verbose=1,
    random_state=42,
    n_jobs=-1  # Use all available cores
)

# Fit RandomizedSearchCV
random_search.fit(X_train_bal, y_train_bal)

# Best model and hyperparameters
print("Best parameters found:", random_search.best_params_)
print("Best score:", random_search.best_score_)
tuning_score = random_search.best_score_
end_tune_time = time.time()
tuning_time = end_tune_time - start_tune_time
print("Tuning_time",tuning_time)

# Logging Best MLPClassifer Model into MLFLOW

# Model details
name ="Tuned_MLPClassifier_on_Balanced_Dataset"
bal_type = "Balanced"
model = random_search.best_estimator_
params = random_search.best_params_

# Record training time
start_train_time = time.time()
model.fit(X_train_bal, y_train_bal)
end_train_time = time.time()

# Calculate training time
training_time = end_train_time - start_train_time

# Record testing time
start_test_time = time.time()
y_pred_bal_train = model.predict(X_train_bal)
y_pred_bal_test = model.predict(X_test_bal)
end_test_time = time.time()

# Calculate testing time
testing_time = end_test_time - start_test_time

# Print model name and times
print(f"Model: {name}")
print(f"params: {params}")
print(f"Training Time: {training_time:.4f} seconds")
print(f"Testing Time: {testing_time:.4f} seconds")
print(f"Tuning Time: {tuning_time:.4f} seconds")

# logging time_df, feature_importance_df, mlflow_logging_and_metric_printing
time_df,feature_importance_df = log_time_and_feature_importances_df(time_df,feature_importance_df,name,training_time,testing_time,model,nadp_features,bal_type,tuning_time)
mlflow_logging_and_metric_printing(model,name,bal_type,X_train_bal,y_train_bal,X_test_bal,y_test_bal,y_pred_bal_train,y_pred_bal_test,tuning_score,**params)

"""**K NEAREST NEIGHBORS MODEL**

Imbalanced Dataset
"""

# Hyper parameter Tuning

# Define the parameter grid
start_tune_time = time.time()
param_dist = {
    'n_neighbors': stats.randint(1, 35),
    'metric': ['euclidean', 'manhattan']
}

# Initialize the KNN model
knn = KNeighborsClassifier()

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=knn,
    param_distributions=param_dist,
    n_iter=3,  # Number of parameter settings to try
    cv=5,  # Number of folds in cross-validation
    verbose=1,
    random_state=42,
    n_jobs=-1  # Use all available cores
)

# Fit RandomizedSearchCV
random_search.fit(X_train_imb, y_train_imb)

# Best model and hyperparameters
print("Best parameters found:", random_search.best_params_)
print("Best score:", random_search.best_score_)
tuning_score = random_search.best_score_
end_tune_time = time.time()
tuning_time = end_tune_time - start_tune_time
print("Tuning_time",tuning_time)

# Logging Best K Nearest Neighbor Model into MLFLOW

# Model details
name = "Tuned_KNN_on_Imbalanced_Dataset"
bal_type = "Imbalanced"
model = random_search.best_estimator_
params = random_search.best_params_

# Record training time
start_train_time = time.time()
model.fit(X_train_imb, y_train_imb)
end_train_time = time.time()

# Calculate training time
training_time = end_train_time - start_train_time

# Record testing time
start_test_time = time.time()
y_pred_imb_train = model.predict(X_train_imb)
y_pred_imb_test = model.predict(X_test_imb)
end_test_time = time.time()

# Calculate testing time
testing_time = end_test_time - start_test_time

# Print model name and times
print(f"Model: {name}")
print(f"params: {params}")
print(f"Training Time: {training_time:.4f} seconds")
print(f"Testing Time: {testing_time:.4f} seconds")
print(f"Tuning Time: {tuning_time:.4f} seconds")

# logging time_df, feature_importance_df, mlflow_logging_and_metric_printing
time_df,feature_importance_df = log_time_and_feature_importances_df(time_df,feature_importance_df,name,training_time,testing_time,model,nadp_features,bal_type,tuning_time)
mlflow_logging_and_metric_printing(model,name,bal_type,X_train_imb,y_train_imb,X_test_imb,y_test_imb,y_pred_imb_train,y_pred_imb_test,tuning_score,**params)

"""*Balanced Dataset*"""

# Hyper parameter Tuning

# Define the parameter grid
start_tune_time = time.time()
param_dist = {
    'n_neighbors': stats.randint(1, 35),
    'metric': ['euclidean', 'manhattan']
}

# Initialize the KNN model
knn = KNeighborsClassifier()

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=knn,
    param_distributions=param_dist,
    n_iter=3,  # Number of parameter settings to try
    cv=5,  # Number of folds in cross-validation
    verbose=1,
    random_state=42,
    n_jobs=-1  # Use all available cores
)

# Fit RandomizedSearchCV
random_search.fit(X_train_bal, y_train_bal)

# Best model and hyperparameters
print("Best parameters found:", random_search.best_params_)
print("Best score:", random_search.best_score_)
tuning_score = random_search.best_score_
end_tune_time = time.time()
tuning_time = end_tune_time - start_tune_time
print("Tuning_time",tuning_time)

# Logging Best K Nearest Neighbor Model into MLFLOW

# Model details
name = "Tuned_KNN_on_Balanced_Dataset"
bal_type = "Balanced"
model = random_search.best_estimator_
params = random_search.best_params_

# Record training time
start_train_time = time.time()
model.fit(X_train_bal, y_train_bal)
end_train_time = time.time()

# Calculate training time
training_time = end_train_time - start_train_time

# Record testing time
start_test_time = time.time()
y_pred_bal_train = model.predict(X_train_bal)
y_pred_bal_test = model.predict(X_test_bal)
end_test_time = time.time()

# Calculate testing time
testing_time = end_test_time - start_test_time

# Print model name and times
print(f"Model: {name}")
print(f"params: {params}")
print(f"Training Time: {training_time:.4f} seconds")
print(f"Testing Time: {testing_time:.4f} seconds")
print(f"Tuning Time: {tuning_time:.4f} seconds")

# logging time_df, feature_importance_df, mlflow_logging_and_metric_printing
time_df,feature_importance_df = log_time_and_feature_importances_df(time_df,feature_importance_df,name,training_time,testing_time,model,nadp_features,bal_type,tuning_time)
mlflow_logging_and_metric_printing(model,name,bal_type,X_train_bal,y_train_bal,X_test_bal,y_test_bal,y_pred_bal_train,y_pred_bal_test,tuning_score,**params)

"""**DECISION TREE MODEL**

Imbalanced Dataset
"""

# Hyper parameter Tuning

# Define the parameter grid
start_tune_time = time.time()
param_dist = {
    'criterion': ['gini', 'entropy', 'log_loss'],
    'splitter': ['best', 'random'],
    'max_depth': stats.randint(2, 20),  # Random integers for max_depth
    'min_samples_split': stats.randint(2, 20),  # Random integers for min_samples_split
    'min_samples_leaf': stats.randint(1, 20),  # Random integers for min_samples_leaf
    'min_weight_fraction_leaf': stats.uniform(0.0, 0.5),  # Uniform distribution for min_weight_fraction_leaf
    'max_features': [None, 'auto', 'sqrt', 'log2'],  # Options for max_features
    'max_leaf_nodes': stats.randint(2, 100),  # Random integers for max_leaf_nodes
    'min_impurity_decrease': stats.uniform(0.0, 0.1),  # Uniform distribution for min_impurity_decrease
    'ccp_alpha': stats.uniform(0.0, 0.1),  # Uniform distribution for ccp_alpha
    'random_state': [42]  # Fixed random state for reproducibility
}

# Initialize the Decision Tree classifier
dtc = DecisionTreeClassifier()

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=dtc,
    param_distributions=param_dist,
    n_iter=200,  # Number of parameter settings to try
    cv=5,  # Number of folds in cross-validation
    verbose=1,
    random_state=42,
    n_jobs=-1  # Use all available cores
)

# Fit RandomizedSearchCV
random_search.fit(X_train_imb, y_train_imb)

# Best model and hyperparameters
print("Best parameters found:", random_search.best_params_)
print("Best score:", random_search.best_score_)
tuning_score = random_search.best_score_
end_tune_time = time.time()
tuning_time = end_tune_time - start_tune_time
print("Tuning_time", tuning_time)

# Logging Best Support Vector Machine Model into MLFLOW

# Model details
name = "Tuned_Decision_Tree_on_Imbalanced_Dataset"
bal_type = "Imbalanced"
model = random_search.best_estimator_
params = random_search.best_params_

# Record training time
start_train_time = time.time()
model.fit(X_train_imb, y_train_imb)
end_train_time = time.time()

# Calculate training time
training_time = end_train_time - start_train_time

# Record testing time
start_test_time = time.time()
y_pred_imb_train = model.predict(X_train_imb)
y_pred_imb_test = model.predict(X_test_imb)
end_test_time = time.time()

# Calculate testing time
testing_time = end_test_time - start_test_time

# Print model name and times
print(f"Model: {name}")
print(f"params: {params}")
print(f"Training Time: {training_time:.4f} seconds")
print(f"Testing Time: {testing_time:.4f} seconds")
print(f"Tuning Time: {tuning_time:.4f} seconds")

# logging time_df, feature_importance_df, mlflow_logging_and_metric_printing
time_df,feature_importance_df = log_time_and_feature_importances_df(time_df,feature_importance_df,name,training_time,testing_time,model,nadp_features,bal_type,tuning_time)
mlflow_logging_and_metric_printing(model,name,bal_type,X_train_imb,y_train_imb,X_test_imb,y_test_imb,y_pred_imb_train,y_pred_imb_test,tuning_score,**params)

"""*Balanced Dataset*"""

# Hyper parameter Tuning

# Define the parameter grid
start_tune_time = time.time()
param_dist = {
    'criterion': ['gini', 'entropy', 'log_loss'],
    'splitter': ['best', 'random'],
    'max_depth': stats.randint(2, 20),  # Random integers for max_depth
    'min_samples_split': stats.randint(2, 20),  # Random integers for min_samples_split
    'min_samples_leaf': stats.randint(1, 20),  # Random integers for min_samples_leaf
    'min_weight_fraction_leaf': stats.uniform(0.0, 0.5),  # Uniform distribution for min_weight_fraction_leaf
    'max_features': [None, 'auto', 'sqrt', 'log2'],  # Options for max_features
    'max_leaf_nodes': stats.randint(2, 100),  # Random integers for max_leaf_nodes
    'min_impurity_decrease': stats.uniform(0.0, 0.1),  # Uniform distribution for min_impurity_decrease
    'ccp_alpha': stats.uniform(0.0, 0.1),  # Uniform distribution for ccp_alpha
    'random_state': [42]  # Fixed random state for reproducibility
}

# Initialize the Decision Tree classifier
dtc = DecisionTreeClassifier()

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=dtc,
    param_distributions=param_dist,
    n_iter=200,  # Number of parameter settings to try
    cv=5,  # Number of folds in cross-validation
    verbose=1,
    random_state=42,
    n_jobs=-1  # Use all available cores
)

# Fit RandomizedSearchCV
random_search.fit(X_train_bal, y_train_bal)

# Best model and hyperparameters
print("Best parameters found:", random_search.best_params_)
print("Best score:", random_search.best_score_)
tuning_score = random_search.best_score_
end_tune_time = time.time()
tuning_time = end_tune_time - start_tune_time
print("Tuning_time", tuning_time)

# Logging Best Decision Tree Model into MLFLOW

# Model details
name = "Tuned_Decision_Tree_on_Balanced_Dataset"
bal_type = "Balanced"
model = random_search.best_estimator_
params = random_search.best_params_

# Record training time
start_train_time = time.time()
model.fit(X_train_bal, y_train_bal)
end_train_time = time.time()

# Calculate training time
training_time = end_train_time - start_train_time

# Record testing time
start_test_time = time.time()
y_pred_bal_train = model.predict(X_train_bal)
y_pred_bal_test = model.predict(X_test_bal)
end_test_time = time.time()

# Calculate testing time
testing_time = end_test_time - start_test_time

# Print model name and times
print(f"Model: {name}")
print(f"params: {params}")
print(f"Training Time: {training_time:.4f} seconds")
print(f"Testing Time: {testing_time:.4f} seconds")
print(f"Tuning Time: {tuning_time:.4f} seconds")

# logging time_df, feature_importance_df, mlflow_logging_and_metric_printing
time_df,feature_importance_df = log_time_and_feature_importances_df(time_df,feature_importance_df,name,training_time,testing_time,model,nadp_features,bal_type,tuning_time)
mlflow_logging_and_metric_printing(model,name,bal_type,X_train_bal,y_train_bal,X_test_bal,y_test_bal,y_pred_bal_train,y_pred_bal_test,tuning_score,**params)

"""**RANDOM FOREST MODEL**

Imbalanced Dataset
"""

# Hyper parameter Tuning

# Define the parameter grid
start_tune_time = time.time()
param_dist = {
    'n_estimators': stats.randint(50, 100),  # Random integers for n_estimators
    'criterion': ['gini', 'entropy', 'log_loss'],
    'max_depth': stats.randint(2, 20),  # Random integers for max_depth
    'min_samples_split': stats.randint(2, 20),  # Random integers for min_samples_split
    'min_samples_leaf': stats.randint(1, 20),  # Random integers for min_samples_leaf
    'min_weight_fraction_leaf': stats.uniform(0.0, 0.5),  # Uniform distribution for min_weight_fraction_leaf
    'max_features': ['auto', 'sqrt', 'log2', None],  # Options for max_features
    'max_leaf_nodes': stats.randint(2, 100),  # Random integers for max_leaf_nodes
    'min_impurity_decrease': stats.uniform(0.0, 0.1),  # Uniform distribution for min_impurity_decrease
    'bootstrap': [True, False],  # Whether bootstrap samples are used when building trees
    'oob_score': [True, False],  # Whether to use out-of-bag samples to estimate the generalization accuracy
    'ccp_alpha': stats.uniform(0.0, 0.1),  # Uniform distribution for ccp_alpha
    'random_state': [42]  # Fixed random state for reproducibility
}

# Initialize the RandomForest classifier
rfc = RandomForestClassifier(class_weight="balanced")

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=rfc,
    param_distributions=param_dist,
    n_iter=20,  # Number of parameter settings to try
    cv=5,  # Number of folds in cross-validation
    verbose=1,
    random_state=42,
    n_jobs=-1  # Use all available cores
)

# Fit RandomizedSearchCV
random_search.fit(X_train_imb, y_train_imb)

# Best model and hyperparameters
print("Best parameters found:", random_search.best_params_)
print("Best score:", random_search.best_score_)
tuning_score = random_search.best_score_
end_tune_time = time.time()
tuning_time = end_tune_time - start_tune_time
print("Tuning_time", tuning_time)

rfc_imb = random_search.best_estimator_

# Logging Best Random Forest Model into MLFLOW

# Model details
name = "Tuned_Random_Forest_on_Imbalanced_Dataset"
bal_type = "Imbalanced"
model = random_search.best_estimator_
params = random_search.best_params_

# Record training time
start_train_time = time.time()
model.fit(X_train_imb, y_train_imb)
end_train_time = time.time()

# Calculate training time
training_time = end_train_time - start_train_time

# Record testing time
start_test_time = time.time()
y_pred_imb_train = model.predict(X_train_imb)
y_pred_imb_test = model.predict(X_test_imb)
end_test_time = time.time()

# Calculate testing time
testing_time = end_test_time - start_test_time

# Print model name and times
print(f"Model: {name}")
print(f"params: {params}")
print(f"Training Time: {training_time:.4f} seconds")
print(f"Testing Time: {testing_time:.4f} seconds")
print(f"Tuning Time: {tuning_time:.4f} seconds")

# logging time_df, feature_importance_df, mlflow_logging_and_metric_printing
time_df,feature_importance_df = log_time_and_feature_importances_df(time_df,feature_importance_df,name,training_time,testing_time,model,nadp_features,bal_type,tuning_time)
mlflow_logging_and_metric_printing(model,name,bal_type,X_train_imb,y_train_imb,X_test_imb,y_test_imb,y_pred_imb_train,y_pred_imb_test,tuning_score,**params)

"""*Balanced Dataset*"""

# Hyper parameter Tuning

# Define the parameter grid
start_tune_time = time.time()
param_dist = {
    'n_estimators': stats.randint(50, 100),  # Random integers for n_estimators
    'criterion': ['gini', 'entropy', 'log_loss'],
    'max_depth': stats.randint(2, 20),  # Random integers for max_depth
    'min_samples_split': stats.randint(2, 20),  # Random integers for min_samples_split
    'min_samples_leaf': stats.randint(1, 20),  # Random integers for min_samples_leaf
    'min_weight_fraction_leaf': stats.uniform(0.0, 0.5),  # Uniform distribution for min_weight_fraction_leaf
    'max_features': ['auto', 'sqrt', 'log2', None],  # Options for max_features
    'max_leaf_nodes': stats.randint(2, 100),  # Random integers for max_leaf_nodes
    'min_impurity_decrease': stats.uniform(0.0, 0.1),  # Uniform distribution for min_impurity_decrease
    'bootstrap': [True, False],  # Whether bootstrap samples are used when building trees
    'oob_score': [True, False],  # Whether to use out-of-bag samples to estimate the generalization accuracy
    'ccp_alpha': stats.uniform(0.0, 0.1),  # Uniform distribution for ccp_alpha
    'random_state': [42]  # Fixed random state for reproducibility
}

# Initialize the RandomForest classifier
rfc = RandomForestClassifier(class_weight="balanced")

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=rfc,
    param_distributions=param_dist,
    n_iter=20,  # Number of parameter settings to try
    cv=5,  # Number of folds in cross-validation
    verbose=1,
    random_state=42,
    n_jobs=-1  # Use all available cores
)

# Fit RandomizedSearchCV
random_search.fit(X_train_bal, y_train_bal)

# Best model and hyperparameters
print("Best parameters found:", random_search.best_params_)
print("Best score:", random_search.best_score_)
tuning_score = random_search.best_score_
end_tune_time = time.time()
tuning_time = end_tune_time - start_tune_time
print("Tuning_time", tuning_time)

rfc_bal = random_search.best_estimator_

# Logging Best Random Forest Model into MLFLOW

# Model details
name = "Tuned_Random_Forest_on_Balanced_Dataset"
bal_type = "Balanced"
model = random_search.best_estimator_
params = random_search.best_params_

# Record training time
start_train_time = time.time()
model.fit(X_train_bal, y_train_bal)
end_train_time = time.time()

# Calculate training time
training_time = end_train_time - start_train_time

# Record testing time
start_test_time = time.time()
y_pred_bal_train = model.predict(X_train_bal)
y_pred_bal_test = model.predict(X_test_bal)
end_test_time = time.time()

# Calculate testing time
testing_time = end_test_time - start_test_time

# Print model name and times
print(f"Model: {name}")
print(f"params: {params}")
print(f"Training Time: {training_time:.4f} seconds")
print(f"Testing Time: {testing_time:.4f} seconds")
print(f"Tuning Time: {tuning_time:.4f} seconds")

# logging time_df, feature_importance_df, mlflow_logging_and_metric_printing
time_df,feature_importance_df = log_time_and_feature_importances_df(time_df,feature_importance_df,name,training_time,testing_time,model,nadp_features,bal_type,tuning_time)
mlflow_logging_and_metric_printing(model,name,bal_type,X_train_bal,y_train_bal,X_test_bal,y_test_bal,y_pred_bal_train,y_pred_bal_test,tuning_score,**params)

"""** BAGGING CLASSIFIER ON BEST_RF MODEL**
Imbalanced Dataset
"""

# Hyper parameter Tuning

rfc_imb

# # Base RandomForest model with the provided parameters
base_rf = RandomForestClassifier(class_weight='balanced',criterion='log_loss', max_depth=10, max_leaf_nodes=40,
                       min_samples_leaf=10,min_samples_split=13,n_estimators=80,
                        min_weight_fraction_leaf=np.float64(0.12), random_state=42)

# Define the parameter grid for Bagging
start_tune_time = time.time()
param_dist = {
    'n_estimators': stats.randint(10,20),  # Number of base estimators in the ensemble
    'max_samples': stats.uniform(0.1, 1.0),  # Fraction of samples to draw from X to train each base estimator
    'max_features': stats.uniform(0.1, 1.0),  # Fraction of features to draw from X to train each base estimator
    'bootstrap': [True, False],  # Whether samples are drawn with replacement
    'bootstrap_features': [True, False],  # Whether features are drawn with replacement
    'random_state': [42]  # Fixed random state for reproducibility
}

# Initialize the Bagging classifier with the RandomForest as the base estimator
bagging_clf = BaggingClassifier(estimator=base_rf,n_jobs=-1)

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=bagging_clf,
    param_distributions=param_dist,
    n_iter=10,  # Number of parameter settings to try
    cv=5,  # Number of folds in cross-validation
    verbose=5,
    random_state=42,
    n_jobs=-1  # Use all available cores
)

# Fit RandomizedSearchCV
random_search.fit(X_train_imb, y_train_imb)

# Best model and hyperparameters
print("Best parameters found:", random_search.best_params_)
print("Best score:", random_search.best_score_)
tuning_score = random_search.best_score_
end_tune_time = time.time()
tuning_time = end_tune_time - start_tune_time
print("Tuning_time", tuning_time)

# Logging Best Bagging RF Model into MLFLOW

# Model details
name = "Tuned_Bagging_RF_on_Imbalanced_Dataset"
bal_type = "Imbalanced"
model = random_search.best_estimator_
params = random_search.best_params_

# Record training time
start_train_time = time.time()
model.fit(X_train_imb, y_train_imb)
end_train_time = time.time()

# Calculate training time
training_time = end_train_time - start_train_time

# Record testing time
start_test_time = time.time()
y_pred_imb_train = model.predict(X_train_imb)
y_pred_imb_test = model.predict(X_test_imb)
end_test_time = time.time()

# Calculate testing time
testing_time = end_test_time - start_test_time

# Print model name and times
print(f"Model: {name}")
print(f"params: {params}")
print(f"Training Time: {training_time:.4f} seconds")
print(f"Testing Time: {testing_time:.4f} seconds")
print(f"Tuning Time: {tuning_time:.4f} seconds")

# logging time_df, feature_importance_df, mlflow_logging_and_metric_printing
time_df,feature_importance_df = log_time_and_feature_importances_df(time_df,feature_importance_df,name,training_time,testing_time,model,nadp_features,bal_type,tuning_time)
mlflow_logging_and_metric_printing(model,name,bal_type,X_train_imb,y_train_imb,X_test_imb,y_test_imb,y_pred_imb_train,y_pred_imb_test,tuning_score,**params)

"""*Balanced Dataset*"""

# Hyper parameter Tuning

rfc_bal

# # Base RandomForest model with the provided parameters
base_rf = RandomForestClassifier(class_weight='balanced',criterion='log_loss', max_depth=10, max_leaf_nodes=40,
                       min_samples_leaf=10,min_samples_split=13,n_estimators=80,
                        min_weight_fraction_leaf=np.float64(0.12), random_state=42)

# Define the parameter grid for Bagging
start_tune_time = time.time()
param_dist = {
    'n_estimators': stats.randint(10, 20),  # Number of base estimators in the ensemble
    'max_samples': stats.uniform(0.1, 1.0),  # Fraction of samples to draw from X to train each base estimator
    'max_features': stats.uniform(0.1, 1.0),  # Fraction of features to draw from X to train each base estimator
    'bootstrap': [True, False],  # Whether samples are drawn with replacement
    'bootstrap_features': [True, False],  # Whether features are drawn with replacement
    'random_state': [42]  # Fixed random state for reproducibility
}

# Initialize the Bagging classifier with the RandomForest as the base estimator
bagging_clf = BaggingClassifier(estimator=base_rf)

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=bagging_clf,
    param_distributions=param_dist,
    n_iter=10,  # Number of parameter settings to try
    cv=5,  # Number of folds in cross-validation
    verbose=1,
    random_state=42,
    n_jobs=-1  # Use all available cores
)

# Fit RandomizedSearchCV
random_search.fit(X_train_bal, y_train_bal)

# Best model and hyperparameters
print("Best parameters found:", random_search.best_params_)
print("Best score:", random_search.best_score_)
tuning_score = random_search.best_score_
end_tune_time = time.time()
tuning_time = end_tune_time - start_tune_time
print("Tuning_time", tuning_time)

# Logging Best Bagging RF Model into MLFLOW

# Model details
name = "Tuned_Bagging_RF_on_Balanced_Dataset"
bal_type = "Balanced"
model = random_search.best_estimator_
params = random_search.best_params_

# Record training time
start_train_time = time.time()
model.fit(X_train_bal, y_train_bal)
end_train_time = time.time()

# Calculate training time
training_time = end_train_time - start_train_time

# Record testing time
start_test_time = time.time()
y_pred_bal_train = model.predict(X_train_bal)
y_pred_bal_test = model.predict(X_test_bal)
end_test_time = time.time()

# Calculate testing time
testing_time = end_test_time - start_test_time

# Print model name and times
print(f"Model: {name}")
print(f"params: {params}")
print(f"Training Time: {training_time:.4f} seconds")
print(f"Testing Time: {testing_time:.4f} seconds")
print(f"Tuning Time: {tuning_time:.4f} seconds")

# logging time_df, feature_importance_df, mlflow_logging_and_metric_printing
time_df,feature_importance_df = log_time_and_feature_importances_df(time_df,feature_importance_df,name,training_time,testing_time,model,nadp_features,bal_type,tuning_time)
mlflow_logging_and_metric_printing(model,name,bal_type,X_train_bal,y_train_bal,X_test_bal,y_test_bal,y_pred_bal_train,y_pred_bal_test,tuning_score,**params)

"""**ADABOOST MODEL**

*Imbalanced Dataset*
"""

# Hyper parameter Tuning

# Define the base estimator (Decision Stump)
base_stump = DecisionTreeClassifier(max_depth=5)

# Define the parameter grid
start_tune_time = time.time()
param_dist = {
    'n_estimators': stats.randint(100, 200),  # Number of boosting stages
    'learning_rate': stats.uniform(0.01, 1.0),  # Step size for boosting
    'algorithm': ['SAMME', 'SAMME.R'],  # Algorithm to use for boosting
}

# Initialize the AdaBoost model with the decision stump as base estimator
ada_boost = AdaBoostClassifier(estimator=base_stump, random_state=42)

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=ada_boost,
    param_distributions=param_dist,
    n_iter=2,  # Number of parameter settings to try
    cv=5,  # Number of folds in cross-validation
    verbose=1,
    random_state=42,
    n_jobs=-1  # Use all available cores
)

# Fit RandomizedSearchCV
random_search.fit(X_train_imb, y_train_imb)

# Best model and hyperparameters
print("Best parameters found:", random_search.best_params_)
print("Best score:", random_search.best_score_)
tuning_score = random_search.best_score_
end_tune_time = time.time()
tuning_time = end_tune_time - start_tune_time
print("Tuning_time:", tuning_time)

# Logging Best Adaboost Model into MLFLOW

# Model details
name = "Tuned_Adaboost_on_Imbalanced_Dataset"
bal_type = "Imbalanced"
model = random_search.best_estimator_
params = random_search.best_params_

# Record training time
start_train_time = time.time()
model.fit(X_train_imb, y_train_imb)
end_train_time = time.time()

# Calculate training time
training_time = end_train_time - start_train_time

# Record testing time
start_test_time = time.time()
y_pred_imb_train = model.predict(X_train_imb)
y_pred_imb_test = model.predict(X_test_imb)
end_test_time = time.time()

# Calculate testing time
testing_time = end_test_time - start_test_time

# Print model name and times
print(f"Model: {name}")
print(f"params: {params}")
print(f"Training Time: {training_time:.4f} seconds")
print(f"Testing Time: {testing_time:.4f} seconds")
print(f"Tuning Time: {tuning_time:.4f} seconds")

# logging time_df, feature_importance_df, mlflow_logging_and_metric_printing
time_df,feature_importance_df = log_time_and_feature_importances_df(time_df,feature_importance_df,name,training_time,testing_time,model,nadp_features,bal_type,tuning_time)
mlflow_logging_and_metric_printing(model,name,bal_type,X_train_imb,y_train_imb,X_test_imb,y_test_imb,y_pred_imb_train,y_pred_imb_test,tuning_score,**params)

"""*Balanced Dataset*"""

# Hyper parameter Tuning

# Define the base estimator (Decision Stump)
base_stump = DecisionTreeClassifier(max_depth=5)

# Define the parameter grid
start_tune_time = time.time()
param_dist = {
    'n_estimators': stats.randint(100, 200),  # Number of boosting stages
    'learning_rate': stats.uniform(0.01, 1.0),  # Step size for boosting
    'algorithm': ['SAMME', 'SAMME.R'],  # Algorithm to use for boosting
 }

# Initialize the AdaBoost model with the decision stump as base estimator
ada_boost = AdaBoostClassifier(estimator=base_stump, random_state=42)

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=ada_boost,
    param_distributions=param_dist,
    n_iter=2,  # Number of parameter settings to try
    cv=5,  # Number of folds in cross-validation
    verbose=1,
    random_state=42,
    n_jobs=-1  # Use all available cores
)

# Fit RandomizedSearchCV
random_search.fit(X_train_bal, y_train_bal)

# Best model and hyperparameters
print("Best parameters found:", random_search.best_params_)
print("Best score:", random_search.best_score_)
tuning_score = random_search.best_score_
end_tune_time = time.time()
tuning_time = end_tune_time - start_tune_time
print("Tuning_time:", tuning_time)

# Logging Best Adaboost Model into MLFLOW

# Model details
name = "Tuned_Adaboost_on_Balanced_Dataset"
bal_type = "Balanced"
model = random_search.best_estimator_
params = random_search.best_params_

# Record training time
start_train_time = time.time()
model.fit(X_train_bal, y_train_bal)
end_train_time = time.time()

# Calculate training time
training_time = end_train_time - start_train_time

# Record testing time
start_test_time = time.time()
y_pred_bal_train = model.predict(X_train_bal)
y_pred_bal_test = model.predict(X_test_bal)
end_test_time = time.time()

# Calculate testing time
testing_time = end_test_time - start_test_time

# Print model name and times
print(f"Model: {name}")
print(f"params: {params}")
print(f"Training Time: {training_time:.4f} seconds")
print(f"Testing Time: {testing_time:.4f} seconds")
print(f"Tuning Time: {tuning_time:.4f} seconds")

# logging time_df, feature_importance_df, mlflow_logging_and_metric_printing
time_df,feature_importance_df = log_time_and_feature_importances_df(time_df,feature_importance_df,name,training_time,testing_time,model,nadp_features,bal_type,tuning_time)
mlflow_logging_and_metric_printing(model,name,bal_type,X_train_bal,y_train_bal,X_test_bal,y_test_bal,y_pred_bal_train,y_pred_bal_test,tuning_score,**params)

"""**GRADIENT BOOST MODEL**

Imbalanced Dataset
"""

# Hyper parameter Tuning

# Start timing the tuning process
start_tune_time = time.time()

# Define the parameter grid
param_dist = {
    'n_estimators': stats.randint(100, 200),          # Number of boosting stages to be run
    'learning_rate': stats.uniform(0.01, 0.3),         # Learning rate shrinks the contribution of each tree
    'max_depth': stats.randint(3, 15),                 # Maximum depth of the individual regression estimators
    'min_samples_split': stats.randint(2, 20),         # Minimum number of samples required to split an internal node
    'min_samples_leaf': stats.randint(1, 20),          # Minimum number of samples required to be at a leaf node
    'max_features': ['auto', 'sqrt', 'log2', None],    # Number of features to consider when looking for the best split
    'subsample': stats.uniform(0.7, 0.3),              # Fraction of samples used for fitting the individual base learners
    'min_impurity_decrease': stats.uniform(0.0, 0.1),  # A node will be split if this split induces a decrease of the impurity
    'random_state': [42]                               # Fixed random state for reproducibility
}

# Initialize the GradientBoostingClassifier
gbc = GradientBoostingClassifier()

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=gbc,
    param_distributions=param_dist,
    n_iter=5,  # Number of parameter settings to try
    cv=5,  # Number of folds in cross-validation
    verbose=5,
    random_state=42,
    n_jobs=-1  # Use all available cores
)

# Fit RandomizedSearchCV
random_search.fit(X_train_imb, y_train_imb)

# Best model and hyperparameters
print("Best parameters found:", random_search.best_params_)
print("Best score:", random_search.best_score_)
tuning_score = random_search.best_score_

# End timing and print tuning time
end_tune_time = time.time()
tuning_time = end_tune_time - start_tune_time
print("Tuning_time:", tuning_time)

# Logging Best Gradient boost Model into MLFLOW

# Model details
name = "Tuned_Gradient_boost_on_Imbalanced_Dataset"
bal_type = "Imbalanced"
model = random_search.best_estimator_
params = random_search.best_params_

# Record training time
start_train_time = time.time()
model.fit(X_train_imb, y_train_imb)
end_train_time = time.time()

# Calculate training time
training_time = end_train_time - start_train_time

# Record testing time
start_test_time = time.time()
y_pred_imb_train = model.predict(X_train_imb)
y_pred_imb_test = model.predict(X_test_imb)
end_test_time = time.time()

# Calculate testing time
testing_time = end_test_time - start_test_time

# Print model name and times
print(f"Model: {name}")
print(f"params: {params}")
print(f"Training Time: {training_time:.4f} seconds")
print(f"Testing Time: {testing_time:.4f} seconds")
print(f"Tuning Time: {tuning_time:.4f} seconds")

# logging time_df, feature_importance_df, mlflow_logging_and_metric_printing
time_df,feature_importance_df = log_time_and_feature_importances_df(time_df,feature_importance_df,name,training_time,testing_time,model,nadp_features,bal_type,tuning_time)
mlflow_logging_and_metric_printing(model,name,bal_type,X_train_imb,y_train_imb,X_test_imb,y_test_imb,y_pred_imb_train,y_pred_imb_test,tuning_score,**params)

"""*Balanced Dataset*"""

# Hyper parameter Tuning

# Start timing the tuning process
start_tune_time = time.time()

# Define the parameter grid
param_dist = {
    'n_estimators': stats.randint(100, 200),          # Number of boosting stages to be run
    'learning_rate': stats.uniform(0.01, 0.3),         # Learning rate shrinks the contribution of each tree
    'max_depth': stats.randint(3, 15),                 # Maximum depth of the individual regression estimators
    'min_samples_split': stats.randint(2, 20),         # Minimum number of samples required to split an internal node
    'min_samples_leaf': stats.randint(1, 20),          # Minimum number of samples required to be at a leaf node
    'max_features': ['auto', 'sqrt', 'log2', None],    # Number of features to consider when looking for the best split
    'subsample': stats.uniform(0.7, 0.3),              # Fraction of samples used for fitting the individual base learners
    'min_impurity_decrease': stats.uniform(0.0, 0.1),  # A node will be split if this split induces a decrease of the impurity
    'random_state': [42]                               # Fixed random state for reproducibility
}

# Initialize the GradientBoostingClassifier
gbc = GradientBoostingClassifier()

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=gbc,
    param_distributions=param_dist,
    n_iter=5,  # Number of parameter settings to try
    cv=5,  # Number of folds in cross-validation
    verbose=5,
    random_state=42,
    n_jobs=-1  # Use all available cores
)

# Fit RandomizedSearchCV
random_search.fit(X_train_bal, y_train_bal)

# Best model and hyperparameters
print("Best parameters found:", random_search.best_params_)
print("Best score:", random_search.best_score_)
tuning_score = random_search.best_score_

# End timing and print tuning time
end_tune_time = time.time()
tuning_time = end_tune_time - start_tune_time
print("Tuning_time:", tuning_time)

# Logging Best Gradient boost Model into MLFLOW

# Model details
name = "Tuned_Gradient_boost_on_Balanced_Dataset"
bal_type = "Balanced"
model = random_search.best_estimator_
params = random_search.best_params_

# Record training time
start_train_time = time.time()
model.fit(X_train_bal, y_train_bal)
end_train_time = time.time()

# Calculate training time
training_time = end_train_time - start_train_time

# Record testing time
start_test_time = time.time()
y_pred_bal_train = model.predict(X_train_bal)
y_pred_bal_test = model.predict(X_test_bal)
end_test_time = time.time()

# Calculate testing time
testing_time = end_test_time - start_test_time

# Print model name and times
print(f"Model: {name}")
print(f"params: {params}")
print(f"Training Time: {training_time:.4f} seconds")
print(f"Testing Time: {testing_time:.4f} seconds")
print(f"Tuning Time: {tuning_time:.4f} seconds")

# logging time_df, feature_importance_df, mlflow_logging_and_metric_printing
time_df,feature_importance_df = log_time_and_feature_importances_df(time_df,feature_importance_df,name,training_time,testing_time,model,nadp_features,bal_type,tuning_time)
mlflow_logging_and_metric_printing(model,name,bal_type,X_train_bal,y_train_bal,X_test_bal,y_test_bal,y_pred_bal_train,y_pred_bal_test,tuning_score,**params)

"""**XGBOOST MODEL**

Imbalanced Dataset
"""

# Hyper parameter Tuning

# Start timing the tuning process
start_tune_time = time.time()

# Define the parameter grid
param_dist = {
    'n_estimators': stats.randint(100, 200),          # Number of boosting rounds
    'learning_rate': stats.uniform(0.01, 0.3),         # Learning rate (shrinkage factor)
    'max_depth': stats.randint(3, 15),                 # Maximum depth of a tree
    'min_child_weight': stats.randint(1, 10),          # Minimum sum of instance weight (hessian) needed in a child
    'gamma': stats.uniform(0, 0.5),                    # Minimum loss reduction required to make a split
    'subsample': stats.uniform(0.7, 0.3),              # Subsample ratio of the training instance
    'colsample_bytree': stats.uniform(0.7, 0.3),       # Subsample ratio of columns when constructing each tree
    'colsample_bylevel': stats.uniform(0.7, 0.3),      # Subsample ratio of columns for each level of a tree
    'colsample_bynode': stats.uniform(0.7, 0.3),       # Subsample ratio of columns for each node (split)
    'reg_alpha': stats.uniform(0, 0.5),                # L1 regularization term on weights
    'reg_lambda': stats.uniform(0.5, 1.5),             # L2 regularization term on weights
    'scale_pos_weight': stats.uniform(0.5, 2),         # Balance of positive and negative weights
    'booster': ['gbtree', 'gblinear', 'dart'],         # Type of booster to use
    'tree_method': ['auto', 'exact', 'approx', 'hist'],# Algorithm used to train trees
    'grow_policy': ['depthwise', 'lossguide'],         # Controls the way new nodes are added to the tree
    'objective': ['binary:logistic', 'multi:softprob'],# Learning task and the corresponding objective function
    'sampling_method': ['uniform', 'gradient_based'],  # Method used to sample training data
    'random_state': [42]                               # Fixed random state for reproducibility
}

# Initialize the XGBClassifier
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=xgb_clf,
    param_distributions=param_dist,
    n_iter=10,  # Number of parameter settings to try
    cv=5,  # Number of folds in cross-validation
    verbose=1,
    random_state=42,
    n_jobs=-1  # Use all available cores
)

# Fit RandomizedSearchCV
random_search.fit(X_train_imb, y_train_imb)

# Best model and hyperparameters
print("Best parameters found:", random_search.best_params_)
print("Best score:", random_search.best_score_)
tuning_score = random_search.best_score_

# End timing and print tuning time
end_tune_time = time.time()
tuning_time = end_tune_time - start_tune_time
print("Tuning_time:", tuning_time)

# Logging Best XG boost Model into MLFLOW

# Model details
name = "Tuned_XG_boost_on_Imbalanced_Dataset"
bal_type = "Imbalanced"
model = random_search.best_estimator_
params = random_search.best_params_

# Record training time
start_train_time = time.time()
model.fit(X_train_imb, y_train_imb)
end_train_time = time.time()

# Calculate training time
training_time = end_train_time - start_train_time

# Record testing time
start_test_time = time.time()
y_pred_imb_train = model.predict(X_train_imb)
y_pred_imb_test = model.predict(X_test_imb)
end_test_time = time.time()

# Calculate testing time
testing_time = end_test_time - start_test_time

# Print model name and times
print(f"Model: {name}")
print(f"params: {params}")
print(f"Training Time: {training_time:.4f} seconds")
print(f"Testing Time: {testing_time:.4f} seconds")
print(f"Tuning Time: {tuning_time:.4f} seconds")

# logging time_df, feature_importance_df, mlflow_logging_and_metric_printing
time_df,feature_importance_df = log_time_and_feature_importances_df(time_df,feature_importance_df,name,training_time,testing_time,model,nadp_features,bal_type,tuning_time)
mlflow_logging_and_metric_printing(model,name,bal_type,X_train_imb,y_train_imb,X_test_imb,y_test_imb,y_pred_imb_train,y_pred_imb_test,tuning_score,**params)

"""*Balanced Dataset*"""

# Hyper parameter Tuning

# Start timing the tuning process
start_tune_time = time.time()

# Define the parameter grid
param_dist = {
    'n_estimators': stats.randint(100, 200),          # Number of boosting rounds
    'learning_rate': stats.uniform(0.01, 0.3),         # Learning rate (shrinkage factor)
    'max_depth': stats.randint(3, 15),                 # Maximum depth of a tree
    'min_child_weight': stats.randint(1, 10),          # Minimum sum of instance weight (hessian) needed in a child
    'gamma': stats.uniform(0, 0.5),                    # Minimum loss reduction required to make a split
    'subsample': stats.uniform(0.7, 0.3),              # Subsample ratio of the training instance
    'colsample_bytree': stats.uniform(0.7, 0.3),       # Subsample ratio of columns when constructing each tree
    'colsample_bylevel': stats.uniform(0.7, 0.3),      # Subsample ratio of columns for each level of a tree
    'colsample_bynode': stats.uniform(0.7, 0.3),       # Subsample ratio of columns for each node (split)
    'reg_alpha': stats.uniform(0, 0.5),                # L1 regularization term on weights
    'reg_lambda': stats.uniform(0.5, 1.5),             # L2 regularization term on weights
    'scale_pos_weight': stats.uniform(0.5, 2),         # Balance of positive and negative weights
    'booster': ['gbtree', 'gblinear', 'dart'],         # Type of booster to use
    'tree_method': ['auto', 'exact', 'approx', 'hist'],# Algorithm used to train trees
    'grow_policy': ['depthwise', 'lossguide'],         # Controls the way new nodes are added to the tree
    'objective': ['binary:logistic', 'multi:softprob'],# Learning task and the corresponding objective function
    'sampling_method': ['uniform', 'gradient_based'],  # Method used to sample training data
    'random_state': [42]                               # Fixed random state for reproducibility
}

# Initialize the XGBClassifier
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=xgb_clf,
    param_distributions=param_dist,
    n_iter=10,  # Number of parameter settings to try
    cv=5,  # Number of folds in cross-validation
    verbose=1,
    random_state=42,
    n_jobs=-1  # Use all available cores
)

# Fit RandomizedSearchCV
random_search.fit(X_train_bal, y_train_bal)

# Best model and hyperparameters
print("Best parameters found:", random_search.best_params_)
print("Best score:", random_search.best_score_)
tuning_score = random_search.best_score_

# End timing and print tuning time
end_tune_time = time.time()
tuning_time = end_tune_time - start_tune_time
print("Tuning_time:", tuning_time)

# Logging Best XG boost Model into MLFLOW

# Model details
name = "Tuned_XG_boost_on_Balanced_Dataset"
bal_type = "Balanced"
model = random_search.best_estimator_
params = random_search.best_params_

# Record training time
start_train_time = time.time()
model.fit(X_train_bal, y_train_bal)
end_train_time = time.time()

# Calculate training time
training_time = end_train_time - start_train_time

# Record testing time
start_test_time = time.time()
y_pred_bal_train = model.predict(X_train_bal)
y_pred_bal_test = model.predict(X_test_bal)
end_test_time = time.time()

# Calculate testing time
testing_time = end_test_time - start_test_time

# Print model name and times
print(f"Model: {name}")
print(f"params: {params}")
print(f"Training Time: {training_time:.4f} seconds")
print(f"Testing Time: {testing_time:.4f} seconds")
print(f"Tuning Time: {tuning_time:.4f} seconds")

# logging time_df, feature_importance_df, mlflow_logging_and_metric_printing
time_df,feature_importance_df = log_time_and_feature_importances_df(time_df,feature_importance_df,name,training_time,testing_time,model,nadp_features,bal_type,tuning_time)
mlflow_logging_and_metric_printing(model,name,bal_type,X_train_bal,y_train_bal,X_test_bal,y_test_bal,y_pred_bal_train,y_pred_bal_test,tuning_score,**params)

"""** EVALUATION OF RESULTS FOR BINARY CLASSIFICATION**"""

# ALL_LOGGED_METRICS

all_logged_metrics_df = all_logged_metrics()
all_logged_metrics_df

"""Output will be all logged metrics as a dataframe. Make sure that 23 Classification related rows and 9 Unsupervised Algorithm rows are present in the dataframe."""

# Fill zero inplace of Null values

all_logged_metrics_df.fillna(value = 0,inplace=True)

# all_logged_metrics_df_plots

# Selecting only classification metrics
df = all_logged_metrics_df.iloc[0:24][['run_name', 'bal_type', 'params_dict',
       'metrics.Accuracy_test', 'metrics.Accuracy_train',
       'metrics.F1_score_test', 'metrics.F1_score_train',
       'metrics.F2_score_test', 'metrics.F2_score_train',
       'metrics.Pr_auc_test', 'metrics.Pr_auc_train', 'metrics.Precision_test',
       'metrics.Precision_train', 'metrics.Recall_test',
       'metrics.Recall_train', 'metrics.Roc_auc_test', 'metrics.Roc_auc_train',
       'metrics.hyper_parameter_tuning_best_est_score']]
all_logged_metrics_df_plots(df)

# printing best models according to each metric

# Assuming your DataFrame is named 'df'
metrics_columns = df.columns[df.columns.str.startswith('metrics.')]

for metric in metrics_columns:
    best_model = df.loc[df[metric].idxmax(), 'run_name']
    best_params = df.loc[df[metric].idxmax(), 'params_dict']
    print(f"{metric} -> best_model -> \"{best_model}\", best_params -> {best_params}")

"""Insights:- Tuned_Adaboost_on_Balanced_Dataset model provides the best metrics. Therefore, we consider that model for deployment of binary classification."""

# TIME_DF AND ITS PLOT

time_df_plots(time_df)

"""Insights:-
1. Training time is highest for Tuned_XG_boost_on_Imbalanced_Dataset.
2. Testing time is highest for Tuned_KNN_on_Balanced_Dataset.
3. Tuning time is highest for Tuned_XG_boost_on_Imbalanced_Dataset. We should avoid these models.
4. Testing time and accuracy are important to select the best model.
5. Testing time is less for Logistic Regression, Decision Tree, and Random Forest.
5. Boosting models like Adaboost and Gradient Boosting have slightly higher testing times, around 5 seconds. However, boosting models provide the best metrics. Therefore, we use the Adaboost model for deployment.
"""

# FEATURE IMPORTANCE DF

# normalizing the values
scaler = StandardScaler()
feature_importance_scaled = pd.DataFrame(scaler.fit_transform(feature_importance_df),
                                         index=feature_importance_df.index,
                                         columns=feature_importance_df.columns)

feature_importance_scaled

# Feature_importance_df_plots

feature_importance_plots(feature_importance_scaled)

# sum of all the feature_importances and normalizing

combined_feature_importance_scaled = feature_importance_scaled.sum(axis = 1)
combined_feature_importance_scaled = pd.DataFrame(combined_feature_importance_scaled,index=combined_feature_importance_scaled.index,columns = ["combined_feature_importances"])
combined_feature_importance_scaled

scaler = StandardScaler()
combined_feature_importance_scaled = pd.DataFrame(scaler.fit_transform(combined_feature_importance_scaled),
                                         index=combined_feature_importance_scaled.index,
                                         columns = combined_feature_importance_scaled.columns)
combined_feature_importance_scaled

feature_importance_plots(combined_feature_importance_scaled)

combined_feature_importance_scaled[np.abs(combined_feature_importance_scaled["combined_feature_importances"])>=0.8].sort_values(by = "combined_feature_importances",ascending=False)

"""Insights:- Above dataframe shows the top 10 features according to combined normalized feature importances.

**PLOT ALL LEARNING CURVES**
"""

def get_experiment_id(experiment_name):
    experiment = mlflow.get_experiment_by_name(experiment_name)
    if experiment:
        return experiment.experiment_id
    else:
        print(f"Experiment '{experiment_name}' not found.")
        return None

def get_run_ids(experiment_id):
    client = mlflow.tracking.MlflowClient()
    runs = client.search_runs(experiment_id)
    return [run.info.run_id for run in runs]

# Example usage
experiment_id = get_experiment_id("nadp_binary")
run_ids = get_run_ids(experiment_id)

def plot_all_learning_curves(experiment_id, run_ids):
    plt.figure(figsize=(10, 40))
    plot_count = 1

    for run_id in run_ids:
        run_path = f"mlruns/{experiment_id}/{run_id}/artifacts/"
        for file in os.listdir(run_path):
            if file.endswith("_learning_curve.png"):
                img = plt.imread(os.path.join(run_path, file))
                plt.subplot(len(run_ids)//2, 2, plot_count)
                plt.imshow(img)
                plt.axis('off')
                plt.title(f"{file.replace('_learning_curve.png', '')}")
                plot_count += 1

    plt.tight_layout()
    plt.show()

# Example usage
plot_all_learning_curves(experiment_id, run_ids)

"""Insights:-     
1. Tuned LightGBM on both balanced and imbalanced datasets shows a consistent learning curve with training and validation scores improving steadily as training size increases.
2. Tuned XGBoost displays a similar pattern, but the validation score tends to plateau sooner compared to the training score, indicating a potential risk of overfitting.
3. Gradient boosting models show smoother learning curves for both balanced and imbalanced datasets, but the gap between training and validation scores suggests some overfitting on imbalanced data.
4. The tuned Adaboost model exhibits steady growth in the learning curves, with balanced datasets showing a closer match between training and validation scores.
5. Bagging models, such as random forest and bagging RF, present varied performance, with balanced datasets achieving better validation alignment compared to imbalanced data.
6. Decision tree models have more erratic learning curves, with training scores significantly higher than validation scores, indicating overfitting.
7. KNN models show stable performance on both balanced and imbalanced datasets, though they do not reach as high accuracy as ensemble models.
8. The tuned MLP classifier shows fluctuating learning curves with occasional dips, but on the balanced dataset, it maintains closer alignment between training and validation.
9. Logistic regression learning curves display consistent performance, with simple logistic regression on balanced data achieving better generalization.
10. The binary k-means learning curve shows a unique pattern with training scores being significantly higher and validation scores decreasing sharply, indicating potential issues with the model’s ability to generalize.

**PLOT ALL AUC PLOTS**
"""

def plot_all_roc_auc_plots(experiment_id, run_ids):
    plt.figure(figsize=(10, 80))
    plot_count = 1

    for run_id in run_ids:
        run_path = f"mlruns/{experiment_id}/{run_id}/artifacts/"
        for file in os.listdir(run_path):
            if file.endswith("_auc_plots.png"):
                img = plt.imread(os.path.join(run_path, file))
                plt.subplot(len(run_ids)//2, 2, plot_count)
                plt.imshow(img)
                plt.axis('off')
                plt.title(f"{file.replace('_auc_plots.png', '')}")
                plot_count += 1

    plt.tight_layout()
    plt.show()

# Example usage
plot_all_roc_auc_plots(experiment_id, run_ids)

"""Insights:-
1. The ROC curves for tuned LightGBM models on both balanced and imbalanced datasets demonstrate high AUC scores, indicating strong classifier performance.
2. Tuned XGBoost models achieve a similar high AUC with curves that approach the top left corner, suggesting excellent discrimination between classes.
3. Gradient boosting models for both balanced and imbalanced datasets show robust ROC curves, signifying good predictive power.
4. Tuned Adaboost exhibits a solid performance with AUC values close to 1, although slight differences in curve shape suggest variations in dataset handling.
5. Bagging RF models present nearly perfect ROC curves, particularly on the balanced dataset, highlighting their effectiveness in classification.
6. Random forest models also display consistently high AUC values, with the balanced dataset having a slightly better ROC curve than the imbalanced one.
7. Decision tree models show high AUC scores, but the imbalance in dataset handling impacts the consistency of the curves.
8. KNN models achieve good AUC, though the curves on balanced data are slightly less steep, suggesting room for improvement in class separation.
9. The MLP classifier demonstrates excellent ROC curves on both balanced and imbalanced datasets, reflecting strong generalization capabilities.
10. Tuned logistic regression models display reliable AUC scores, with consistent curves that show slight differences between balanced and imbalanced datasets.
11. Simple logistic regression has high AUC, with curves that suggest stable performance across both balanced and imbalanced datasets.

**PRINT ALL CLASSIFICATION REPORTS**
"""

def print_all_classification_reports(experiment_id, run_ids):
    for run_id in run_ids:
        run_path = f"mlruns/{experiment_id}/{run_id}/artifacts/"
        for file in os.listdir(run_path):
            if file.endswith("_classification_report.csv"):
                report_df = pd.read_csv(os.path.join(run_path, file), index_col=0)
                print(f"\n{file.replace('_classification_report.csv', '')}:\n")
                print(report_df)
                print("\n" + "-"*80 + "\n")

# Example usage
print_all_classification_reports(experiment_id, run_ids)

"""Insights:- Output has all classification reports. Here we can observe all the models are providing metrics more than 0.9. Boosting models providing metrics more than 0.99

**MULTI-CLASS CLASSIFICATION UTILIZING OPTIMAL MODELS**
"""

# PREPROCESSING FOR MULTI-CLASS CLASSIFICATION

nadp_multi = nadp_add.copy(deep=True)
nadp_multi = nadp_multi.drop(["attack_or_normal","attack","lastflag","service_category","flag_category"],axis = 1)
# Checking null values
sum(nadp_multi.isna().sum())

# # Checking duplicats after removing unwanted features
nadp_multi.duplicated().sum()

# Drop duplicates
nadp_multi.drop_duplicates(keep="first",inplace=True)

#  Seperate features and target
nadp_X_multi = nadp_multi.drop(["attack_category"], axis=1)  # Features
nadp_y_multi = nadp_multi["attack_category"]  # Target variable

# Split the data with stratification
nadp_X_train_multi, nadp_X_test_multi, nadp_y_train_multi, nadp_y_test_multi = train_test_split(nadp_X_multi, nadp_y_multi, test_size=0.2, random_state=42, stratify=nadp_y_multi)

# Verify the value counts in train and test sets
print("Training set value counts:\n", nadp_y_train_multi.value_counts())
print("Test set value counts:\n", nadp_y_test_multi.value_counts())

# three categorical features
nadp_X_train_multi.describe(include = "object")

# checking duplicates after train test split
nadp_X_train_multi[nadp_X_train_multi.duplicated(keep=False)]

# checking duplicates related nadp_y_train_multi
nadp_y_train_multi[nadp_X_train_multi.duplicated(keep=False)]

# REMOVING DUPLICATES WHOSE nadp_y_train_multi == 0 (keeping attacked rows)
# Create a boolean mask for y_train where the value is 0
mask_y_equals_normal = nadp_y_train_multi == "Normal"

# Identify duplicates in X_train where y_train is 0
duplicates_mask = nadp_X_train_multi.duplicated(keep=False)

# Combine both masks to identify the rows to keep
rows_to_keep = nadp_X_train_multi[~(duplicates_mask & mask_y_equals_normal)]

# Remove duplicates from X_train and corresponding values in y_train
nadp_X_train_multi = nadp_X_train_multi.loc[rows_to_keep.index]
nadp_y_train_multi = nadp_y_train_multi.loc[rows_to_keep.index]

# Optionally, reset the index
nadp_X_train_multi.reset_index(drop=True, inplace=True)
nadp_y_train_multi.reset_index(drop=True, inplace=True)

# Final check of duplicates
nadp_X_train_multi.duplicated().sum()

nadp_X_train_multi_encoded = nadp_X_train_multi.copy(deep=True)
nadp_y_train_multi_encoded = nadp_y_train_multi.copy(deep=True)

# Get value counts from the training set and create encoding for 'attack_category'
attack_category_value_counts = nadp_y_train_multi_encoded.value_counts()
attack_category_encoding = {category: rank for rank, category in enumerate(attack_category_value_counts.index)}
nadp_y_train_multi_encoded = nadp_y_train_multi_encoded.map(attack_category_encoding)

# Initialize OneHotEncoder
ohe_encoder = OneHotEncoder(drop="first",sparse_output=False)  # Drop first to avoid multicollinearity

# Fit and transform the selected columns
encoded_data = ohe_encoder.fit_transform(nadp_X_train_multi_encoded[['protocoltype', 'flag']])

# Convert to DataFrame with proper column names
encoded_nadp_X_train_multi_encoded = pd.DataFrame(encoded_data, columns=ohe_encoder.get_feature_names_out(['protocoltype', 'flag']))

# reset index
nadp_X_train_multi_encoded = nadp_X_train_multi_encoded.reset_index(drop=True)
encoded_nadp_X_train_multi_encoded = encoded_nadp_X_train_multi_encoded.reset_index(drop=True)

# Combine the original DataFrame with the encoded DataFrame
nadp_X_train_multi_encoded = pd.concat([nadp_X_train_multi_encoded.drop(columns=['protocoltype', 'flag']), encoded_nadp_X_train_multi_encoded], axis=1)

# Get value counts from the training set and create encoding for 'service'
service_value_counts = nadp_X_train_multi_encoded['service'].value_counts()
service_encoding = {category: rank for rank, category in enumerate(service_value_counts.index)}
nadp_X_train_multi_encoded['service'] = nadp_X_train_multi_encoded['service'].map(service_encoding)

# Save OneHotEncoder
with open('ohe_encoder_multi.pkl', 'wb') as f:
    pickle.dump(ohe_encoder, f)

# Save service encoding mapping
with open('service_encoding_multi.pkl', 'wb') as f:
    pickle.dump(service_encoding, f)

# Save service encoding mapping
with open('attack_category_encoding_multi.pkl', 'wb') as f:
    pickle.dump(attack_category_encoding, f)

# Assuming nadp_X_test_multi is your test dataset
nadp_X_test_multi_encoded = nadp_X_test_multi.copy(deep=True)
nadp_y_test_multi_encoded = nadp_y_test_multi.copy(deep=True)

# Apply frequency encoding for 'attack_category' in the test dataset
nadp_y_test_multi_encoded = nadp_y_test_multi_encoded.map(attack_category_encoding)

# Transform 'protocoltype' and 'flag' columns using the fitted OneHotEncoder
encoded_test_data = ohe_encoder.transform(nadp_X_test_multi_encoded[['protocoltype', 'flag']])
encoded_nadp_X_test_multi_encoded = pd.DataFrame(encoded_test_data, columns=ohe_encoder.get_feature_names_out(['protocoltype', 'flag']))

# Reset index for both DataFrames
encoded_nadp_X_test_multi_encoded = encoded_nadp_X_test_multi_encoded.reset_index(drop=True)
nadp_X_test_multi_encoded = nadp_X_test_multi_encoded.reset_index(drop=True)
nadp_y_test_multi_encoded = nadp_y_test_multi_encoded.reset_index(drop=True)

# Combine the original DataFrame with the encoded DataFrame
nadp_X_test_multi_encoded = pd.concat([nadp_X_test_multi_encoded.drop(columns=['protocoltype', 'flag']), encoded_nadp_X_test_multi_encoded], axis=1)

# Apply frequency encoding for 'service' in the test dataset
nadp_X_test_multi_encoded['service'] = nadp_X_test_multi_encoded['service'].map(service_encoding)

# For any new service types in the test dataset that weren't in the training set, assign max + 1
max_service_value = nadp_X_train_multi_encoded['service'].max()
nadp_X_test_multi_encoded['service'].fillna(max_service_value + 1, inplace=True)

# SCALING
# Create a StandardScaler object
nadp_X_train_multi_scaler = StandardScaler()

# Fit the scaler to the training features and transform them
nadp_X_train_multi_scaled = nadp_X_train_multi_scaler.fit_transform(nadp_X_train_multi_encoded)

# Convert the scaled training features back to a DataFrame
nadp_X_train_multi_scaled = pd.DataFrame(nadp_X_train_multi_scaled, columns=nadp_X_train_multi_encoded.columns)

# Scale the test features using the same scaler
nadp_X_test_multi_scaled = nadp_X_train_multi_scaler.transform(nadp_X_test_multi_encoded)

# Convert the scaled test features back to a DataFrame
nadp_X_test_multi_scaled = pd.DataFrame(nadp_X_test_multi_scaled, columns=nadp_X_test_multi_encoded.columns)

# Save the scaler to a file
with open('nadp_X_train_multi_scaler.pkl', 'wb') as file:
    pickle.dump(nadp_X_train_multi_scaler, file)

def calculate_vif(X):
    vif = pd.DataFrame()
    vif["Features"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif

def remove_worst_feature(X):
    vif = calculate_vif(X)
    vif["VIF"] = round(vif["VIF"], 2)
    vif = vif.sort_values(by="VIF", ascending=False)

    # Check if all VIF values are less than 10
    if vif["VIF"].max() < 10:
        return X  # Stop if all VIFs are acceptable

def calculate_vif(X):
    vif = pd.DataFrame()
    vif["Features"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif

def remove_worst_feature(X):
    vif = calculate_vif(X)
    vif["VIF"] = round(vif["VIF"], 2)
    vif = vif.sort_values(by="VIF", ascending=False)

    # Check if all VIF values are less than 10
    if vif["VIF"].max() < 10:
        return X  # Stop if all VIFs are acceptable

    # Remove the feature with the highest VIF
    worst_feature = vif["Features"].iloc[0]
    print(f"Removing feature: {worst_feature} with VIF: {vif['VIF'].iloc[0]}")

    # Recursively call the function with the reduced dataset
    return remove_worst_feature(X.drop(columns=[worst_feature]))

# VIF should be applied only among continuous features
X_t = nadp_X_train_multi_scaled[['duration', 'srcbytes', 'dstbytes', 'wrongfragment',
       'urgent', 'hot', 'numfailedlogins', 'numcompromised', 'numroot', 'numfilecreations', 'numshells',
       'numaccessfiles', 'count', 'srvcount','serrorrate', 'srvserrorrate', 'rerrorrate', 'srvrerrorrate',
       'samesrvrate', 'diffsrvrate', 'srvdiffhostrate', 'dsthostcount',
       'dsthostsrvcount', 'dsthostsamesrvrate', 'dsthostdiffsrvrate',
       'dsthostsamesrcportrate', 'dsthostsrvdiffhostrate', 'dsthostserrorrate',
       'dsthostsrvserrorrate', 'dsthostrerrorrate', 'dsthostsrvrerrorrate',
       'serrors_count', 'rerrors_count', 'samesrv_count', 'diffsrv_count',
       'serrors_srvcount', 'rerrors_srvcount', 'srvdiffhost_srvcount',
       'dsthost_serrors_count', 'dsthost_rerrors_count',
       'dsthost_samesrv_count', 'dsthost_diffsrv_count',
       'dsthost_serrors_srvcount', 'dsthost_rerrors_srvcount',
       'dsthost_samesrcport_srvcount', 'dsthost_srvdiffhost_srvcount',
       'srcbytes/sec', 'dstbytes/sec']]
VIF_reduced = remove_worst_feature(X_t)

def calculate_vif(X):
    vif = pd.DataFrame()
    vif["Features"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif

def remove_worst_feature(X):
    vif = calculate_vif(X)
    vif["VIF"] = round(vif["VIF"], 2)
    vif = vif.sort_values(by="VIF", ascending=False)

    # Check if all VIF values are less than 10
    if vif["VIF"].max() < 10:
        return X  # Stop if all VIFs are acceptable

    # Remove the feature with the highest VIF
    worst_feature = vif["Features"].iloc[0]
    print(f"Removing feature: {worst_feature} with VIF: {vif['VIF'].iloc[0]}")

    # Recursively call the function with the reduced dataset
    return remove_worst_feature(X.drop(columns=[worst_feature]))

# VIF should be applied only among continuous features
X_t = nadp_X_train_multi_scaled[['duration', 'srcbytes', 'dstbytes', 'wrongfragment',
       'urgent', 'hot', 'numfailedlogins', 'numcompromised', 'numroot', 'numfilecreations', 'numshells',
       'numaccessfiles', 'count', 'srvcount','serrorrate', 'srvserrorrate', 'rerrorrate', 'srvrerrorrate',
       'samesrvrate', 'diffsrvrate', 'srvdiffhostrate', 'dsthostcount',
       'dsthostsrvcount', 'dsthostsamesrvrate', 'dsthostdiffsrvrate',
       'dsthostsamesrcportrate', 'dsthostsrvdiffhostrate', 'dsthostserrorrate',
       'dsthostsrvserrorrate', 'dsthostrerrorrate', 'dsthostsrvrerrorrate',
       'serrors_count', 'rerrors_count', 'samesrv_count', 'diffsrv_count',
       'serrors_srvcount', 'rerrors_srvcount', 'srvdiffhost_srvcount',
       'dsthost_serrors_count', 'dsthost_rerrors_count',
       'dsthost_samesrv_count', 'dsthost_diffsrv_count',
       'dsthost_serrors_srvcount', 'dsthost_rerrors_srvcount',
       'dsthost_samesrcport_srvcount', 'dsthost_srvdiffhost_srvcount',
       'srcbytes/sec', 'dstbytes/sec']]
VIF_reduced = remove_worst_feature(X_t)

# The reduced dataset will have all VIFs < 10
print("Final features after VIF removal:", VIF_reduced.columns)
# VIF should be applied only among continuous features
X_t = nadp_X_train_multi_scaled[['duration', 'srcbytes', 'dstbytes', 'wrongfragment',
       'urgent', 'hot', 'numfailedlogins', 'numcompromised', 'numroot', 'numfilecreations', 'numshells',
       'numaccessfiles', 'count', 'srvcount','serrorrate', 'srvserrorrate', 'rerrorrate', 'srvrerrorrate',
       'samesrvrate', 'diffsrvrate', 'srvdiffhostrate', 'dsthostcount',
       'dsthostsrvcount', 'dsthostsamesrvrate', 'dsthostdiffsrvrate',
       'dsthostsamesrcportrate', 'dsthostsrvdiffhostrate', 'dsthostserrorrate',
       'dsthostsrvserrorrate', 'dsthostrerrorrate', 'dsthostsrvrerrorrate',
       'serrors_count', 'rerrors_count', 'samesrv_count', 'diffsrv_count',
       'serrors_srvcount', 'rerrors_srvcount', 'srvdiffhost_srvcount',
       'dsthost_serrors_count', 'dsthost_rerrors_count',
       'dsthost_samesrv_count', 'dsthost_diffsrv_count',
       'dsthost_serrors_srvcount', 'dsthost_rerrors_srvcount',
       'dsthost_samesrcport_srvcount', 'dsthost_srvdiffhost_srvcount',
       'srcbytes/sec', 'dstbytes/sec']]
VIF_reduced = remove_worst_feature(X_t)

# The reduced dataset will have all VIFs < 10
print("Final features after VIF removal:", VIF_reduced.columns)

"""Insights:- VIF reduced features are same in both multi and binary class classifiation

Final Insights:-
1. Model Performance and Efficiency

    Anomaly Detection Framework:

        Binary Classification: AdaBoost achieves 99% accuracy in distinguishing normal vs. anomalous network traffic.

        Multi-Class Classification: LightGBM identifies specific attack types (e.g., DoS, Probe) with high precision.

        Use Case: Enables rapid identification of intrusions, reducing administrative response latency.

2. Computational Trade-offs

    Feature Engineering Impact:

        Clustering techniques (LOF, DBSCAN) add ~60 seconds to test-time latency due to density-based calculations.

        Optimization Suggestion: Remove these clustering-derived features (e.g., binary_kmeans_adv) to prioritize speed in real-time deployments.

3. Critical Features for Detection

    Top Influential Features:

        Binary Classification: error_flag_or_not, urgent_or_not, service, srcbytes, diffsrvcount.

        Multi-Class Classification: Flag_SF, dstbytes, service.

    Cluster Strategy Insights:

        Sparse/dense clustering (alpha/beta/gamma approach) via binary_kmeans_adv outperforms multi-class clustering, suggesting tailored feature engineering improves model performance.

4. Deployment and Real-Time Alerts

    Streamlit Integration:

        Enables live network traffic analysis with instant anomaly alerts.

        Benefits: Minimizes threat response time by providing actionable insights to administrators.

Key Takeaways

    Accuracy vs. Speed: Balance high detection accuracy (99%) with latency reduction by streamlining feature engineering.

    Actionable Monitoring: Focus on critical features like service and srcbytes during data collection to enhance model efficiency.

    Scalability: LightGBM and AdaBoost’s computational efficiency supports scalable real-world deployment.

Recommendation:-
1. Model Enhancement

Further boost detection accuracy and robustness by fine-tuning hyperparameters for AdaBoost and LightGBM models. Expanding the algorithm search to include alternatives like Random Forest and XGBoost can also uncover models better suited to the data and operational requirements


2. Scalability for Large Datasets

As your data volume increases, leverage distributed computing frameworks (such as Apache Spark or Hadoop) to efficiently train models and accelerate inference across multiple machines. This parallel processing approach ensures the solution remains responsive and cost-effective as demands grow


3. Seamless Integration

Integrate the anomaly detection model into existing network monitoring or intrusion detection systems (IDS) using APIs, real-time data pipelines, or batch processes. Proper integration automates threat detection and enables immediate alerts, strengthening the organization’s security posture


4. Continuous Data Quality Improvement

Maintain high detection performance by continually updating your dataset with recent and diverse network activity. Pay special attention to categorical features like service and protocol_type, as their quality and variety directly influence model effectiveness

5. Regular retraining and feature engineering ensure the system adapts to evolving network behaviors and threats.

This approach ensures your anomaly detection system remains accurate, scalable, easy to integrate, and resilient to changing network environments.
"""

































































































